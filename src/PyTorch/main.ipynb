{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"main.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1O_jy_tDYIZZ4D2Iy8z6jAZGqXiCSDeP-","authorship_tag":"ABX9TyNAKKBQtoMyiXsIre+IW9+n"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"AXoOkXgxNlt1","colab_type":"code","colab":{}},"source":["!pip install -U -q pyDrive\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7JvZIYIxNpky","colab_type":"code","colab":{}},"source":["auth.authenticate_user()\n","gauth = GoogleAuth()\n","gauth.credentials = GoogleCredentials.get_application_default()\n","drive = GoogleDrive(gauth)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lpQosBafTKjy","colab_type":"code","colab":{}},"source":["import os\n","os.listdir()\n","os.chdir(\"drive/My Drive/Colab Notebooks/Text-Generation/PyTorch/\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KEHgnQJC0vFT","colab_type":"code","colab":{}},"source":["# coding: utf-8\n","import argparse\n","import time\n","import math\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.onnx\n","import data\n","import model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-swEUtv_01QX","colab_type":"code","colab":{}},"source":["# parser = argparse.ArgumentParser(description='PyTorch Wikitext-2 RNN/LSTM/GRU/Transformer Language Model')\n","# parser.add_argument('--data', type=str, default='./data/paul_graham',\n","#                     help='location of the data corpus')\n","# parser.add_argument('--model', type=str, default='LSTM',\n","#                     help='type of recurrent net (RNN_TANH, RNN_RELU, LSTM, GRU, Transformer)')\n","# parser.add_argument('--emsize', type=int, default=200,\n","#                     help='size of word embeddings')\n","# parser.add_argument('--nhid', type=int, default=200,\n","#                     help='number of hidden units per layer')\n","# parser.add_argument('--nlayers', type=int, default=2,\n","#                     help='number of layers')\n","# parser.add_argument('--lr', type=float, default=20,\n","#                     help='initial learning rate')\n","# parser.add_argument('--clip', type=float, default=0.25,\n","#                     help='gradient clipping')\n","# parser.add_argument('--epochs', type=int, default=40,\n","#                     help='upper epoch limit')\n","# parser.add_argument('--batch_size', type=int, default=20, metavar='N',\n","#                     help='batch size')\n","# parser.add_argument('--bptt', type=int, default=35,\n","#                     help='sequence length')\n","# parser.add_argument('--dropout', type=float, default=0.2,\n","#                     help='dropout applied to layers (0 = no dropout)')\n","# parser.add_argument('--tied', action='store_true',\n","#                     help='tie the word embedding and softmax weights')\n","# parser.add_argument('--seed', type=int, default=1111,\n","#                     help='random seed')\n","# parser.add_argument('--cuda', action='store_true',\n","#                     help='use CUDA')\n","# parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n","#                     help='report interval')\n","# parser.add_argument('--save', type=str, default='model.pt',\n","#                     help='path to save the final model')\n","# parser.add_argument('--onnx-export', type=str, default='',\n","#                     help='path to export the final model in onnx format')\n","\n","# parser.add_argument('--nhead', type=int, default=2,\n","#                     help='the number of heads in the encoder/decoder of the transformer model')\n","\n","# args = parser.parse_args()\n","\n","args = {}\n","args[\"cuda\"] = True\n","args[\"save\"] = \"model3.pt\"\n","args[\"onnx_export\"] = \"\"\n","args[\"log_interval\"] = 200\n","args[\"nhead\"] = 2\n","args[\"seed\"] = 1111\n","args[\"tied\"] = False\n","args[\"dropout\"] = 0.65\n","args[\"bptt\"] = 35\n","args[\"batch_size\"] = 20\n","args[\"epochs\"] = 40\n","args[\"clip\"] = 0.25\n","args[\"lr\"] = 20\n","args[\"nlayers\"] = 2\n","args[\"nhid\"] = 1500\n","args[\"emsize\"] = 1500\n","args[\"data\"] = \"./data/paul_graham\"\n","args[\"model\"] = \"LSTM\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSo7etLy038c","colab_type":"code","colab":{}},"source":["# Set the random seed manually for reproducibility.\n","torch.manual_seed(args[\"seed\"])\n","if torch.cuda.is_available():\n","    if not args[\"cuda\"]:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","device = torch.device(\"cuda\" if args[\"cuda\"] else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wVaHcew408vG","colab_type":"code","colab":{}},"source":["###############################################################################\n","# Load data\n","###############################################################################\n","\n","corpus = data.Corpus(args[\"data\"])\n","\n","# Starting from sequential data, batchify arranges the dataset into columns.\n","# For instance, with the alphabet as the sequence and batch size 4, we'd get\n","# ┌ a g m s ┐\n","# │ b h n t │\n","# │ c i o u │\n","# │ d j p v │\n","# │ e k q w │\n","# └ f l r x ┘.\n","# These columns are treated as independent by the model, which means that the\n","# dependence of e. g. 'g' on 'f' can not be learned, but allows more efficient\n","# batch processing.\n","\n","def batchify(data, bsz):\n","    # Work out how cleanly we can divide the dataset into bsz parts.\n","    nbatch = data.size(0) // bsz\n","    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n","    data = data.narrow(0, 0, nbatch * bsz)\n","    # Evenly divide the data across the bsz batches.\n","    data = data.view(bsz, -1).t().contiguous()\n","    return data.to(device)\n","\n","eval_batch_size = 10\n","train_data = batchify(corpus.train, args[\"batch_size\"])\n","val_data = batchify(corpus.valid, eval_batch_size)\n","test_data = batchify(corpus.test, eval_batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CKj3F3Us0-VN","colab_type":"code","colab":{}},"source":["###############################################################################\n","# Build the model\n","###############################################################################\n","\n","ntokens = len(corpus.dictionary)\n","if args[\"model\"] == 'Transformer':\n","    model = model.TransformerModel(ntokens, args[\"emsize\"], args[\"nhead\"], args[\"nhid\"], args[\"nlayers\"], args[\"dropout\"]).to(device)\n","else:\n","    model = model.RNNModel(args[\"model\"], ntokens, args[\"emsize\"], args[\"nhid\"], args[\"nlayers\"], args[\"dropout\"], args[\"tied\"]).to(device)\n","\n","criterion = nn.NLLLoss()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P7uBb0Cj0-_K","colab_type":"code","outputId":"84afb287-1eb7-4433-fd7a-a9d35cfdc7c2","executionInfo":{"status":"ok","timestamp":1585531614630,"user_tz":240,"elapsed":2462862,"user":{"displayName":"Nikunj Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgD0pS1NL2dBJiFQz8hU82Sdb-UZb7jgwPjLw5f=s64","userId":"16906182826931650917"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["###############################################################################\n","# Training code\n","###############################################################################\n","\n","def repackage_hidden(h):\n","    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n","\n","    if isinstance(h, torch.Tensor):\n","        return h.detach()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)\n","\n","\n","# get_batch subdivides the source data into chunks of length args.bptt.\n","# If source is equal to the example output of the batchify function, with\n","# a bptt-limit of 2, we'd get the following two Variables for i = 0:\n","# ┌ a g m s ┐ ┌ b h n t ┐\n","# └ b h n t ┘ └ c i o u ┘\n","# Note that despite the name of the function, the subdivison of data is not\n","# done along the batch dimension (i.e. dimension 1), since that was handled\n","# by the batchify function. The chunks are along dimension 0, corresponding\n","# to the seq_len dimension in the LSTM.\n","\n","def get_batch(source, i):\n","    seq_len = min(args[\"bptt\"], len(source) - 1 - i)\n","    data = source[i:i+seq_len]\n","    target = source[i+1:i+1+seq_len].view(-1)\n","    return data, target\n","\n","\n","def evaluate(data_source):\n","    # Turn on evaluation mode which disables dropout.\n","    model.eval()\n","    total_loss = 0.\n","    ntokens = len(corpus.dictionary)\n","    if args[\"model\"] != 'Transformer':\n","        hidden = model.init_hidden(eval_batch_size)\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(0) - 1, args[\"bptt\"]):\n","            data, targets = get_batch(data_source, i)\n","            if args[\"model\"] == 'Transformer':\n","                output = model(data)\n","                output = output.view(-1, ntokens)\n","            else:\n","                output, hidden = model(data, hidden)\n","                hidden = repackage_hidden(hidden)\n","            total_loss += len(data) * criterion(output, targets).item()\n","    return total_loss / (len(data_source) - 1)\n","\n","\n","def train():\n","    # Turn on training mode which enables dropout.\n","    model.train()\n","    total_loss = 0.\n","    start_time = time.time()\n","    ntokens = len(corpus.dictionary)\n","    if args[\"model\"] != 'Transformer':\n","        hidden = model.init_hidden(args[\"batch_size\"])\n","    for batch, i in enumerate(range(0, train_data.size(0) - 1, args[\"bptt\"])):\n","        data, targets = get_batch(train_data, i)\n","        # Starting each batch, we detach the hidden state from how it was previously produced.\n","        # If we didn't, the model would try backpropagating all the way to start of the dataset.\n","        model.zero_grad()\n","        if args[\"model\"] == 'Transformer':\n","            output = model(data)\n","            output = output.view(-1, ntokens)\n","        else:\n","            hidden = repackage_hidden(hidden)\n","            output, hidden = model(data, hidden)\n","        loss = criterion(output, targets)\n","        loss.backward()\n","\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), args[\"clip\"])\n","        for p in model.parameters():\n","            p.data.add_(-lr, p.grad.data)\n","\n","        total_loss += loss.item()\n","\n","        if batch % args[\"log_interval\"] == 0 and batch > 0:\n","            cur_loss = total_loss / args[\"log_interval\"]\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | ms/batch {:5.2f} | '\n","                    'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch, len(train_data) // args[\"bptt\"], lr,\n","                elapsed * 1000 / args[\"log_interval\"], cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","\n","def export_onnx(path, batch_size, seq_len):\n","    print('The model is also exported in ONNX format at {}'.\n","          format(os.path.realpath(args[\"onnx_export\"])))\n","    model.eval()\n","    dummy_input = torch.LongTensor(seq_len * batch_size).zero_().view(-1, batch_size).to(device)\n","    hidden = model.init_hidden(batch_size)\n","    torch.onnx.export(model, (dummy_input, hidden), path)\n","\n","\n","# Loop over epochs.\n","lr = args[\"lr\"]\n","best_val_loss = None\n","\n","# At any point you can hit Ctrl + C to break out of training early.\n","try:\n","    for epoch in range(1, args[\"epochs\"]+1):\n","        epoch_start_time = time.time()\n","        train()\n","        val_loss = evaluate(val_data)\n","        print('-' * 89)\n","        print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                           val_loss, math.exp(val_loss)))\n","        print('-' * 89)\n","        # Save the model if the validation loss is the best we've seen so far.\n","        if not best_val_loss or val_loss < best_val_loss:\n","            with open(args[\"save\"], 'wb') as f:\n","                torch.save(model, f)\n","            best_val_loss = val_loss\n","        else:\n","            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n","            lr /= 4.0\n","except KeyboardInterrupt:\n","    print('-' * 89)\n","    print('Exiting from training early')\n","\n","# Load the best saved model.\n","with open(args[\"save\"], 'rb') as f:\n","    model = torch.load(f)\n","    # after load the rnn params are not a continuous chunk of memory\n","    # this makes them a continuous chunk, and will speed up forward pass\n","    # Currently, only rnn model supports flatten_parameters function.\n","    if args[\"model\"] in ['RNN_TANH', 'RNN_RELU', 'LSTM', 'GRU']:\n","        model.rnn.flatten_parameters()\n","\n","# Run on test data.\n","test_loss = evaluate(test_data)\n","print('=' * 89)\n","print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","    test_loss, math.exp(test_loss)))\n","print('=' * 89)\n","\n","if len(args[\"onnx_export\"]) > 0:\n","    # Export the model in ONNX format.\n","    export_onnx(args[\"onnx_export\"], batch_size=1, seq_len=args[\"bptt\"])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["| epoch   1 |   200/ 1110 batches | lr 20.00 | ms/batch 161.85 | loss  8.28 | ppl  3939.36\n","| epoch   1 |   400/ 1110 batches | lr 20.00 | ms/batch 179.50 | loss  6.99 | ppl  1082.59\n","| epoch   1 |   600/ 1110 batches | lr 20.00 | ms/batch 171.67 | loss  6.39 | ppl   594.18\n","| epoch   1 |   800/ 1110 batches | lr 20.00 | ms/batch 174.22 | loss  6.12 | ppl   456.69\n","| epoch   1 |  1000/ 1110 batches | lr 20.00 | ms/batch 174.81 | loss  5.98 | ppl   395.16\n","-----------------------------------------------------------------------------------------\n","| end of epoch   1 | time: 219.74s | valid loss  5.78 | valid ppl   322.23\n","-----------------------------------------------------------------------------------------\n","| epoch   2 |   200/ 1110 batches | lr 20.00 | ms/batch 176.48 | loss  5.82 | ppl   338.40\n","| epoch   2 |   400/ 1110 batches | lr 20.00 | ms/batch 174.59 | loss  5.70 | ppl   298.84\n","| epoch   2 |   600/ 1110 batches | lr 20.00 | ms/batch 173.62 | loss  5.56 | ppl   260.52\n","| epoch   2 |   800/ 1110 batches | lr 20.00 | ms/batch 174.56 | loss  5.51 | ppl   246.95\n","| epoch   2 |  1000/ 1110 batches | lr 20.00 | ms/batch 174.71 | loss  5.46 | ppl   234.04\n","-----------------------------------------------------------------------------------------\n","| end of epoch   2 | time: 222.47s | valid loss  5.35 | valid ppl   210.68\n","-----------------------------------------------------------------------------------------\n","| epoch   3 |   200/ 1110 batches | lr 20.00 | ms/batch 175.76 | loss  5.43 | ppl   227.12\n","| epoch   3 |   400/ 1110 batches | lr 20.00 | ms/batch 175.17 | loss  5.34 | ppl   208.55\n","| epoch   3 |   600/ 1110 batches | lr 20.00 | ms/batch 173.92 | loss  5.24 | ppl   187.99\n","| epoch   3 |   800/ 1110 batches | lr 20.00 | ms/batch 173.45 | loss  5.21 | ppl   183.16\n","| epoch   3 |  1000/ 1110 batches | lr 20.00 | ms/batch 173.83 | loss  5.18 | ppl   177.04\n","-----------------------------------------------------------------------------------------\n","| end of epoch   3 | time: 221.92s | valid loss  5.10 | valid ppl   164.17\n","-----------------------------------------------------------------------------------------\n","| epoch   4 |   200/ 1110 batches | lr 20.00 | ms/batch 176.01 | loss  5.18 | ppl   177.04\n","| epoch   4 |   400/ 1110 batches | lr 20.00 | ms/batch 175.07 | loss  5.11 | ppl   165.23\n","| epoch   4 |   600/ 1110 batches | lr 20.00 | ms/batch 173.61 | loss  5.01 | ppl   150.37\n","| epoch   4 |   800/ 1110 batches | lr 20.00 | ms/batch 173.76 | loss  5.00 | ppl   148.54\n","| epoch   4 |  1000/ 1110 batches | lr 20.00 | ms/batch 174.29 | loss  4.97 | ppl   144.15\n","-----------------------------------------------------------------------------------------\n","| end of epoch   4 | time: 222.12s | valid loss  4.90 | valid ppl   134.09\n","-----------------------------------------------------------------------------------------\n","| epoch   5 |   200/ 1110 batches | lr 20.00 | ms/batch 176.03 | loss  4.98 | ppl   145.36\n","| epoch   5 |   400/ 1110 batches | lr 20.00 | ms/batch 175.17 | loss  4.91 | ppl   135.85\n","| epoch   5 |   600/ 1110 batches | lr 20.00 | ms/batch 173.68 | loss  4.83 | ppl   125.06\n","| epoch   5 |   800/ 1110 batches | lr 20.00 | ms/batch 174.41 | loss  4.82 | ppl   123.70\n","| epoch   5 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.34 | loss  4.79 | ppl   120.59\n","-----------------------------------------------------------------------------------------\n","| end of epoch   5 | time: 222.42s | valid loss  4.73 | valid ppl   113.04\n","-----------------------------------------------------------------------------------------\n","| epoch   6 |   200/ 1110 batches | lr 20.00 | ms/batch 177.35 | loss  4.80 | ppl   121.33\n","| epoch   6 |   400/ 1110 batches | lr 20.00 | ms/batch 173.65 | loss  4.74 | ppl   114.18\n","| epoch   6 |   600/ 1110 batches | lr 20.00 | ms/batch 175.73 | loss  4.65 | ppl   105.11\n","| epoch   6 |   800/ 1110 batches | lr 20.00 | ms/batch 174.03 | loss  4.65 | ppl   104.59\n","| epoch   6 |  1000/ 1110 batches | lr 20.00 | ms/batch 173.49 | loss  4.63 | ppl   102.74\n","-----------------------------------------------------------------------------------------\n","| end of epoch   6 | time: 222.25s | valid loss  4.57 | valid ppl    96.13\n","-----------------------------------------------------------------------------------------\n","| epoch   7 |   200/ 1110 batches | lr 20.00 | ms/batch 176.48 | loss  4.63 | ppl   102.98\n","| epoch   7 |   400/ 1110 batches | lr 20.00 | ms/batch 174.37 | loss  4.57 | ppl    96.68\n","| epoch   7 |   600/ 1110 batches | lr 20.00 | ms/batch 173.98 | loss  4.50 | ppl    89.87\n","| epoch   7 |   800/ 1110 batches | lr 20.00 | ms/batch 173.82 | loss  4.49 | ppl    89.24\n","| epoch   7 |  1000/ 1110 batches | lr 20.00 | ms/batch 174.12 | loss  4.47 | ppl    87.48\n","-----------------------------------------------------------------------------------------\n","| end of epoch   7 | time: 222.32s | valid loss  4.41 | valid ppl    82.59\n","-----------------------------------------------------------------------------------------\n","| epoch   8 |   200/ 1110 batches | lr 20.00 | ms/batch 176.70 | loss  4.47 | ppl    87.74\n","| epoch   8 |   400/ 1110 batches | lr 20.00 | ms/batch 176.23 | loss  4.41 | ppl    82.47\n","| epoch   8 |   600/ 1110 batches | lr 20.00 | ms/batch 175.39 | loss  4.34 | ppl    76.69\n","| epoch   8 |   800/ 1110 batches | lr 20.00 | ms/batch 174.48 | loss  4.34 | ppl    76.69\n","| epoch   8 |  1000/ 1110 batches | lr 20.00 | ms/batch 174.43 | loss  4.31 | ppl    74.78\n","-----------------------------------------------------------------------------------------\n","| end of epoch   8 | time: 222.98s | valid loss  4.29 | valid ppl    72.80\n","-----------------------------------------------------------------------------------------\n","| epoch   9 |   200/ 1110 batches | lr 20.00 | ms/batch 177.17 | loss  4.32 | ppl    75.48\n","| epoch   9 |   400/ 1110 batches | lr 20.00 | ms/batch 175.50 | loss  4.26 | ppl    70.92\n","| epoch   9 |   600/ 1110 batches | lr 20.00 | ms/batch 174.14 | loss  4.19 | ppl    66.05\n","| epoch   9 |   800/ 1110 batches | lr 20.00 | ms/batch 174.82 | loss  4.19 | ppl    66.11\n","| epoch   9 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.25 | loss  4.17 | ppl    64.71\n","-----------------------------------------------------------------------------------------\n","| end of epoch   9 | time: 223.11s | valid loss  4.15 | valid ppl    63.20\n","-----------------------------------------------------------------------------------------\n","| epoch  10 |   200/ 1110 batches | lr 20.00 | ms/batch 176.69 | loss  4.17 | ppl    64.66\n","| epoch  10 |   400/ 1110 batches | lr 20.00 | ms/batch 176.04 | loss  4.12 | ppl    61.64\n","| epoch  10 |   600/ 1110 batches | lr 20.00 | ms/batch 174.23 | loss  4.04 | ppl    56.64\n","| epoch  10 |   800/ 1110 batches | lr 20.00 | ms/batch 174.97 | loss  4.04 | ppl    56.97\n","| epoch  10 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.52 | loss  4.03 | ppl    56.12\n","-----------------------------------------------------------------------------------------\n","| end of epoch  10 | time: 223.06s | valid loss  4.04 | valid ppl    56.89\n","-----------------------------------------------------------------------------------------\n","| epoch  11 |   200/ 1110 batches | lr 20.00 | ms/batch 176.77 | loss  4.03 | ppl    56.31\n","| epoch  11 |   400/ 1110 batches | lr 20.00 | ms/batch 176.73 | loss  3.97 | ppl    53.13\n","| epoch  11 |   600/ 1110 batches | lr 20.00 | ms/batch 175.97 | loss  3.89 | ppl    49.10\n","| epoch  11 |   800/ 1110 batches | lr 20.00 | ms/batch 175.70 | loss  3.90 | ppl    49.50\n","| epoch  11 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.25 | loss  3.88 | ppl    48.53\n","-----------------------------------------------------------------------------------------\n","| end of epoch  11 | time: 223.58s | valid loss  3.93 | valid ppl    50.70\n","-----------------------------------------------------------------------------------------\n","| epoch  12 |   200/ 1110 batches | lr 20.00 | ms/batch 177.25 | loss  3.88 | ppl    48.60\n","| epoch  12 |   400/ 1110 batches | lr 20.00 | ms/batch 174.57 | loss  3.83 | ppl    46.17\n","| epoch  12 |   600/ 1110 batches | lr 20.00 | ms/batch 174.72 | loss  3.76 | ppl    42.87\n","| epoch  12 |   800/ 1110 batches | lr 20.00 | ms/batch 174.21 | loss  3.76 | ppl    43.13\n","| epoch  12 |  1000/ 1110 batches | lr 20.00 | ms/batch 176.21 | loss  3.75 | ppl    42.42\n","-----------------------------------------------------------------------------------------\n","| end of epoch  12 | time: 222.99s | valid loss  3.86 | valid ppl    47.67\n","-----------------------------------------------------------------------------------------\n","| epoch  13 |   200/ 1110 batches | lr 20.00 | ms/batch 177.12 | loss  3.76 | ppl    42.76\n","| epoch  13 |   400/ 1110 batches | lr 20.00 | ms/batch 174.43 | loss  3.70 | ppl    40.26\n","| epoch  13 |   600/ 1110 batches | lr 20.00 | ms/batch 174.18 | loss  3.63 | ppl    37.66\n","| epoch  13 |   800/ 1110 batches | lr 20.00 | ms/batch 174.91 | loss  3.63 | ppl    37.89\n","| epoch  13 |  1000/ 1110 batches | lr 20.00 | ms/batch 174.03 | loss  3.62 | ppl    37.36\n","-----------------------------------------------------------------------------------------\n","| end of epoch  13 | time: 222.69s | valid loss  3.75 | valid ppl    42.47\n","-----------------------------------------------------------------------------------------\n","| epoch  14 |   200/ 1110 batches | lr 20.00 | ms/batch 176.98 | loss  3.62 | ppl    37.41\n","| epoch  14 |   400/ 1110 batches | lr 20.00 | ms/batch 175.84 | loss  3.58 | ppl    35.72\n","| epoch  14 |   600/ 1110 batches | lr 20.00 | ms/batch 175.50 | loss  3.50 | ppl    33.17\n","| epoch  14 |   800/ 1110 batches | lr 20.00 | ms/batch 174.79 | loss  3.52 | ppl    33.70\n","| epoch  14 |  1000/ 1110 batches | lr 20.00 | ms/batch 174.39 | loss  3.50 | ppl    33.16\n","-----------------------------------------------------------------------------------------\n","| end of epoch  14 | time: 223.17s | valid loss  3.69 | valid ppl    40.08\n","-----------------------------------------------------------------------------------------\n","| epoch  15 |   200/ 1110 batches | lr 20.00 | ms/batch 176.53 | loss  3.51 | ppl    33.48\n","| epoch  15 |   400/ 1110 batches | lr 20.00 | ms/batch 175.17 | loss  3.46 | ppl    31.72\n","| epoch  15 |   600/ 1110 batches | lr 20.00 | ms/batch 174.34 | loss  3.39 | ppl    29.68\n","| epoch  15 |   800/ 1110 batches | lr 20.00 | ms/batch 175.98 | loss  3.40 | ppl    30.08\n","| epoch  15 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.76 | loss  3.39 | ppl    29.74\n","-----------------------------------------------------------------------------------------\n","| end of epoch  15 | time: 223.34s | valid loss  3.61 | valid ppl    37.02\n","-----------------------------------------------------------------------------------------\n","| epoch  16 |   200/ 1110 batches | lr 20.00 | ms/batch 176.47 | loss  3.40 | ppl    29.87\n","| epoch  16 |   400/ 1110 batches | lr 20.00 | ms/batch 175.58 | loss  3.35 | ppl    28.60\n","| epoch  16 |   600/ 1110 batches | lr 20.00 | ms/batch 174.23 | loss  3.29 | ppl    26.72\n","| epoch  16 |   800/ 1110 batches | lr 20.00 | ms/batch 175.30 | loss  3.30 | ppl    27.15\n","| epoch  16 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.56 | loss  3.29 | ppl    26.92\n","-----------------------------------------------------------------------------------------\n","| end of epoch  16 | time: 223.19s | valid loss  3.55 | valid ppl    34.65\n","-----------------------------------------------------------------------------------------\n","| epoch  17 |   200/ 1110 batches | lr 20.00 | ms/batch 176.58 | loss  3.30 | ppl    27.04\n","| epoch  17 |   400/ 1110 batches | lr 20.00 | ms/batch 175.81 | loss  3.25 | ppl    25.78\n","| epoch  17 |   600/ 1110 batches | lr 20.00 | ms/batch 174.88 | loss  3.18 | ppl    24.11\n","| epoch  17 |   800/ 1110 batches | lr 20.00 | ms/batch 175.11 | loss  3.21 | ppl    24.68\n","| epoch  17 |  1000/ 1110 batches | lr 20.00 | ms/batch 174.81 | loss  3.20 | ppl    24.55\n","-----------------------------------------------------------------------------------------\n","| end of epoch  17 | time: 222.96s | valid loss  3.49 | valid ppl    32.78\n","-----------------------------------------------------------------------------------------\n","| epoch  18 |   200/ 1110 batches | lr 20.00 | ms/batch 176.97 | loss  3.21 | ppl    24.69\n","| epoch  18 |   400/ 1110 batches | lr 20.00 | ms/batch 175.50 | loss  3.16 | ppl    23.47\n","| epoch  18 |   600/ 1110 batches | lr 20.00 | ms/batch 174.22 | loss  3.10 | ppl    22.22\n","| epoch  18 |   800/ 1110 batches | lr 20.00 | ms/batch 174.77 | loss  3.11 | ppl    22.46\n","| epoch  18 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.37 | loss  3.11 | ppl    22.48\n","-----------------------------------------------------------------------------------------\n","| end of epoch  18 | time: 223.05s | valid loss  3.30 | valid ppl    27.17\n","-----------------------------------------------------------------------------------------\n","| epoch  19 |   200/ 1110 batches | lr 20.00 | ms/batch 176.04 | loss  3.12 | ppl    22.71\n","| epoch  19 |   400/ 1110 batches | lr 20.00 | ms/batch 176.16 | loss  3.08 | ppl    21.75\n","| epoch  19 |   600/ 1110 batches | lr 20.00 | ms/batch 175.44 | loss  3.01 | ppl    20.35\n","| epoch  19 |   800/ 1110 batches | lr 20.00 | ms/batch 175.09 | loss  3.03 | ppl    20.80\n","| epoch  19 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.00 | loss  3.03 | ppl    20.75\n","-----------------------------------------------------------------------------------------\n","| end of epoch  19 | time: 223.03s | valid loss  3.22 | valid ppl    25.07\n","-----------------------------------------------------------------------------------------\n","| epoch  20 |   200/ 1110 batches | lr 20.00 | ms/batch 177.51 | loss  3.04 | ppl    20.98\n","| epoch  20 |   400/ 1110 batches | lr 20.00 | ms/batch 175.44 | loss  3.00 | ppl    20.01\n","| epoch  20 |   600/ 1110 batches | lr 20.00 | ms/batch 174.08 | loss  2.94 | ppl    18.90\n","| epoch  20 |   800/ 1110 batches | lr 20.00 | ms/batch 175.76 | loss  2.96 | ppl    19.37\n","| epoch  20 |  1000/ 1110 batches | lr 20.00 | ms/batch 176.16 | loss  2.97 | ppl    19.48\n","-----------------------------------------------------------------------------------------\n","| end of epoch  20 | time: 223.42s | valid loss  3.20 | valid ppl    24.55\n","-----------------------------------------------------------------------------------------\n","| epoch  21 |   200/ 1110 batches | lr 20.00 | ms/batch 177.35 | loss  2.97 | ppl    19.42\n","| epoch  21 |   400/ 1110 batches | lr 20.00 | ms/batch 175.08 | loss  2.93 | ppl    18.73\n","| epoch  21 |   600/ 1110 batches | lr 20.00 | ms/batch 174.43 | loss  2.87 | ppl    17.57\n","| epoch  21 |   800/ 1110 batches | lr 20.00 | ms/batch 175.80 | loss  2.88 | ppl    17.90\n","| epoch  21 |  1000/ 1110 batches | lr 20.00 | ms/batch 176.37 | loss  2.90 | ppl    18.14\n","-----------------------------------------------------------------------------------------\n","| end of epoch  21 | time: 223.49s | valid loss  3.09 | valid ppl    21.89\n","-----------------------------------------------------------------------------------------\n","| epoch  22 |   200/ 1110 batches | lr 20.00 | ms/batch 177.17 | loss  2.90 | ppl    18.20\n","| epoch  22 |   400/ 1110 batches | lr 20.00 | ms/batch 175.59 | loss  2.86 | ppl    17.55\n","| epoch  22 |   600/ 1110 batches | lr 20.00 | ms/batch 174.91 | loss  2.81 | ppl    16.65\n","| epoch  22 |   800/ 1110 batches | lr 20.00 | ms/batch 175.36 | loss  2.83 | ppl    16.87\n","| epoch  22 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.54 | loss  2.84 | ppl    17.16\n","-----------------------------------------------------------------------------------------\n","| end of epoch  22 | time: 223.39s | valid loss  3.03 | valid ppl    20.67\n","-----------------------------------------------------------------------------------------\n","| epoch  23 |   200/ 1110 batches | lr 20.00 | ms/batch 176.87 | loss  2.84 | ppl    17.05\n","| epoch  23 |   400/ 1110 batches | lr 20.00 | ms/batch 175.49 | loss  2.80 | ppl    16.42\n","| epoch  23 |   600/ 1110 batches | lr 20.00 | ms/batch 174.63 | loss  2.75 | ppl    15.63\n","| epoch  23 |   800/ 1110 batches | lr 20.00 | ms/batch 175.19 | loss  2.78 | ppl    16.04\n","| epoch  23 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.54 | loss  2.78 | ppl    16.10\n","-----------------------------------------------------------------------------------------\n","| end of epoch  23 | time: 223.30s | valid loss  2.98 | valid ppl    19.68\n","-----------------------------------------------------------------------------------------\n","| epoch  24 |   200/ 1110 batches | lr 20.00 | ms/batch 176.65 | loss  2.79 | ppl    16.23\n","| epoch  24 |   400/ 1110 batches | lr 20.00 | ms/batch 176.51 | loss  2.74 | ppl    15.53\n","| epoch  24 |   600/ 1110 batches | lr 20.00 | ms/batch 175.44 | loss  2.69 | ppl    14.66\n","| epoch  24 |   800/ 1110 batches | lr 20.00 | ms/batch 174.65 | loss  2.72 | ppl    15.23\n","| epoch  24 |  1000/ 1110 batches | lr 20.00 | ms/batch 174.72 | loss  2.73 | ppl    15.29\n","-----------------------------------------------------------------------------------------\n","| end of epoch  24 | time: 223.24s | valid loss  2.81 | valid ppl    16.54\n","-----------------------------------------------------------------------------------------\n","| epoch  25 |   200/ 1110 batches | lr 20.00 | ms/batch 177.16 | loss  2.74 | ppl    15.47\n","| epoch  25 |   400/ 1110 batches | lr 20.00 | ms/batch 175.95 | loss  2.69 | ppl    14.79\n","| epoch  25 |   600/ 1110 batches | lr 20.00 | ms/batch 174.77 | loss  2.64 | ppl    14.06\n","| epoch  25 |   800/ 1110 batches | lr 20.00 | ms/batch 175.14 | loss  2.67 | ppl    14.47\n","| epoch  25 |  1000/ 1110 batches | lr 20.00 | ms/batch 175.32 | loss  2.68 | ppl    14.65\n","-----------------------------------------------------------------------------------------\n","| end of epoch  25 | time: 223.29s | valid loss  2.88 | valid ppl    17.88\n","-----------------------------------------------------------------------------------------\n","| epoch  26 |   200/ 1110 batches | lr 5.00 | ms/batch 177.37 | loss  2.63 | ppl    13.89\n","| epoch  26 |   400/ 1110 batches | lr 5.00 | ms/batch 175.75 | loss  2.50 | ppl    12.13\n","| epoch  26 |   600/ 1110 batches | lr 5.00 | ms/batch 174.56 | loss  2.41 | ppl    11.11\n","| epoch  26 |   800/ 1110 batches | lr 5.00 | ms/batch 174.37 | loss  2.42 | ppl    11.23\n","| epoch  26 |  1000/ 1110 batches | lr 5.00 | ms/batch 174.53 | loss  2.39 | ppl    10.90\n","-----------------------------------------------------------------------------------------\n","| end of epoch  26 | time: 222.83s | valid loss  2.64 | valid ppl    13.97\n","-----------------------------------------------------------------------------------------\n","| epoch  27 |   200/ 1110 batches | lr 5.00 | ms/batch 176.95 | loss  2.45 | ppl    11.54\n","| epoch  27 |   400/ 1110 batches | lr 5.00 | ms/batch 175.44 | loss  2.37 | ppl    10.75\n","| epoch  27 |   600/ 1110 batches | lr 5.00 | ms/batch 174.18 | loss  2.30 | ppl     9.98\n","| epoch  27 |   800/ 1110 batches | lr 5.00 | ms/batch 174.74 | loss  2.32 | ppl    10.13\n","| epoch  27 |  1000/ 1110 batches | lr 5.00 | ms/batch 175.25 | loss  2.32 | ppl    10.13\n","-----------------------------------------------------------------------------------------\n","| end of epoch  27 | time: 222.92s | valid loss  2.54 | valid ppl    12.71\n","-----------------------------------------------------------------------------------------\n","| epoch  28 |   200/ 1110 batches | lr 5.00 | ms/batch 176.54 | loss  2.37 | ppl    10.69\n","| epoch  28 |   400/ 1110 batches | lr 5.00 | ms/batch 175.84 | loss  2.30 | ppl     9.99\n","| epoch  28 |   600/ 1110 batches | lr 5.00 | ms/batch 174.51 | loss  2.23 | ppl     9.32\n","| epoch  28 |   800/ 1110 batches | lr 5.00 | ms/batch 174.97 | loss  2.25 | ppl     9.50\n","| epoch  28 |  1000/ 1110 batches | lr 5.00 | ms/batch 175.05 | loss  2.26 | ppl     9.59\n","-----------------------------------------------------------------------------------------\n","| end of epoch  28 | time: 222.94s | valid loss  2.53 | valid ppl    12.60\n","-----------------------------------------------------------------------------------------\n","| epoch  29 |   200/ 1110 batches | lr 5.00 | ms/batch 176.83 | loss  2.31 | ppl    10.08\n","| epoch  29 |   400/ 1110 batches | lr 5.00 | ms/batch 175.40 | loss  2.25 | ppl     9.51\n","| epoch  29 |   600/ 1110 batches | lr 5.00 | ms/batch 174.35 | loss  2.18 | ppl     8.89\n","| epoch  29 |   800/ 1110 batches | lr 5.00 | ms/batch 174.23 | loss  2.21 | ppl     9.13\n","| epoch  29 |  1000/ 1110 batches | lr 5.00 | ms/batch 174.63 | loss  2.21 | ppl     9.13\n","-----------------------------------------------------------------------------------------\n","| end of epoch  29 | time: 222.97s | valid loss  2.48 | valid ppl    11.97\n","-----------------------------------------------------------------------------------------\n","| epoch  30 |   200/ 1110 batches | lr 5.00 | ms/batch 176.02 | loss  2.26 | ppl     9.62\n","| epoch  30 |   400/ 1110 batches | lr 5.00 | ms/batch 176.12 | loss  2.21 | ppl     9.07\n","| epoch  30 |   600/ 1110 batches | lr 5.00 | ms/batch 174.81 | loss  2.14 | ppl     8.52\n","| epoch  30 |   800/ 1110 batches | lr 5.00 | ms/batch 174.45 | loss  2.17 | ppl     8.74\n","| epoch  30 |  1000/ 1110 batches | lr 5.00 | ms/batch 174.79 | loss  2.18 | ppl     8.85\n","-----------------------------------------------------------------------------------------\n","| end of epoch  30 | time: 222.81s | valid loss  2.43 | valid ppl    11.41\n","-----------------------------------------------------------------------------------------\n","| epoch  31 |   200/ 1110 batches | lr 5.00 | ms/batch 177.90 | loss  2.23 | ppl     9.27\n","| epoch  31 |   400/ 1110 batches | lr 5.00 | ms/batch 175.52 | loss  2.16 | ppl     8.71\n","| epoch  31 |   600/ 1110 batches | lr 5.00 | ms/batch 174.31 | loss  2.11 | ppl     8.26\n","| epoch  31 |   800/ 1110 batches | lr 5.00 | ms/batch 174.69 | loss  2.13 | ppl     8.45\n","| epoch  31 |  1000/ 1110 batches | lr 5.00 | ms/batch 176.27 | loss  2.15 | ppl     8.55\n","-----------------------------------------------------------------------------------------\n","| end of epoch  31 | time: 223.26s | valid loss  2.39 | valid ppl    10.91\n","-----------------------------------------------------------------------------------------\n","| epoch  32 |   200/ 1110 batches | lr 5.00 | ms/batch 177.65 | loss  2.18 | ppl     8.87\n","| epoch  32 |   400/ 1110 batches | lr 5.00 | ms/batch 174.82 | loss  2.13 | ppl     8.42\n","| epoch  32 |   600/ 1110 batches | lr 5.00 | ms/batch 174.10 | loss  2.08 | ppl     7.98\n","| epoch  32 |   800/ 1110 batches | lr 5.00 | ms/batch 174.94 | loss  2.11 | ppl     8.22\n","| epoch  32 |  1000/ 1110 batches | lr 5.00 | ms/batch 175.27 | loss  2.11 | ppl     8.27\n","-----------------------------------------------------------------------------------------\n","| end of epoch  32 | time: 223.29s | valid loss  2.36 | valid ppl    10.58\n","-----------------------------------------------------------------------------------------\n","| epoch  33 |   200/ 1110 batches | lr 5.00 | ms/batch 177.62 | loss  2.15 | ppl     8.58\n","| epoch  33 |   400/ 1110 batches | lr 5.00 | ms/batch 175.05 | loss  2.10 | ppl     8.16\n","| epoch  33 |   600/ 1110 batches | lr 5.00 | ms/batch 174.12 | loss  2.04 | ppl     7.72\n","| epoch  33 |   800/ 1110 batches | lr 5.00 | ms/batch 174.35 | loss  2.07 | ppl     7.94\n","| epoch  33 |  1000/ 1110 batches | lr 5.00 | ms/batch 175.64 | loss  2.08 | ppl     8.01\n","-----------------------------------------------------------------------------------------\n","| end of epoch  33 | time: 222.94s | valid loss  2.32 | valid ppl    10.21\n","-----------------------------------------------------------------------------------------\n","| epoch  34 |   200/ 1110 batches | lr 5.00 | ms/batch 177.11 | loss  2.12 | ppl     8.36\n","| epoch  34 |   400/ 1110 batches | lr 5.00 | ms/batch 174.96 | loss  2.08 | ppl     8.00\n","| epoch  34 |   600/ 1110 batches | lr 5.00 | ms/batch 174.30 | loss  2.02 | ppl     7.52\n","| epoch  34 |   800/ 1110 batches | lr 5.00 | ms/batch 175.96 | loss  2.05 | ppl     7.76\n","| epoch  34 |  1000/ 1110 batches | lr 5.00 | ms/batch 175.49 | loss  2.06 | ppl     7.86\n","-----------------------------------------------------------------------------------------\n","| end of epoch  34 | time: 223.17s | valid loss  2.30 | valid ppl    10.00\n","-----------------------------------------------------------------------------------------\n","| epoch  35 |   200/ 1110 batches | lr 5.00 | ms/batch 178.32 | loss  2.09 | ppl     8.09\n","| epoch  35 |   400/ 1110 batches | lr 5.00 | ms/batch 173.91 | loss  2.05 | ppl     7.73\n","| epoch  35 |   600/ 1110 batches | lr 5.00 | ms/batch 175.16 | loss  1.99 | ppl     7.32\n","| epoch  35 |   800/ 1110 batches | lr 5.00 | ms/batch 176.32 | loss  2.01 | ppl     7.45\n","| epoch  35 |  1000/ 1110 batches | lr 5.00 | ms/batch 174.38 | loss  2.04 | ppl     7.70\n","-----------------------------------------------------------------------------------------\n","| end of epoch  35 | time: 223.32s | valid loss  2.26 | valid ppl     9.56\n","-----------------------------------------------------------------------------------------\n","| epoch  36 |   200/ 1110 batches | lr 5.00 | ms/batch 176.97 | loss  2.06 | ppl     7.84\n","| epoch  36 |   400/ 1110 batches | lr 5.00 | ms/batch 175.47 | loss  2.02 | ppl     7.52\n","| epoch  36 |   600/ 1110 batches | lr 5.00 | ms/batch 174.43 | loss  1.96 | ppl     7.13\n","| epoch  36 |   800/ 1110 batches | lr 5.00 | ms/batch 175.51 | loss  2.00 | ppl     7.37\n","| epoch  36 |  1000/ 1110 batches | lr 5.00 | ms/batch 176.19 | loss  2.01 | ppl     7.44\n","-----------------------------------------------------------------------------------------\n","| end of epoch  36 | time: 223.73s | valid loss  2.22 | valid ppl     9.22\n","-----------------------------------------------------------------------------------------\n","| epoch  37 |   200/ 1110 batches | lr 5.00 | ms/batch 177.53 | loss  2.03 | ppl     7.65\n","| epoch  37 |   400/ 1110 batches | lr 5.00 | ms/batch 175.16 | loss  1.99 | ppl     7.33\n","| epoch  37 |   600/ 1110 batches | lr 5.00 | ms/batch 174.83 | loss  1.94 | ppl     6.97\n","| epoch  37 |   800/ 1110 batches | lr 5.00 | ms/batch 175.23 | loss  1.97 | ppl     7.14\n","| epoch  37 |  1000/ 1110 batches | lr 5.00 | ms/batch 175.39 | loss  1.99 | ppl     7.32\n","-----------------------------------------------------------------------------------------\n","| end of epoch  37 | time: 223.30s | valid loss  2.20 | valid ppl     9.01\n","-----------------------------------------------------------------------------------------\n","| epoch  38 |   200/ 1110 batches | lr 5.00 | ms/batch 177.08 | loss  2.01 | ppl     7.48\n","| epoch  38 |   400/ 1110 batches | lr 5.00 | ms/batch 174.94 | loss  1.98 | ppl     7.21\n","| epoch  38 |   600/ 1110 batches | lr 5.00 | ms/batch 174.43 | loss  1.92 | ppl     6.81\n","| epoch  38 |   800/ 1110 batches | lr 5.00 | ms/batch 175.69 | loss  1.94 | ppl     6.98\n","| epoch  38 |  1000/ 1110 batches | lr 5.00 | ms/batch 175.99 | loss  1.97 | ppl     7.14\n","-----------------------------------------------------------------------------------------\n","| end of epoch  38 | time: 223.35s | valid loss  2.16 | valid ppl     8.64\n","-----------------------------------------------------------------------------------------\n","| epoch  39 |   200/ 1110 batches | lr 5.00 | ms/batch 176.45 | loss  1.99 | ppl     7.30\n","| epoch  39 |   400/ 1110 batches | lr 5.00 | ms/batch 175.80 | loss  1.96 | ppl     7.09\n","| epoch  39 |   600/ 1110 batches | lr 5.00 | ms/batch 174.83 | loss  1.89 | ppl     6.63\n","| epoch  39 |   800/ 1110 batches | lr 5.00 | ms/batch 174.76 | loss  1.92 | ppl     6.83\n","| epoch  39 |  1000/ 1110 batches | lr 5.00 | ms/batch 174.92 | loss  1.95 | ppl     7.00\n","-----------------------------------------------------------------------------------------\n","| end of epoch  39 | time: 222.87s | valid loss  2.14 | valid ppl     8.48\n","-----------------------------------------------------------------------------------------\n","| epoch  40 |   200/ 1110 batches | lr 5.00 | ms/batch 177.22 | loss  1.97 | ppl     7.18\n","| epoch  40 |   400/ 1110 batches | lr 5.00 | ms/batch 175.30 | loss  1.93 | ppl     6.89\n","| epoch  40 |   600/ 1110 batches | lr 5.00 | ms/batch 174.34 | loss  1.87 | ppl     6.48\n","| epoch  40 |   800/ 1110 batches | lr 5.00 | ms/batch 174.64 | loss  1.90 | ppl     6.68\n","| epoch  40 |  1000/ 1110 batches | lr 5.00 | ms/batch 174.85 | loss  1.92 | ppl     6.80\n","-----------------------------------------------------------------------------------------\n","| end of epoch  40 | time: 222.83s | valid loss  2.10 | valid ppl     8.15\n","-----------------------------------------------------------------------------------------\n","=========================================================================================\n","| End of training | test loss  2.27 | test ppl     9.69\n","=========================================================================================\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6hWCyCaNhnhH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}