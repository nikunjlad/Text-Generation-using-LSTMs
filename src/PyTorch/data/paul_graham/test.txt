
Appendix: Defining Spam

I think there is a rough
consensus on what spam is, but it would be useful to have
an explicit definition.  We'll need to do this if we want to establish
a central corpus of spam, or even to compare spam filtering
rates meaningfully.

To start with, spam is not unsolicited commercial email.
If someone in my neighborhood heard that I was looking for an old
Raleigh three-speed in good condition, and sent me an email
offering to sell me one, I'd be delighted, and yet this
email would be both commercial and unsolicited.  The
defining feature of spam (in fact, its raison d'etre)
is not that it is unsolicited, but that it is automated.

It is merely incidental, too, that spam is usually commercial.
If someone started sending mass email to support some political
cause, for example, it would be just as much spam as email
promoting a porn site.

I propose we define spam as unsolicited automated email.
This definition thus includes some email
that many legal definitions of spam don't.  Legal definitions
of spam, influenced presumably by lobbyists, tend to exclude
mail sent by companies that have an "existing relationship" with
the recipient.  But buying something from a company, for
example, does not imply that you have solicited
ongoing email from them.
If I order something from an online
store, and they then send me a stream of spam, it's still
spam.

Companies sending spam often give you a way to "unsubscribe,"
or ask you to go to their site and change your "account
preferences" if you want to stop getting spam.  This is
not enough to stop the mail from being spam.  Not opting out
is not the same as opting in.  Unless the   
recipient explicitly checked a clearly labelled box (whose
default was no) asking to receive the email, then it is spam.

In some business relationships, you do implicitly solicit
certain kinds of mail.   When you order online, I think you
implicitly solicit a receipt, and notification when the
order ships.
I don't mind when Verisign sends me mail warning that
a domain name is about to expire (at least, if they are the
actual 
registrar for it).  But when Verisign sends me
email offering a FREE Guide to Building My
E-Commerce Web Site, that's spam.


Notes:

[1] The examples in this article are translated
into Common Lisp for, believe it or not, greater accessibility.
The application described here is one that we wrote in order to
test a new Lisp dialect called Arc that is 
not yet released.

[2] Currently the lowest rate seems to be about $200 to send a million spams.
That's very cheap, 1/50th of a cent per spam.
But filtering out 95%
of spam, for example, would increase the spammers' cost to reach
a given audience by a factor of 20.  Few can have
margins big enough to absorb that.

[3] As a rule of thumb, the more qualifiers there are before the
name of a country, the more corrupt the rulers.  A
country called The Socialist People's Democratic Republic
of X is probably the last place in the world you'd want to live.


Thanks to Sarah Harlin for reading drafts of this; Daniel Giffin (who is 
also writing the production Arc interpreter) for several good ideas about
filtering and for creating our mail infrastructure; Robert Morris,
Trevor Blackwell and Erann Gat for many discussions about spam; Raph 
Levien for advice about trust metrics;  and Chip Coldwell 
and Sam Steingold for advice about statistics.



You'll find this essay and 14 others in
Hackers & Painters.





More Info:


Want to start a startup?  Get funded by
Y Combinator.





May 2002





"We were after the C++ programmers. We managed to drag a 
lot of them about halfway to Lisp."

- Guy Steele, co-author of the Java spec






In the software business there is an ongoing
struggle between the pointy-headed academics, and another
equally formidable force, the pointy-haired bosses.  Everyone
knows who the pointy-haired boss is, right?  I think most
people in the technology world not only recognize this
cartoon character, but know the actual person in their company
that he is modelled upon.

The pointy-haired boss miraculously combines two qualities
that are common by themselves, but rarely seen together:
(a) he knows nothing whatsoever about technology, and
(b) he has very strong opinions about it.

Suppose, for example, you need to write a piece of software.
The pointy-haired boss has no idea how this software
has to work, and can't tell one programming language from
another, and yet he knows what language you should write it in.
Exactly.  He thinks you should write it in Java.

Why does he think this?  Let's
take a look inside the brain of the pointy-haired boss.  What
he's thinking is something like this.  Java is a standard.
I know it must be, because I read about it in the press all the time.
Since it is a standard, I won't get in trouble for using it.
And that also means there will always be lots of Java programmers,
so if the programmers working for me now quit, as programmers
working for me mysteriously always do, I can easily replace
them.

Well, this doesn't sound that unreasonable.  But it's all
based on one unspoken assumption, and that assumption
turns out to be false.  The pointy-haired boss believes that all
programming languages are pretty much equivalent.
If that were true, he would be right on
target.  If languages are all equivalent, sure, use whatever 
language everyone else is using.

But all languages are not equivalent, and I think I can prove
this to you without even getting into the differences between them.
If you asked the pointy-haired boss in 1992 what language     
software should be written in, he would have answered with as
little hesitation as he does today.  Software should be  
written in C++.  But if languages are all equivalent, why should the
pointy-haired boss's opinion ever change?  In fact, why should
the developers of Java have even bothered to create a new
language?

Presumably, if you create a new language, it's because you think
it's better in some way than what people already had.  And in fact, Gosling
makes it clear in the first Java white paper that Java
was designed to fix some problems with C++.
So there you have it: languages are not all equivalent.
If you follow the
trail through the pointy-haired boss's brain to Java and then
back through Java's history to its origins, you end up holding
an idea that contradicts the assumption you started with.

So, who's right?  James Gosling, or the pointy-haired boss?
Not surprisingly, Gosling is right.  Some languages are better,
for certain problems, than others.  And you know, that raises some
interesting questions.  Java was designed to be better, for certain
problems, than C++.  What problems?  When is Java better and 
when is C++?  Are there situations where other languages are
better than either of them?

Once you start considering this question, you have opened a
real can of worms.  If the pointy-haired boss had to think
about the problem in its full complexity, it would make his
brain explode.  As long as he considers all languages   
equivalent, all he has to do is choose the one
that seems to have the most momentum, and since that is more
a question of fashion than technology, even he
can probably get the right answer.
But if languages vary, he suddenly
has to solve two simultaneous equations, trying to find
an optimal balance between two things he knows nothing   
about: the relative suitability of the twenty or so leading
languages for the problem he needs to solve, and the odds of
finding programmers, libraries, etc. for each.
If that's what's on the other side of the door, it
is no surprise that the pointy-haired boss doesn't want to open it.

The disadvantage of believing that all programming languages
are equivalent is that it's not true.  But the advantage is 
that it makes your life a lot simpler.
And I think that's the main reason the idea is so widespread.
It is a comfortable idea.

We know that Java must be pretty good, because it is the
cool, new programming language.  Or is it?  If you look at the world of
programming languages from a distance, it looks like Java is
the latest thing.  (From far enough away, all you can see is
the large, flashing billboard paid for by Sun.)
But if you look at this world
up close, you find that there are degrees of coolness.  Within
the hacker subculture, there is another language called Perl
that is considered a lot cooler than Java.  Slashdot, for
example, is generated by Perl.  I don't think you would find
those guys using Java Server Pages.  But there is another,
newer language, called Python, whose users tend to look down on Perl,
and more waiting in the wings.

If you look at these languages in order, Java, Perl, Python,
you notice an interesting pattern.  At least, you notice this
pattern if you are a Lisp hacker.  Each one is progressively 
more like Lisp.  Python copies even features
that many Lisp hackers consider to be mistakes.
You could translate simple Lisp programs into Python line for line.
It's 2002, and programming languages have almost caught up 
with 1958.

Catching Up with Math

What I mean is that
Lisp was first discovered by John McCarthy in 1958,
and popular programming languages are only now
catching up with the ideas he developed then.

Now, how could that be true?  Isn't computer technology something
that changes very rapidly?  I mean, in 1958, computers were
refrigerator-sized behemoths with the processing power of    
a wristwatch.  How could any technology that old even be
relevant, let alone superior to the latest developments?

I'll tell you how.  It's because Lisp was not really
designed to be a programming language, at least not in the sense
we mean today.  What we mean by a programming language is
something we use to tell a computer what to do.   McCarthy
did eventually intend to develop a programming language in
this sense, but the Lisp that we actually ended up with was based
on something separate that he did as a 
theoretical exercise-- an effort
to define a more convenient alternative to the Turing Machine.
As McCarthy said later,

Another way to show that Lisp was neater than Turing machines
was to write a universal Lisp function
and show that it is briefer and more comprehensible than the
description of a universal Turing machine.
This was the Lisp function eval..., 
which computes the value of
a Lisp expression....
Writing eval required inventing a notation representing Lisp
functions as Lisp data, and such a notation
was devised for the purposes of the paper with no thought that
it would be used to express Lisp programs in practice.

What happened next was that, some time in late 1958, Steve Russell,
one of McCarthy's
grad students, looked at this definition of eval and realized  
that if he translated it into machine language, the result
would be a Lisp interpreter.

This was a big surprise at the time.
Here is what McCarthy said about it later in an interview:

Steve Russell said, look, why don't I program this eval..., and
I said to him, ho, ho, you're confusing theory with practice,
this eval is intended for reading, not for
computing. But he went ahead and did it. That is, he compiled the eval
in my paper into [IBM] 704 machine
code, fixing bugs, and then advertised this as a Lisp interpreter,
which it certainly was. So at that point Lisp
had essentially the form that it has today....

Suddenly, in a matter of weeks I think, McCarthy found his theoretical
exercise transformed into an actual programming language-- and a
more powerful one than he had intended.

So the short explanation of why this 1950s language is not
obsolete is that it was not technology but math, and
math doesn't get stale.   The right thing to compare Lisp
to is not 1950s hardware, but, say, the Quicksort
algorithm, which was discovered in 1960 and is still
the fastest general-purpose sort.

There is one other language still
surviving from the 1950s, Fortran, and it represents the
opposite approach to language design.  Lisp was a
piece of theory that unexpectedly got turned into a
programming language.  Fortran was developed intentionally as
a programming language, but what we would now consider a
very low-level one.

Fortran I, the language that was
developed in 1956, was a very different animal from present-day
Fortran.   Fortran I was pretty much assembly
language with math.  In some ways it was less
powerful than more recent assembly languages; there were no   
subroutines, for example, only branches.
Present-day Fortran is now arguably closer to Lisp than to
Fortran I.

Lisp and Fortran were the trunks of two separate evolutionary trees, 
one rooted in math and one rooted in machine architecture.
These two trees have been converging ever since.
Lisp started out powerful, and over the next twenty years
got fast.  So-called mainstream languages started out
fast, and over the next forty years gradually got more powerful,
until now the most advanced
of them are fairly close to Lisp.
Close, but they are still missing a few things....

What Made Lisp Different

When it was first developed, Lisp embodied nine new
ideas.  Some of these we now take for granted, others are
only seen in more advanced languages, and two are still
unique to Lisp.  The nine ideas are, in order of their
adoption by the mainstream,

 Conditionals.  A conditional is an if-then-else
construct.  We take these for granted now, but Fortran I
didn't have them. It had only a conditional goto
closely based on the underlying machine instruction.

 A function type. In Lisp, functions are
a data type just like integers or strings.
They have a literal representation, can be stored in variables,
can be passed as arguments, and so on.

 Recursion.  Lisp was the first programming language to
support it.

 Dynamic typing.  In Lisp, all variables
are effectively pointers. Values are what
have types, not variables, and assigning or binding
variables means copying pointers, not what they point to.

 Garbage-collection.

 Programs composed of expressions.  Lisp programs are
trees of expressions, each of which returns a value.
This is in contrast to Fortran
and most succeeding languages, which distinguish between
expressions and statements.

It was natural to have this
distinction in Fortran I because
you could not nest statements.  And
so while you needed expressions for math to work, there was
no point in making anything else return a value, because
there could not be anything waiting for it.

This limitation
went away with the arrival of block-structured languages,
but by then it was too late. The distinction between
expressions and statements was entrenched.  It spread from
Fortran into Algol and then to both their descendants.

 A symbol type.  Symbols are effectively pointers to strings
stored in a hash table.  So
you can test equality by comparing a pointer,
instead of comparing each character.

 A notation for code using trees of symbols and constants.

 The whole language there all the time.  There is
no real distinction between read-time, compile-time, and runtime.
You can compile or run code while reading, read or run code
while compiling, and read or compile code at runtime.

Running code at read-time lets users reprogram Lisp's syntax;
running code at compile-time is the basis of macros; compiling
at runtime is the basis of Lisp's use as an extension
language in programs like Emacs; and reading at runtime
enables programs to communicate using s-expressions, an
idea recently reinvented as XML.

When Lisp first appeared, these ideas were far
removed from ordinary programming practice, which was
dictated largely by the hardware available in the late 1950s.
Over time, the default language, embodied
in a succession of popular languages, has
gradually evolved toward Lisp.  Ideas 1-5 are now widespread.
Number 6 is starting to appear in the mainstream.  
Python has a form of 7, though there doesn't seem to be    
any syntax for it.

As for number 8, this may be the most interesting of the
lot.  Ideas 8 and 9 only became part of Lisp
by accident, because Steve Russell implemented
something McCarthy had never intended to be implemented.
And yet these ideas turn out to be responsible for
both Lisp's strange appearance and its most distinctive
features.  Lisp looks strange not so much because
it has a strange syntax as because it has no syntax;
you express programs directly in the parse trees that
get built behind the scenes when other languages are
parsed, and these trees are made
of lists, which are Lisp data structures.

Expressing the language in its own data structures turns
out to be a very powerful feature. Ideas 8 and 9
together mean that you
can write programs that write programs.  That may sound
like a bizarre idea, but it's an everyday thing in Lisp. 
The most common way to do it is with something called a        
macro.

The term "macro" does not mean in Lisp what it means in other
languages.
A Lisp macro can be anything from an abbreviation
to a compiler for a new language.
If you want to really understand Lisp,
or just expand your programming horizons, I would 
learn more about macros.

Macros (in the Lisp sense) are still, as far as
I know, unique to Lisp.
This is partly because in order to have macros you
probably have to make your language look as strange as
Lisp.  It may also be because if you do add that final
increment of power, you can no
longer claim to have invented a new language, but only
a new dialect of Lisp.

I mention this mostly
as a joke, but it is quite true. If you define
a language that has car, cdr, cons, quote, cond, atom,
eq, and
a notation for functions expressed as lists, then you
can build all the rest of Lisp out of it.  That is in
fact the defining quality of Lisp: it was in order to
make this so that McCarthy gave Lisp the shape it has.

Where Languages Matter

So suppose Lisp does represent a kind of limit     
that mainstream languages are approaching asymptotically-- does
that mean you should actually use it to write software?
How much do you lose by using a less powerful language?
Isn't it wiser, sometimes, not to be
at the very edge of innovation?
And isn't popularity to some extent
its own justification?  Isn't the pointy-haired boss right,
for example, to want to use a language for which he can easily
hire programmers?

There are, of course, projects where the choice of programming
language doesn't matter much.  As a
rule, the more demanding the application, the more
leverage you get from using a powerful language.  But
plenty of projects are not demanding at all.
Most programming probably consists of writing 
little glue programs, and for 
little glue programs you
can use any language that you're already
familiar with and that has good libraries for whatever you
need to do.  If you just need to feed data from one   
Windows app to another, sure, use Visual Basic.

You can write little glue programs in Lisp too
(I use it as a desktop calculator), but the biggest win
for languages like Lisp is at the other end of
the spectrum, where you need to write sophisticated
programs to solve hard problems in the face of fierce competition.
A good example is the
airline fare search program that ITA Software licenses to
Orbitz.  These
guys entered a market already dominated by two big,
entrenched competitors, Travelocity and Expedia, and  
seem to have just humiliated them technologically.

The core of ITA's application is a 200,000 line Common Lisp program
that searches many orders of magnitude more possibilities
than their competitors, who apparently
are still using mainframe-era programming techniques.
(Though ITA is also in a sense
using a mainframe-era programming language.)
I have never seen any of ITA's code, but according to
one of their top hackers they use a lot of macros,
and I am not surprised to hear it.

Centripetal Forces

I'm not saying there is no cost to using uncommon  
technologies.  The pointy-haired boss is not completely
mistaken to worry about this.  But because he doesn't understand
the risks, he tends to magnify them.

I can think of three problems that could arise from using
less common languages.  Your programs might not work well with
programs written in other languages.  You might have fewer
libraries at your disposal.  And you might have trouble
hiring programmers.

How much of a problem is each of these?  The importance of
the first varies depending on whether you have control
over the whole system.  If you're writing software that has
to run on a remote user's machine on top of a buggy,
closed operating system (I mention no names), there may be
advantages to writing your application in the
same language as the OS.
But if you control the whole system and
have the source code of all the parts, as ITA presumably does, you
can use whatever languages you want.  If
any incompatibility arises, you can fix it yourself.

In server-based applications you can
get away with using the most advanced technologies,
and I think this is the main
cause of what Jonathan Erickson calls the "programming language
renaissance."  This is why we even hear about new
languages like Perl and Python.  We're not hearing about these
languages because people are using them to write Windows
apps, but because people are using them on servers.  And as
software shifts 
off the desktop and onto servers (a future even
Microsoft seems resigned to), there will be less
and less pressure to use middle-of-the-road technologies.

As for libraries, their importance also
depends on the application.  For less demanding problems,
the availability of libraries can outweigh the intrinsic power
of the language.  Where is the breakeven point?  Hard to say
exactly, but wherever it is, it is short of anything you'd
be likely to call an application.  If a company considers
itself to be in the software business, and they're writing
an application that will be one of their products,
then it will probably involve several hackers and take at
least six months to write.  In a project of that
size, powerful languages probably start to outweigh
the convenience of pre-existing libraries.

The third worry of the pointy-haired boss, the difficulty
of hiring programmers, I think is a red herring.   How many
hackers do you need to hire, after all?  Surely by now we
all know that software is best developed by teams of less
than ten people.   And you shouldn't have trouble hiring
hackers on that scale for any language anyone has ever heard
of.  If you can't find ten Lisp hackers, then your company is
probably based in the wrong city for developing software.

In fact, choosing a more powerful language probably decreases the
size of the team you need, because (a) if you use a more powerful
language you probably won't need as many hackers,
and (b) hackers who work in more advanced languages are likely
to be smarter.

I'm not saying that you won't get a lot of pressure to use
what are perceived as "standard" technologies.  At Viaweb
(now Yahoo Store),
we raised some eyebrows among VCs and potential acquirers by
using Lisp.  But we also raised eyebrows by using
generic Intel boxes as servers instead of
"industrial strength" servers like Suns, for using a
then-obscure open-source Unix variant called FreeBSD instead
of a real commercial OS like Windows NT, for ignoring
a supposed e-commerce standard called 
SET that no one now
even remembers, and so on.

You can't let the suits make technical decisions for you.
Did it
alarm some potential acquirers that we used Lisp?  Some, slightly,
but if we hadn't used Lisp, we wouldn't have been
able to write the software that made them want to buy us.
What seemed like an anomaly to them was in fact
cause and effect.

If you start a startup, don't design your product to please
VCs or potential acquirers.  Design your product to please
the users.  If you win the users, everything else will
follow.  And if you don't, no one will care
how comfortingly orthodox your technology choices were.

The Cost of Being Average

How much do you lose by using a less powerful language?  
There is actually some data out there about that.

The most convenient measure of power is probably 
code size.
The point of high-level
languages is to give you bigger abstractions-- bigger bricks,
as it were, so you don't need as many to build
a wall of a given size.
So the more powerful
the language, the shorter the program (not simply in
characters, of course, but in distinct elements).

How does a more powerful language enable you to write
shorter programs?  One technique you can use, if the language will
let you, is something called 
bottom-up programming.  Instead of
simply writing your application in the base language, you
build on top of the base language a language for writing
programs like yours, then write your program
in it. The combined code can be much shorter than if you
had written your whole program in the base language-- indeed,
this is how most compression algorithms work.
A bottom-up program should be easier to modify as well,  
because in many cases the language layer won't have to change
at all.

Code size is important, because the time it takes
to write a program depends mostly on its length.
If your program would be three times as long in another
language, it will take three times as long to write-- and
you can't get around this by hiring more people, because
beyond a certain size new hires are actually a net lose.
Fred Brooks described this phenomenon in his famous
book The Mythical Man-Month, and everything I've seen
has tended to confirm what he said.

So how much shorter are your programs if you write them in
Lisp?  Most of the numbers I've heard for Lisp
versus C, for example, have been around 7-10x.
But a recent article about ITA in 
New
Architect magazine said that
"one line of Lisp can replace 20 lines of C," and since
this article was full of quotes from ITA's president, I
assume they got this number from ITA.  If so then
we can put some faith in it; ITA's software includes a lot
of C and  C++ as well as Lisp, so they are speaking from
experience.

My guess is that these multiples aren't even constant.
I think they increase when
you face harder problems and also when you have smarter
programmers.  A really good hacker can squeeze more
out of better tools.

As one data point on the curve, at any rate,
if you were to compete with ITA and
chose to write your software in C, they would be able to develop
software twenty times faster than you.
If you spent a year on a new feature, they'd be able to
duplicate it in less than three weeks.  Whereas if they spent
just three months developing something new, it would be
five years before you had it too.

And you know what?  That's the best-case scenario.
When you talk about code-size ratios, you're implicitly assuming
that you can actually write the program in the weaker language.
But in fact there are limits on what programmers can do.
If you're trying to solve a hard problem with a language that's
too low-level, you reach a point where there is just too 
much to keep in your head at once.

So when I say it would take ITA's imaginary
competitor five years to duplicate something ITA could
write in Lisp in three months, I mean five years
if nothing goes wrong.  In fact, the way things work in 
most companies, any
development project that would take five years is
likely never to get finished at all.

I admit this is an extreme case.  ITA's hackers seem to
be unusually smart, and C is a pretty low-level language.
But in a competitive market, even a differential of two or
three to one would
be enough to guarantee that you'd always be behind.

A Recipe

This is the kind of possibility that the pointy-haired boss
doesn't even want to think about.  And so most of them don't.
Because, you know, when it comes down to it, the pointy-haired
boss doesn't mind if his company gets their ass kicked, so
long as no one can prove it's his fault.
The safest plan for him personally
is to stick close to the center of the herd.

Within large organizations, the phrase used to
describe this approach is "industry best practice."
Its purpose is to shield the pointy-haired
boss from responsibility: if he chooses
something that is "industry best practice," and the company
loses, he can't be blamed.  He didn't choose, the industry did.

I believe this term was originally used to describe
accounting methods and so on.  What it means, roughly,
is don't do anything weird.  And in accounting that's
probably a good idea.  The terms "cutting-edge" and  
"accounting" do not sound good together.  But when you import
this criterion into decisions about technology, you start
to get the wrong answers.

Technology often should be
cutting-edge.  In programming languages, as Erann Gat
has pointed out, what "industry best practice"  actually
gets you is not the best, but merely the
average.  When a decision causes you to develop software at
a fraction of the rate of more aggressive competitors,  
"best practice" is a misnomer.


So here we have two pieces of information that I think are
very valuable.  In fact, I know it from my own experience.
Number 1, languages vary in power.  Number 2, most managers
deliberately ignore this.  Between them, these two facts
are literally a recipe for making money.  ITA is an example
of this recipe in action.
If you want to win in a software
business, just take on the hardest problem you can find,
use the most powerful language you can get, and wait for
your competitors' pointy-haired bosses to revert to the mean.







Appendix: Power

As an illustration of what I mean about the relative power
of programming languages, consider the following problem.
We want to write a function that generates accumulators-- a
function that takes a number n, and
returns a function that takes another number i and
returns n incremented by i.

(That's incremented by, not plus.  An accumulator
has to accumulate.)

In Common Lisp this would be

(defun foo (n)
  (lambda (i) (incf n i)))

and in Perl 5,

sub foo {  
  my ($n) = @_;
  sub {$n += shift}
}

which has more elements than the Lisp version because
you have to extract parameters manually in Perl.

In Smalltalk the code is slightly longer than in Lisp

foo: n                              
  |s|                      
  s := n.                          
  ^[:i| s := s+i. ] 

because although in general lexical variables work, you can't
do an assignment to a parameter, so you have to create a
new variable s.

In Javascript the example is, again, slightly longer, because 
Javascript retains
the distinction between statements and
expressions, so you need explicit return statements
to return values:

function foo(n) { 
  return function (i) { 
           return n += i } }

(To be fair, Perl also retains
this distinction, but deals with it in typical Perl fashion
by letting you omit returns.)

If you try to translate the Lisp/Perl/Smalltalk/Javascript code into 
Python you run into some limitations.  Because Python
doesn't fully support lexical variables,
you have to create a data structure to hold the value of n.
And although
Python does have a function data type, there is no
literal representation for one (unless the body is
only a single expression) so you need to create a named
function to return.  This is what you end up with:

def foo(n):
  s = [n]
  def bar(i):
    s[0] += i
    return s[0] 
  return bar

Python users might legitimately ask why they can't
just write

def foo(n):
  return lambda i: return n += i

or even

def foo(n):
  lambda i: n += i

and my guess is that they probably will, one day.
(But if they don't want to wait for Python to evolve the rest
of the way into Lisp, they could always just...)


In OO languages, you can, to a limited extent, simulate
a closure (a function that refers to variables defined in
enclosing scopes) by defining a class with one method
and a field to replace each variable from an enclosing
scope.  This makes the programmer do the kind of code
analysis that would be done by the compiler in a language
with full support for lexical scope, and it won't work
if more than one function refers to the same variable,
but it is enough in simple cases like this.

Python experts seem to agree that this is the
preferred way to solve the problem in Python, writing
either

def foo(n):
  class acc:
    def __init__(self, s):
        self.s = s
    def inc(self, i):
        self.s += i
        return self.s
  return acc(n).inc

or

class foo:
  def __init__(self, n):
      self.n = n
  def __call__(self, i):
      self.n += i
      return self.n

I include these because I wouldn't want Python
advocates to say I was misrepresenting the language,   
but both seem to me more complex than the first   
version.  You're doing the same thing, setting up
a separate place to hold the accumulator; it's just
a field in an object instead of the head of a list.
And the use of these special,
reserved field names, especially __call__, seems
a bit of a hack.

In the rivalry between Perl and Python, the claim of the
Python hackers seems to be that
that Python is a more elegant alternative to Perl, but what
this case shows is that power is the ultimate elegance:
the Perl program is simpler (has fewer elements), even if the
syntax is a bit uglier.

How about other languages? In the other languages
mentioned in this talk-- Fortran, C, C++, Java, and
Visual Basic-- it is not clear whether you can actually
solve this problem.
Ken Anderson says that the following code is about as close
as you can get in Java:

public interface Inttoint {
  public int call(int i);
}


public static Inttoint foo(final int n) {
  return new Inttoint() {
    int s = n;
    public int call(int i) {
    s = s + i;
    return s;
    }};
}

This falls short of the spec because it only works for
integers.  After many email exchanges with Java hackers,
I would say that writing a properly polymorphic version
that behaves like the preceding examples is somewhere
between damned awkward and impossible.  If anyone wants to
write one I'd be very curious to see it, but I personally
have timed out.

It's not literally true that you can't solve this
problem in other languages, of course.  The fact
that all these languages are Turing-equivalent means
that, strictly speaking, you can write any program in
any of them.  So how would you do it?  In the limit case,
by writing a Lisp
interpreter in the less powerful language.

That sounds like a joke, but it happens so often to
varying degrees in large programming projects that
there is a name for the phenomenon, Greenspun's Tenth
Rule:

 Any sufficiently
     complicated C or Fortran program contains an ad hoc
     informally-specified bug-ridden slow implementation of half of
     Common Lisp.

If you try to solve a
hard problem, the question is not whether you will use
a powerful enough language, but whether you will (a)
use a powerful language, (b) write a de facto interpreter
for one, or (c) yourself become a human compiler for one.
We see this already
begining to happen in the Python example, where we are
in effect simulating the code that a compiler
would generate to implement a lexical variable.

This practice is not only common, but institutionalized.  For example,
in the OO world you hear a good deal about 
"patterns".
I wonder if these patterns are not sometimes evidence of case (c),
the human compiler, at work.  When I see patterns in my programs,
I consider it a sign of trouble.  The shape of a program
should reflect only the problem it needs to solve.
Any other regularity in the code is a sign, to me at
least, that I'm using abstractions that aren't powerful
enough-- often that I'm generating by hand the
expansions of some macro that I need to write.



Notes


 The IBM 704 CPU was about the size of a refrigerator,
but a lot heavier.  The CPU weighed 3150 pounds,
and the 4K of RAM was in a separate
box weighing another 4000 pounds.  The
Sub-Zero 690, one of the largest household refrigerators,
weighs 656 pounds.

 Steve Russell also wrote the first (digital) computer
game, Spacewar, in 1962.

 If you want to trick a pointy-haired boss into letting you
write software in Lisp, you could try telling him it's XML.

 Here is the accumulator generator in other Lisp dialects:

Scheme: (define (foo n) 
          (lambda (i) (set! n (+ n i)) n))
Goo:    (df foo (n) (op incf n _)))
Arc:    (def foo (n) [++ n _])

 Erann Gat's sad tale about
"industry best practice" at JPL inspired me to address
this generally misapplied phrase.

 Peter Norvig found that
16 of the 23 patterns in Design Patterns were 
"invisible
or simpler" in Lisp.

 Thanks to the many people who answered my questions about
various languages and/or read drafts of this, including
Ken Anderson, Trevor Blackwell, Erann Gat, Dan Giffin, Sarah Harlin,
Jeremy Hylton, Robert Morris, Peter Norvig, Guy Steele, and Anton
van Straaten.
They bear no blame for any opinions expressed.




Related:

Many people have responded to this talk,
so I have set up an additional page to deal with the issues they have
raised: Re: Revenge of the Nerds.

It also set off an extensive and often useful discussion on the 
LL1
mailing list.  See particularly the mail by Anton van Straaten on semantic
compression.

Some of the mail on LL1 led me to try to go deeper into the subject
of language power in Succinctness is Power.

A larger set of canonical implementations of the accumulator
generator benchmark are collected together on their own page.

Japanese Translation, Spanish
Translation, 
Chinese Translation
May 2002





"The quantity of meaning compressed into a small space by 
algebraic signs, is another circumstance that facilitates 
the reasonings we are accustomed to carry on by their aid."

- Charles Babbage, quoted in Iverson's Turing Award Lecture





In the discussion about issues raised by Revenge 
of the Nerds on the LL1 mailing list, Paul Prescod wrote
something that stuck in my mind.

Python's goal is regularity and readability, not succinctness.

On the face of it, this seems a rather damning thing to claim about a 
programming language.  As far as I can tell, succinctness = power.
If so, then substituting, we get

Python's goal is regularity and readability, not power.

and this doesn't seem a tradeoff (if it is a tradeoff)
that you'd want to make. 
It's not far from saying that Python's goal is not to be effective 
as a programming language.

Does succinctness = power?  This seems to me an important question,
maybe the most important question for anyone interested in
language design, and one that it would be useful to confront
directly.  I don't feel sure yet that the answer is a simple yes, but it seems 
a good hypothesis to begin with.

Hypothesis

My hypothesis is that succinctness is power, or is close enough
that except in pathological examples you can treat them as 
identical.

It seems to me that succinctness is what programming languages are 
for.  Computers would be just as happy to be told what to
do directly in machine language.  I think that the main
reason we take the trouble to develop high-level languages is to
get leverage, so that we can say (and more importantly, think)
in 10 lines of a high-level language what would require 1000
lines of machine language.  In other words,
the main point of high-level languages is to make source code smaller.

If smaller source code is the purpose of high-level languages, and
the power of something is how well it achieves its purpose, then
the measure of the power of a programming language is how small it
makes your programs.

Conversely, a language that doesn't make your programs small is
doing a bad job of what programming languages are supposed to
do, like a knife that doesn't cut well, or printing that's illegible.


Metrics

Small in what sense though?  The most common measure of code size is
lines of code.  But I think that this metric is the most common because
it is the easiest to measure.  I don't think anyone really believes
it is the true test of the length of a program. Different
languages have different conventions for how much you should put
on a line; in C a lot of lines have nothing on them but a delimiter or two.

Another easy test is the number of characters in a 
program, but this is not very good either; some languages (Perl,
for example) just
use shorter identifiers than others.

I think a better measure of the size of a program would be the 
number of elements, where an element is anything that
would be a distinct node if you drew a tree representing the 
source code. The name of
a variable or function is an element; 
an integer or a floating-point number is an element;
a segment of literal text is an element;
an element of a pattern, or a format directive, is an element;
a new block is an element.  There are borderline cases
(is -5 two elements or one?) but I think most of them are the
same for every language, so they don't affect comparisons much.

This metric needs fleshing out, and
it could require interpretation in the case of specific languages,
but I think it tries to measure the right thing, which is the 
number of parts a program has.  I think the tree you'd draw in this
exercise is what you have to make in your head in order to
conceive of the program, and so its size is proportionate to the
amount of work you have to do to write or read it.

Design

This kind of metric would allow us to compare different languages,
but that is not, at least for me, its main value.  The main value
of the succinctness test is as a guide in designing languages.
The most useful comparison between languages is between two
potential variants of the same language.  What can I do in the
language to make programs shorter?

If the conceptual load of
a program is proportionate to its complexity, and a given programmer
can tolerate a fixed conceptual load, then this is the same as asking,
what can I do to enable programmers to get the most done?  And
that seems to me identical to asking, how can I design a good
language?

(Incidentally, nothing makes it more patently obvious that the old
chestnut "all languages are equivalent" is false than designing
languages.  When you are designing a new language, you're constantly
comparing two languages-- the language if I did x, and if I didn't-- to
decide which is better.  If this were really a meaningless question,
you might as well flip a coin.)

Aiming for succinctness seems a good way to find new ideas.
If you can do something that makes many
different programs shorter, it is probably not a coincidence: you have 
probably discovered a useful new abstraction.  You might even be
able to write a program to help by searching
source code for repeated patterns.  Among other languages, those
with a reputation for succinctness would be the ones to look to for
new ideas: Forth, Joy, Icon.

Comparison

The first person to write about these issues, as far as I know, was
Fred Brooks in the Mythical Man Month.  He wrote
that programmers seemed to generate about the same
amount of code per day regardless of the language.
When I first read this in my early twenties,
it was a big surprise to me and seemed to have huge implications.
It meant that (a) the only way to get software written faster was to
use a more succinct language, and (b) someone who took the
trouble to do this could leave competitors who didn't in the dust.

Brooks' hypothesis, if it's true, seems to be at the very heart of hacking.
In the years since, I've paid close attention to any evidence I could
get on the question, from formal studies to anecdotes about individual
projects.   I have seen nothing to contradict him.

I have not yet seen evidence that seemed to me conclusive,
and I don't expect to.  Studies
like Lutz Prechelt's comparison of programming languages, while
generating the kind of results I expected, tend to use problems that
are too short to be meaningful tests.  A better test of a language is
what happens in programs that take a month to write.  And the only
real test, if you believe as I do that the main purpose of a language
is to be good to think in (rather than just to tell a computer what to
do once you've thought of it) is what new things you can write in it.
So any language comparison where
you have to meet a predefined spec is testing slightly the wrong
thing.

The true test of a language is how well you can discover
and solve new problems, not
how well you can use it to solve a problem someone else has
already formulated.  These two are quite different criteria.
In art, mediums like embroidery and mosaic work well if you
know beforehand what you want to make, but are absolutely lousy if
you don't.  When you want to discover the image as you make it--
as you have to do with anything as complex as an image of a
person, for example-- you need to use a more fluid medium like pencil or
ink wash or oil paint.  And indeed, the way tapestries and mosaics are made in
practice is to make a painting first, then copy it.  (The word
"cartoon" was originally used to describe a painting intended for
this purpose).

What this means is that we are never likely to have accurate comparisons
of the relative power of programming languages.  We'll have precise
comparisons, but not accurate ones.  In particular, explicit studies
for the purpose of comparing languages,
because they will probably use small problems, and will necessarily use
predefined problems, will tend to underestimate the power of the
more powerful languages.

Reports from the field, though they will necessarily be less precise than
"scientific" studies, are likely to be more meaningful.  For example, 
Ulf Wiger of Ericsson did a 
study that 
concluded that Erlang was 4-10x
more succinct than C++, and proportionately faster to develop 
software in:

Comparisons between Ericsson-internal development projects indicate
similar line/hour productivity, including all phases of software development,
rather independently of which language (Erlang, PLEX, C, C++, or Java)
was used.  What differentiates the different languages then becomes source
code volume.

 The study also deals explictly with a point that was 
only implicit in Brooks' book (since he measured lines of debugged code):
programs written in more powerful languages tend to have fewer bugs.
That becomes an end in itself, possibly more important than programmer
productivity, in applications like network switches.

The Taste Test

Ultimately, I think you have to go with your gut.  What does it feel
like to program in the language?  I think the way to find (or design)
the best language is to become hypersensitive to how well a language
lets you think, then choose/design the language that feels best.  If
some language feature is awkward or restricting, don't worry, you'll
know about it.

Such hypersensitivity will come at a cost.  You'll find that you can't
stand programming in clumsy languages.   I find it unbearably
restrictive to program in languages without macros, just as someone used
to dynamic typing finds it unbearably restrictive to have to go back to
programming in a language where you have to declare the type of
every variable, and can't make a list of objects of different types.


I'm not the only one.  I know many Lisp hackers that this has happened
to.  In fact, the most accurate measure of the relative power of programming
languages might be the percentage of people who know the language
who will take any job where they get to use that language, regardless
of the application domain.

Restrictiveness

I think most hackers know what it means for a language to feel restrictive.
What's happening when you feel that?  I think it's the same feeling
you get when the street you want to take is blocked off, and you have to
take a long detour to get where you wanted to go.  There is something
you want to say, and the language won't let you.

What's really going on here, I think, is that a restrictive language is
one that isn't succinct enough.  The problem is not simply that you can't
say what you planned to.  It's that the detour the language makes you
take is longer.  Try this thought experiment.  Suppose there were
some program you wanted to write, and the language wouldn't let you
express it the way you planned to, but instead forced you to write the
program in some other way that was shorter.  For me at least,
that wouldn't feel very restrictive.  It would be like the street you
wanted to take being blocked off, and the policeman at the 
intersection directing you to a shortcut instead of a detour.  Great!

I think most (ninety percent?) of 
the feeling of restrictiveness comes from being forced to make the program
you write in the language longer than one you have in your head.
Restrictiveness is mostly lack of succinctness.
So when a language feels restrictive, what that (mostly) means is that it isn't
succinct enough, and when a language isn't succinct, it will
feel restrictive.

Readability

The quote I began with mentions two other qualities, regularity and
readability.  I'm not sure what regularity is, or what advantage, 
if any, code that is regular and readable has over code that is merely
readable.  But I think I know what is meant by readability, and I think
it is also related to succinctness.

We have to be careful here to distinguish between the readability of
an individual line of code and the readability of the whole program.
It's the second that matters.  I agree that a line of Basic is likely to be
more readable than a line of Lisp.  But a program written in Basic is
is going to have more lines than the same program written in
Lisp (especially once you cross over into Greenspunland). The
total effort of reading the Basic program will surely be greater.

total effort  = effort per line x number of lines

I'm not as sure that readability is directly proportionate to succinctness
as I am that power is, but certainly succinctness is a factor 
(in the mathematical sense; see equation above) in readability.
So it may not even be meaningful to say that the goal of a language is
readability, not succinctness; it could be like saying the goal was readability,
not readability.

What readability-per-line does mean, to the user encountering the language
for the first time, is that source code will look unthreatening.  So
readability-per-line
could be a good marketing decision, even if it is a bad design
decision.  It's isomorphic to the very successful technique of letting
people pay in installments: instead of frightening them with a high
upfront price, you tell them the low monthly payment.  Installment plans
are a net lose for the buyer, though, as mere readability-per-line probably
is for the programmer.
The buyer is going to make a lot of those low, low payments; and 
the programmer is going to read a lot of those individually readable lines.

This tradeoff predates programming languages.  If you're used to reading
novels and newspaper articles, your first experience of reading a math
paper can be dismaying.  It could take half an hour to read a single page.  
And yet, I am pretty sure that the notation is not the problem, even though
it may feel like it is.  The math paper is hard to read 
because the ideas are hard.  If you expressed the same ideas in prose
(as mathematicians had to do before they evolved succinct notations),
they wouldn't be any easier to read, because the paper would grow to the
size of a book.

To What Extent?

A number of people have rejected
the idea that succinctness = power.  I think it would be more useful, instead
of simply arguing that they are the same or aren't, to ask:
to what extent does succinctness = power?
Because clearly succinctness is
a large part of what higher-level languages are for.  If it is not all they're
for, then what else are they for, and how important, relatively, are these
other functions?

I'm not proposing this just to make the debate more civilized.  I really
want to know the answer.  When, if ever,  is a language too succinct for 
its own good?

The hypothesis I began with was that, except in pathological examples,
I thought succinctness could be considered identical with power.  What
I meant was that in any language anyone would design, they
would be identical, but that if someone wanted to design a language
explicitly to disprove this hyphothesis, they could probably do it.  I'm
not even sure of that, actually.

Languages, not Programs

We should be clear that we are talking about the succinctness
of languages, not of individual programs.
It certainly is possible for individual programs to be written too densely.

I wrote about this in On Lisp. A complex macro
may have to save many times its own length to be justified.  If writing
some hairy macro could save you ten lines of code every time you use it,
and the macro is itself ten lines of code, then you get a net saving in
lines if you use it more than once.  But that could still be a bad move,
because macro definitions are harder to read than ordinary code.  You 
might have to use the macro ten or twenty times before it yielded a net
improvement in readability.

I'm sure every language has such tradeoffs (though I suspect the stakes
get higher as the language gets more powerful).  Every programmer must
have seen code that some clever person has made marginally shorter
by using dubious programming tricks.

So there is no argument about that-- at least, not from me.  Individual
programs can certainly be too succinct for their own good.  The question
is, can a language be?  Can a language compel programmers to write
code that's short (in elements) at the expense of overall readability?

One reason it's hard to imagine a language being too succinct is that if
there were some excessively compact way to phrase something, there would
probably also be a longer way.  For example, if you felt Lisp programs using
a lot of macros or higher-order functions were too dense,  you could, if you
preferred, write code that was isomorphic to Pascal.  If you
don't want to express factorial in Arc as a call to a higher-order function

(rec zero 1 * 1-)

you can also write out a recursive definition:

(rfn fact (x) (if (zero x) 1 (* x (fact (1- x)))))

Though I can't off the top of my head think of any examples, I am interested
in the question of whether a language could be too succinct.  Are there languages 
that force you to write code in a way that is crabbed and incomprehensible?
If anyone has examples, I would be very interested to see them.

(Reminder: What I'm looking for are programs that are very dense according
to the metric of "elements" sketched above, not merely programs that are
short because delimiters can be omitted and everything has a one-character name.)





Kevin Kelleher suggested an interesting way to compare programming
languages: to describe each in terms of the problem it
fixes.  The surprising thing is how many, and how well, languages can be
described this way.



Algol: Assembly language is too low-level.

Pascal: Algol doesn't have enough data types.

Modula: Pascal is too wimpy for systems programming.


Simula: Algol isn't good enough at simulations.

Smalltalk: Not everything in Simula is an object.

Fortran: Assembly language is too low-level.

Cobol: Fortran is scary.

PL/1: Fortran doesn't have enough data types.

Ada: Every existing language is missing something.

Basic: Fortran is scary.

APL: Fortran isn't good enough at manipulating arrays.

J: APL requires its own character set.

C: Assembly language is too low-level.

C++: C is too low-level.

Java: C++ is a kludge.  And Microsoft is going to crush us.

C#: Java is controlled by Sun.


Lisp: Turing Machines are an awkward way to describe computation.

Scheme: MacLisp is a kludge.

T: Scheme has no libraries.

Common Lisp: There are too many dialects of Lisp.

Dylan: Scheme has no libraries, and Lisp syntax is scary.


Perl: Shell scripts/awk/sed are not enough like programming languages.

Python: Perl is a kludge.

Ruby: Perl is a kludge, and Lisp syntax is scary.

Prolog: Programming is not enough like logic.





February 2002





"...Copernicus'
aesthetic objections to [equants] provided one essential
motive for his rejection of the Ptolemaic system...."

- Thomas Kuhn, The Copernican Revolution

"All of us had been trained by Kelly Johnson and believed
fanatically in his insistence that an airplane that looked
beautiful would fly the same way."

- Ben Rich, Skunk Works

"Beauty is the first test: there is no permanent place in this
world for ugly mathematics."

- G. H. Hardy, A Mathematician's Apology




I was talking recently to a friend who teaches
at MIT.  His field is hot now and
every year he is inundated by applications from
would-be graduate students.  "A lot of them seem smart,"
he said.  "What I can't tell is whether they have any kind
of taste."

Taste.  You don't hear that word much now.
And yet we still need the underlying
concept, whatever we call it.  What my friend meant was
that he wanted students who were not just good technicians,
but who could use their technical knowledge to
design beautiful things.

Mathematicians call good work "beautiful,"
and so, either now or in the past, have
scientists, engineers, musicians, architects, designers,
writers, and painters.
Is it just a coincidence that they used the same word, or is   
there some overlap in what they meant?  If there
is an overlap, can we use one field's discoveries
about beauty to help us in another?

For those of us who design things, these are not just
theoretical questions.  If there is such a thing as
beauty, we need to be able to recognize it.  We need
good taste to make good things.
Instead of
treating beauty as an airy abstraction, to be either blathered
about or avoided depending on how one feels about airy
abstractions, let's try considering it as a practical question:
how do you make good stuff?



If you mention taste nowadays, a lot of people will tell
you that "taste is subjective."
They believe this because it really feels that
way to them.  When they like something, they have no idea
why.  It could be because it's beautiful, or because their
mother had one, or because they saw a movie star with one
in a magazine, or because they know it's expensive.
Their thoughts are a tangle of unexamined impulses.

Most of us are encouraged, as children, to leave this tangle
unexamined.  If you make fun of your little brother for
coloring people green in his coloring book, your
mother is likely to tell you something like "you like to
do it your way and he likes to do it his way."

Your mother at this point is not trying to teach you
important truths about aesthetics.  She's trying to get
the two of you to stop bickering.

Like many of the half-truths adults tell us, this one
contradicts other things they tell us.  After dinning
into you that taste is merely a matter of personal preference,
they take you to the museum and tell you that you should
pay attention because Leonardo is a great artist.

What goes through the kid's head at this point?  What does
he think "great artist" means?  After having been
told for years that everyone just likes to do
things their own way, he is
unlikely to head straight for the conclusion that a great
artist is someone whose work is better than the others'.
A far more likely theory, in his Ptolemaic model of
the universe, is that a great artist is something that's
good for you, like broccoli, because someone said so in a book.



Saying that taste is just personal preference is a good way
to prevent disputes.  The trouble is, it's not true.
You feel this when you start to design things.

Whatever job people do, they naturally want to do better.
Football players
like to win games.  CEOs like to increase earnings.  It's
a matter of pride, and a real pleasure, to get better at
your job. But if
your job is to design things, and there is no such thing
as beauty, then there is no way to get better at your job.
If taste is just personal preference, then everyone's is   
already perfect: you like whatever you like, and that's it.

As in any job, as you continue to design things, you'll get
better at it.  Your tastes will change.  And, like anyone
who gets better at their job, you'll know you're getting
better.  If so,
your old tastes were
not merely different, but worse.  Poof goes the axiom that
taste can't be wrong.

Relativism is fashionable at the moment, and that may hamper
you from thinking about taste, even as yours grows.
But if you come out of the closet and admit, at least to yourself,
that there is such a thing as good and bad design, then you
can start to study good design in detail.
How has
your taste changed?  When you made mistakes, what
caused you to make them?  What have other people learned about
design?

Once you start to examine the question, it's surprising how
much different fields' ideas of beauty have in common.  The same
principles of good design crop up again and again.



Good design is simple.  You hear this from math to
painting.  In math it means that a shorter proof tends to be
a better one.  Where axioms are concerned, especially,
less is more.  It means much the same thing in programming.
For architects and designers it means that beauty should
depend on a few carefully chosen structural elements
rather than a profusion of superficial ornament.  (Ornament
is not in itself bad, only when it's camouflage on insipid
form.)  Similarly, in painting, a
still life of a few carefully observed and solidly
modelled objects will tend to be more interesting than a
stretch of flashy
but mindlessly repetitive painting of, say, a lace collar.
In writing it means: say what you mean
and say it briefly.

It seems strange to have to emphasize simplicity.
You'd think simple would be the default.  Ornate
is more work.  But something seems to come over people
when they try to be creative.  Beginning writers adopt   
a pompous tone that doesn't sound anything like the way 
they speak.  Designers trying to be artistic resort to
swooshes and curlicues.  Painters discover that they're expressionists.
It's all evasion.
Underneath
the long words or the "expressive" brush strokes, there
is not much going on, and that's frightening.

When you're
forced to be simple, you're forced to face the real problem.
When you can't deliver ornament, you have to deliver
substance.



Good design is timeless.
In math, every proof is timeless unless it contains a mistake.
So what does Hardy mean when he says there is no permanent 
place for ugly mathematics?  He means the same thing Kelly Johnson did:
if something is ugly, it can't be the best solution.  There
must be a better one, and eventually
someone will discover it.

Aiming at timelessness is a way to make
yourself find the best answer:
if you can imagine someone surpassing you, you should do it yourself.
Some of the greatest masters did this so well that they
left little room for those who came after.
Every engraver since Durer has had to live in his shadow.

Aiming at timelessness is also a way to evade
the grip of fashion.  Fashions almost by definition
change with time, so if you can make something that
will still look good far into the future, then its
appeal must derive more from merit and less from fashion.

Strangely enough, if you want to make something that will 
appeal to future generations, one way to do it is to
try to appeal to past generations.  It's hard to guess what
the future will be like, but we can be sure it will be
like the past in caring nothing for present fashions.
So if you can make something that appeals to people today
and would also have appealed to people in 1500, there is a good
chance it will appeal to people in 2500.



Good design solves the right problem. The typical
stove has four burners arranged in a square, and a dial
to control each.  How do you arrange the dials?  The
simplest answer is to put them in a row.  But this is a
simple answer to the wrong question.
The dials are for humans to use, and if you put them in a row,
the unlucky human will have to stop and think each time
about which dial matches which burner.  Better to arrange the dials
in a square like the burners.

A lot of bad design is industrious, but misguided.
In the mid twentieth century there was a vogue for
setting text in sans-serif fonts.
These fonts are closer to the pure, underlying letterforms.
But in text that's not the problem you're trying to solve. 
For legibility it's more important that letters be easy
to tell apart.
It may look Victorian, but a Times Roman lowercase g is
easy to tell from a lowercase y.

Problems can be improved as well as solutions.
In software, an intractable problem can usually be replaced
by an equivalent one that's easy to solve.
Physics progressed faster as the problem became
predicting observable behavior, instead of reconciling it
with scripture.



Good design is suggestive.
Jane Austen's novels contain almost no
description; instead of telling you how
everything looks, she tells her story so well that you   
envision the scene for yourself.
Likewise, a painting that suggests is usually more engaging
than one that tells.  Everyone makes up their own story about the
Mona Lisa.

In architecture and design, this
principle means that a building or object should let you 
use it how you want: a good building, for example, will
serve as a backdrop for whatever life people want to lead in it, instead
of making them live as if they were executing a program
written by the architect.

In software, it means you should give users a few
basic elements that they can combine as they wish, like Lego. 
In math it means a proof that
becomes the basis for a lot of new work is
preferable to a proof that was difficult,
but doesn't lead to future discoveries; in the
sciences generally, citation is considered a rough
indicator of merit.



Good design is often slightly funny.  This one
may not always be true.  But Durer's 
engravings
and Saarinen's 
womb chair and the 
Pantheon and the
original Porsche 911 all seem
to me slightly funny.  Godel's incompleteness theorem
seems like a practical joke.

I think it's because humor is related to strength.
To have a sense of humor is to be strong:
to keep one's sense of humor is to shrug off misfortunes,
and to lose one's sense of humor is to be wounded by them.
And so the mark-- or at least the prerogative-- of strength
is not to take
oneself too seriously.
The confident will often, like
swallows, seem to be making fun of the whole process slightly,
as Hitchcock does in his films or Bruegel in his paintings-- or
Shakespeare, for that matter.

Good design may not have to be funny, but it's hard to
imagine something that could be called humorless also being
good design.



Good design is hard.  If you look at the people who've
done great work, one thing they all seem to have in common is that they
worked very hard.  If you're not working hard,
you're probably wasting your time.

Hard problems call for great
efforts.  In math, difficult proofs require ingenious solutions,
and those tend to be interesting.  Ditto in engineering.

When you
have to climb a mountain you toss everything unnecessary
out of your pack.  And so an architect who has to build
on a difficult site, or a small budget, will find that he
is forced to produce an elegant design.  Fashions and
flourishes get knocked aside by the difficult business
of solving the problem at all.

Not every kind of hard is good.  There is good pain and bad pain.
You want the kind of pain you get from going running, not the
kind you get from stepping on a nail.
A difficult
problem could be good for a designer, but a fickle client or unreliable
materials would not be.

In art, the highest place has traditionally been given to
paintings of people.  There is something to this tradition,
and not just because pictures of faces get to press
buttons in our brains that other pictures don't.  We are 
so good at looking at faces that we force anyone who
draws them to work hard to satisfy us.  If you
draw a tree and you change the angle of a branch
five degrees, no one will know.  When you change the angle
of someone's eye five degrees, people notice.

When Bauhaus designers adopted Sullivan's "form follows function,"
what they meant was, form should follow function.  And
if function is hard enough, form is forced to follow it,
because there is no effort to spare for error.  Wild animals
are beautiful because they have hard lives.



Good design looks easy.  Like great athletes,
great designers make it look easy.  Mostly this is
an illusion.  The easy, conversational tone of good
writing comes only on the eighth rewrite.

In science and engineering, some of the greatest
discoveries seem so simple that you say to yourself,
I could have thought of that.  The discoverer is
entitled to reply, why didn't you?

Some Leonardo heads are just a few lines.  You look
at them and you think, all you have to do is get eight
or ten lines in the right place and you've made this beautiful
portrait.  Well, yes, but you have to get them in
exactly the right place.  The slightest error
will make the whole thing collapse.

Line drawings are in fact the most difficult visual
medium, because they demand near perfection.
In math terms, they are a closed-form solution; lesser  
artists literally solve the same problems by successive
approximation.  One of the reasons kids give up drawing
at ten or so is that they decide to start
drawing like grownups, and one of the first things
they try is a line drawing of a face.  Smack!

In most fields the appearance of ease seems to come with
practice.  Perhaps what practice does is train your
unconscious mind to handle tasks that used to
require conscious thought.  In some cases
you literally train your body.  An expert pianist can
play notes faster than the brain can send signals to
his hand.  
Likewise an artist, after a while, can
make visual perception flow in through his eye and
out through his hand as automatically as someone tapping his foot to
a beat.

When people talk about being in
"the zone," I think what they mean is that the
spinal cord has the situation under control.
Your spinal cord is less hesitant, and
it frees conscious thought for the hard problems.




Good design uses symmetry.
I think symmetry may just
be one way to achieve simplicity, but it's important enough
to be mentioned on its own.
Nature uses it a lot, which is a good sign.

There are two kinds of symmetry, repetition and recursion.
Recursion means repetition in subelements, like the
pattern of veins in a leaf.

Symmetry is unfashionable in some fields now, in reaction to
excesses in the past.  Architects started consciously
making buildings asymmetric in Victorian times and by the
1920s asymmetry was an explicit premise of modernist architecture.
Even these buildings only tended to be asymmetric
about major axes, though; there were hundreds of minor symmetries.

In writing you find symmetry at every level, from the phrases
in a sentence to the plot of a novel.  You find the same
in music and art.
Mosaics (and some Cezannes) get extra visual punch by making
the whole picture out of the same atoms.  Compositional 
symmetry yields some of the most memorable paintings,  
especially when two halves react to one another, as in 
the Creation of Adam or 
American Gothic.

In math and engineering, recursion, especially, is a big win.
Inductive proofs are wonderfully short.  In software,
a problem that can be solved by recursion is nearly always
best solved that way. The Eiffel Tower looks striking partly
because it is a recursive solution, a tower on a tower.

The danger of symmetry, and repetition especially, is that
it can be used as a substitute for thought.



Good design resembles nature.  It's not so much that
resembling nature is intrinsically good as that nature
has had a long time to work on the
problem.  It's a good sign when your answer resembles nature's.

It's not cheating to copy.
Few would deny that a story should be like life.
Working from life is a valuable tool in painting too, though its
role has often been misunderstood.
The aim is not simply to make a record.
The point of painting from life is
that it gives your mind something to chew on:  when your
eyes are looking at something, your hand will do more
interesting work.

Imitating nature also works in engineering.  Boats have
long had spines and ribs like an animal's ribcage.
In some cases we may have to wait for better technology:
early aircraft designers were mistaken to
design aircraft that looked like birds, because they didn't
have materials or power sources light enough (the Wrights' engine
weighed 152 lbs. and
generated only 12 hp.) or control systems sophisticated
enough for machines that flew like birds, but I could
imagine little unmanned reconnaissance planes flying
like birds in fifty years.

Now that we have enough computer power, we can imitate nature's   
method as well as its results.  Genetic algorithms may let us
create things too complex to design in the ordinary
sense.



Good design is redesign.  It's rare to get things right
the first time.  Experts expect to throw away some early work.
They plan for plans to change.

It takes confidence to throw work away.  You have to be able 
to think, there's more where that came from.   
When people first start drawing, for example,
they're often reluctant to redo parts that aren't
right; they feel they've been lucky to get that far,   
and if they try to redo something, it will turn out worse.  Instead
they convince themselves that the drawing is not that bad,
really-- in fact, maybe they meant it to look that way.

Dangerous territory, that; if anything you should
cultivate dissatisfaction.
In Leonardo's drawings there are often five
or six attempts to get a line right.
The distinctive back of the Porsche
911 only appeared in the redesign of an awkward 
prototype.
In Wright's early plans for the 
Guggenheim,
the right half was a ziggurat; he inverted it to get the
present shape.

Mistakes are natural.  Instead of treating them
as disasters, make them easy to acknowledge and easy to fix.
Leonardo more or less invented the sketch, as a
way to make drawing bear a greater weight of exploration.
Open-source software has fewer bugs because it admits the
possibility of bugs.

It helps to have a medium that makes change easy.
When oil paint replaced tempera in the fifteenth century,
it helped
painters to deal with difficult subjects like the human 
figure because, unlike tempera, oil can be blended and overpainted.




Good design can copy.  Attitudes to copying
often make a round trip.  A novice
imitates without knowing it; next he tries
consciously to be original; finally, he decides it's
more important to be right than original.

Unknowing imitation is almost a recipe for bad design.
If you don't know where your ideas are coming from,
you're probably imitating an imitator.
Raphael so pervaded mid-nineteenth century taste that almost 
anyone who tried to draw was imitating him, often at several
removes.
It was this, more than Raphael's own work, that bothered
the Pre-Raphaelites.

The ambitious are not content to imitate. The
second phase in the growth of taste is a conscious
attempt at originality.

I think the
greatest masters go on to achieve a kind of selflessness.
They just want to get the right answer, and if part of the
right answer has already been discovered by someone else,
that's no reason not to use it.
They're confident enough to take from anyone without
feeling that their own vision will be lost in the process.




Good design is often strange.  Some of the very best work
has an uncanny quality: Euler's 
Formula, 
Bruegel's
Hunters in the Snow, the 
SR-71, Lisp.  They're not just
beautiful, but strangely beautiful.

I'm not sure why.  It may just be my own stupidity.  A
can-opener must seem miraculous to a dog.  Maybe if I were smart
enough it would seem the most natural thing in the world that
ei*pi = -1.  It is after all necessarily true.

Most of the qualities I've mentioned are things that can be
cultivated, but I don't think it works to cultivate strangeness.
The best you can do is not squash it if it starts to appear.
Einstein didn't try to make relativity strange.
He tried to make it true, and the truth turned out to be strange.

At an art school where I once studied, the students wanted
most of all to develop a personal style.
But if you just try to make good things, you'll  
inevitably do it in a distinctive way, just as each person
walks in a distinctive way.  Michelangelo was not trying
to paint like Michelangelo.  He was just trying to paint
well; he couldn't help painting like Michelangelo.

The only style worth having is the one you can't help.
And this is especially true for strangeness.  There is no
shortcut to it.  The Northwest Passage that the Mannerists,
the Romantics, and two generations of American high school
students have searched for does not seem to exist.  The
only way to get there is to go through good and come out
the other side.




Good design happens in chunks.  The inhabitants
of fifteenth century Florence included Brunelleschi, Ghiberti,
Donatello, Masaccio, Filippo Lippi, 
Fra Angelico, Verrocchio, Botticelli, Leonardo, and Michelangelo.
Milan at the time was as big as Florence.
How many fifteenth century Milanese artists can you name?

Something was happening in Florence in the fifteenth century.
And it can't have been heredity, because it isn't happening now.
You have to assume that whatever
inborn ability Leonardo and Michelangelo had, there were
people born in Milan with just as much.  What happened to
the Milanese Leonardo?

There are roughly a thousand times
as many people alive in the US right now as lived in
Florence during the fifteenth century.  A thousand Leonardos
and a thousand Michelangelos walk among us.
If DNA ruled, we should be greeted daily by artistic
marvels.  We aren't, and the reason is that to make Leonardo
you need more than his innate ability.  You also need Florence 
in 1450.

Nothing is more powerful
than a community of talented people working on related
problems.  Genes count for little by comparison: being a genetic
Leonardo was not enough to compensate for having been born   
near Milan instead of Florence.
Today we move around more, but great work still comes
disproportionately from a few hotspots:
the Bauhaus, the Manhattan Project, the New Yorker,
Lockheed's Skunk Works, Xerox Parc.

At any given time there are a
few hot topics and a few groups doing great work on them,
and it's nearly impossible to do
good work yourself if you're too far removed from one
of these centers.  You can push or pull these trends
to some extent, but you can't break away from them.
(Maybe you can, but the Milanese Leonardo couldn't.)




Good design is often daring.  At every period   
of history, people have believed things that were just   
ridiculous, and believed them so strongly that you risked  
ostracism or even violence by saying otherwise.

If our own time were any different, that would be remarkable.
As far as I can tell it isn't.

This problem afflicts not just every
era, but in some degree every field.
Much Renaissance art was in its time considered shockingly secular:
according to Vasari, Botticelli repented and gave up painting, and
Fra Bartolommeo and Lorenzo di Credi actually burned some of their
work.
Einstein's theory of relativity offended many contemporary physicists,
and was not fully accepted for decades-- in France, not until the
1950s.

Today's experimental error is tomorrow's new theory.  If
you want to discover great new things, then instead of turning
a blind eye to the places where conventional wisdom and
truth don't quite meet, you should pay particular attention 
to them.



As a practical matter, I think it's easier to see ugliness
than to imagine beauty.  Most of the people who've made beautiful
things seem to have done it by fixing something that they    
thought ugly.  Great work usually seems to happen because someone sees
something and thinks, I could do better than that.  Giotto
saw traditional Byzantine madonnas painted according to a
formula that had satisfied everyone for centuries, and to him
they looked wooden and unnatural.
Copernicus was so troubled by a hack that all his contemporaries
could tolerate that he felt there must be a better solution.

Intolerance for ugliness is not in itself enough.  You have to
understand a field well before you develop a good nose for
what needs fixing.  You have to do your homework.  But as
you become expert in a field, you'll start to hear little
voices saying, What a hack!  There must be a better way.
Don't ignore those voices.  Cultivate them.  The recipe for
great work is: very exacting taste, plus the ability
to gratify it.





Notes

Sullivan
 actually said "form ever follows function," but   
I think the usual misquotation is closer to what modernist
architects meant.


Stephen G. Brush, "Why was Relativity Accepted?"
Phys. Perspect. 1 (1999) 184-214.

There is a kind of mania for object-oriented programming at the moment, but

some of the smartest programmers I know are some of the least excited about it.

My own feeling is that object-oriented
programming is a useful technique in some
cases, but it isn't something that has to pervade every program you
write.  You should be able to define new types,
but you shouldn't have to express every program as the
definition of new types.

I think there are five reasons people like object-oriented   
programming, and three and a half of them are bad:


 Object-oriented programming is exciting   
if you have a statically-typed language without 
lexical closures or macros.  To some degree, it offers a way around these
limitations.  (See Greenspun's Tenth Rule.)

 Object-oriented programming is popular in big companies,
because it suits the way they write software.  At big companies,
software tends to be written by large (and frequently changing)  
teams of
mediocre programmers.  Object-oriented programming imposes a
discipline on these programmers that prevents any one of them
from doing too much damage.  The price is that the resulting
code is bloated with protocols and full of duplication.  
This is not too high a price for big companies, because their
software is probably going to be bloated and full of 
duplication anyway.

 Object-oriented
programming generates a lot of what looks like work.
Back in the days of fanfold, there was a type of programmer who
would only put five or ten lines of code on a page, preceded
by twenty lines of elaborately formatted comments. 
Object-oriented programming is like crack for these people: it lets
you incorporate all this scaffolding right into your source
code.  Something that a Lisp hacker might handle by pushing
a symbol onto a list becomes a whole file of classes and
methods.  So it is a good tool if you want to convince yourself,
or someone else, that you are doing a lot of work.

 If a language is itself an object-oriented program, it can
be extended by users.  Well, maybe.  Or maybe you can do
even better by offering the sub-concepts
of object-oriented programming a la carte.  Overloading, 
for example, is not intrinsically tied to classes.  We'll see.

 Object-oriented abstractions map neatly onto the domains
of  certain specific kinds of programs, like simulations and CAD
systems.                                        


I personally have never needed object-oriented abstractions.
Common Lisp has an enormously powerful object system and I've
never used it once.  I've done a lot of things (e.g. making         
hash tables full of closures) that would have required 
object-oriented techniques to do in wimpier languages, but
I have never had to use CLOS.

Maybe I'm just stupid, or have worked on some limited subset
of applications.  There is a danger in designing a language
based on one's own experience of programming.  But it seems
more dangerous to put stuff in that you've never needed 
because it's thought to be a good idea.
December 2001 (rev. May 2002)

(This article came about in response to some questions on
the LL1 mailing list.  It is now
incorporated in Revenge of the Nerds.)

When McCarthy designed Lisp in the late 1950s, it was
a radical departure from existing languages,
the most important of which was Fortran.

Lisp embodied nine new ideas:


1. Conditionals.  A conditional is an if-then-else
construct.  We take these for granted now.  They were 
invented
by McCarthy in the course of developing Lisp. 
(Fortran at that time only had a conditional
goto, closely based on the branch instruction in the 
underlying hardware.)  McCarthy, who was on the Algol committee, got
conditionals into Algol, whence they spread to most other
languages.

2. A function type. In Lisp, functions are first class 
objects-- they're a data type just like integers, strings,
etc, and have a literal representation, can be stored in variables,
can be passed as arguments, and so on.

3. Recursion.  Recursion existed as a mathematical concept
before Lisp of course, but Lisp was the first programming language to support
it.  (It's arguably implicit in making functions first class
objects.)

4. A new concept of variables.  In Lisp, all variables
are effectively pointers. Values are what
have types, not variables, and assigning or binding
variables means copying pointers, not what they point to.

5. Garbage-collection.

6. Programs composed of expressions. Lisp programs are 
trees of expressions, each of which returns a value.  
(In some Lisps expressions
can return multiple values.)  This is in contrast to Fortran
and most succeeding languages, which distinguish between
expressions and statements.

It was natural to have this
distinction in Fortran because (not surprisingly in a language
where the input format was punched cards) the language was
line-oriented.  You could not nest statements.  And
so while you needed expressions for math to work, there was
no point in making anything else return a value, because
there could not be anything waiting for it.

This limitation
went away with the arrival of block-structured languages,
but by then it was too late. The distinction between
expressions and statements was entrenched.  It spread from 
Fortran into Algol and thence to both their descendants.

When a language is made entirely of expressions, you can
compose expressions however you want.  You can say either
(using Arc syntax)

(if foo (= x 1) (= x 2))

or

(= x (if foo 1 2))

7. A symbol type.  Symbols differ from strings in that
you can test equality by comparing a pointer.

8. A notation for code using trees of symbols.

9. The whole language always available.  
There is
no real distinction between read-time, compile-time, and runtime.
You can compile or run code while reading, read or run code
while compiling, and read or compile code at runtime.

Running code at read-time lets users reprogram Lisp's syntax;
running code at compile-time is the basis of macros; compiling
at runtime is the basis of Lisp's use as an extension
language in programs like Emacs; and reading at runtime
enables programs to communicate using s-expressions, an
idea recently reinvented as XML.


When Lisp was first invented, all these ideas were far
removed from ordinary programming practice, which was
dictated largely by the hardware available in the late 1950s.

Over time, the default language, embodied
in a succession of popular languages, has
gradually evolved toward Lisp.  1-5 are now widespread.
6 is starting to appear in the mainstream.
Python has a form of 7, though there doesn't seem to be
any syntax for it.  
8, which (with 9) is what makes Lisp macros
possible, is so far still unique to Lisp,
perhaps because (a) it requires those parens, or something 
just as bad, and (b) if you add that final increment of power, 
you can no 
longer claim to have invented a new language, but only
to have designed a new dialect of Lisp ; -)

Though useful to present-day programmers, it's
strange to describe Lisp in terms of its
variation from the random expedients other languages
adopted.  That was not, probably, how McCarthy
thought of it.  Lisp wasn't designed to fix the mistakes
in Fortran; it came about more as the byproduct of an
attempt to axiomatize computation.


September 2001


(This article explains why much of the next generation of software
may be server-based, what that will mean for programmers,
and why this new kind of software is a great opportunity for startups.
It's derived from a talk at BBN Labs.)


In the summer of 1995, my friend Robert Morris and I decided to
start a startup.  The PR campaign leading up to Netscape's IPO was
running full blast then, and there was a lot of talk in the press
about online commerce.  At the time there might have been thirty
actual stores on the Web, all made by hand.  If there were going
to be a lot of online stores, there would need to be software for making
them, so we decided to write some.

For the first week or so we intended to make this an ordinary   
desktop application.  Then one day we had the idea of making the
software run on our Web server, using the browser as an
interface.  We tried rewriting the software to work over
the Web, and it was clear that this was the way to go.
If we wrote our software to run on the server, it would be a lot easier
for the users and for us as well.

This turned out to be a good plan.  Now, as 
Yahoo Store, this
software is the most popular online store builder, with
about 14,000 users.

When we started Viaweb, hardly anyone understood what we meant when
we said that the software ran on the server.  It was not until
Hotmail was launched a year later that people started to get it.
Now everyone knows that this is a valid approach.  There is
a name now for what we were: an Application Service Provider,
or ASP.

I think that a lot of the next generation of software will be
written on this model.  Even Microsoft, who have the most to
lose, seem to see the inevitablity of moving some things off
the desktop. If software moves
off the desktop and onto servers, it will mean a very different
world for developers.  This article describes the surprising
things we saw, as some of the first visitors to this new world.
To the extent software does move onto
servers, what I'm describing here is the future.

The Next Thing?

When we look back on the desktop software era, I think we'll marvel
at the inconveniences people put up with, just as we marvel now at
what early car owners put up with.  For the first twenty or thirty
years, you had to be a car expert to own a car.  But cars were such
a big win that lots of people who weren't car experts wanted to
have them as well.

Computers are in this phase now.  When you own a desktop computer,
you end up learning a lot more than you wanted to know about what's
happening inside it.  But more than half the households in the US
own one.  My mother has a computer that she uses for email and for
keeping accounts.  About a year ago she was alarmed to receive a
letter from Apple, offering her a discount on a new version of the
operating system.  There's something wrong when a sixty-five year
old woman who wants to use a computer for email and accounts has
to think about installing new operating systems.  Ordinary users
shouldn't even know the words "operating system," much less "device
driver" or "patch."

There is now another way to deliver software that will save users
from becoming system administrators.  Web-based applications are
programs that run on Web servers and use Web pages as the user
interface.  For the average user this new kind of software will be
easier, cheaper, more mobile, more reliable, and often more powerful
than desktop software.

With Web-based software, most users won't have to think about
anything except the applications they use.  All the messy, changing
stuff will be sitting on a server somewhere, maintained by the kind
of people who are good at that kind of thing.  And so you won't
ordinarily need a computer, per se, to use software.  All you'll
need will be something with a keyboard, a screen, and a Web browser.
Maybe it will have wireless Internet access.  Maybe it will also
be your cell phone.  Whatever it is, it will be consumer electronics:
something that costs about $200, and that people choose mostly
based on how the case looks.  You'll pay more for Internet services
than you do for the hardware, just as you do now with telephones. [1]

It will take about a tenth of a second for a click to get to the
server and back, so users of heavily interactive software, like
Photoshop, will still want to have the computations happening on
the desktop.  But if you look at the kind of things most people
use computers for, a tenth of a second latency would not be a
problem.  My mother doesn't really need a desktop computer, and
there are a lot of people like her.

The Win for Users

Near my house there is a car with a bumper sticker that reads "death
before inconvenience."  Most people, most of the time, will take
whatever choice requires least work.  If Web-based software wins,
it will be because it's more convenient.  And it looks as if it
will be, for users and developers both.

To use a purely Web-based application, all you need is a browser
connected to the Internet.  So you can use a Web-based application
anywhere.  When you install software on your desktop computer, you
can only use it on that computer.  Worse still, your files are
trapped on that computer.  The inconvenience of this model becomes
more and more evident as people get used to networks.

The thin end of the wedge here was Web-based email.  Millions of
people now realize that you should have access to email messages
no matter where you are.  And if you can see your email, why not
your calendar?  
If you can discuss a document with your colleagues,
why can't you edit it?  Why should any of your data be trapped on
some computer sitting on a faraway desk?

The whole idea of "your computer" is going away, and being replaced
with "your data."  You should be able to get at your data from any
computer.  Or rather, any client, and a client doesn't have to be
a computer.

Clients shouldn't store data; they should be like telephones.  In
fact they may become telephones, or vice versa.  And as clients
get smaller, you have another reason not to keep your data on them:
something you carry around with you can be lost or stolen.   Leaving
your PDA in a taxi is like a disk crash, except that your data is
handed to someone else 
instead of being vaporized.

With purely Web-based software, neither your data nor the applications
are kept on the client.  So you don't have to install anything to
use it.  And when there's no installation, you don't have to worry
about installation going wrong.  There can't be incompatibilities
between the application and your operating system, because the
software doesn't run on your operating system.

Because it needs no installation, it will be easy, and common, to
try Web-based software before you "buy" it.  You should expect to
be able to test-drive any Web-based application for free, just by
going to the site where it's offered.  At Viaweb our whole site
was like a big arrow pointing users to the test drive.

After trying the demo, signing up for the service should require
nothing more than filling out a brief form (the briefer the better).
And that should be the last work the user has to do.  With Web-based
software, you should get new releases without paying extra, or
doing any work, or possibly even knowing about it.

Upgrades won't be the big shocks they are now.  Over time applications
will quietly grow more powerful.  This will take some effort on
the part of the developers.  They will have to design software so
that it can be updated without confusing the users.  That's a new
problem, but there are ways to solve it.

With Web-based applications, everyone uses the same version, and
bugs can be fixed as soon as they're discovered.  So Web-based
software should have far fewer bugs than desktop software.  At
Viaweb, I doubt we ever had ten known bugs at any one time.  That's
orders of magnitude better than desktop software.

Web-based applications can be used by several people at the same
time.  This is an obvious win for collaborative applications, but
I bet users will start to want this in most applications once they
realize it's possible.  It will often be useful to let two people
edit the same document, for example.  Viaweb let multiple users
edit a site simultaneously, more because that was the right way to
write the software than because we expected users to want to, but
it turned out that many did.

When you use a Web-based application, your data will be safer.
Disk crashes won't be a thing of the past, but users won't hear
about them anymore.  They'll happen within server farms.  And
companies offering Web-based applications will actually do backups--
not only because they'll have real system administrators worrying
about such things, but because an ASP that does lose people's data
will be in big, big trouble.  When people lose their own data in
a disk crash, they can't get that mad, because they only have
themselves to be mad at.  When a company loses their data for them,
they'll get a lot madder.

Finally, Web-based software should be less vulnerable to viruses.
If the client doesn't run anything except a browser, there's less
chance of running viruses, and no data locally to damage.  And a
program that attacked the servers themselves should find them very
well defended. [2]

For users, Web-based software will be less stressful.  I think if
you looked inside the average Windows user you'd find a huge and
pretty much untapped desire for software meeting that description.
Unleashed, it could be a powerful force.

City of Code

To developers, the most conspicuous difference between Web-based
and desktop software is that a Web-based application is not a single
piece of code.  It will be a collection of programs of different
types rather than a single big binary.  And so designing Web-based
software is like desiging a city rather than a building: as well
as buildings you need roads, street signs, utilities, police and
fire departments, and plans for both growth and various kinds of
disasters.

At Viaweb, software included fairly big applications that users
talked to directly, programs that those programs used, programs
that ran constantly in the background looking for problems, programs
that tried to restart things if they broke, programs that ran
occasionally to compile statistics or build indexes for searches,
programs we ran explicitly to garbage-collect resources or to move
or restore data, programs that pretended to be users (to measure
performance or expose bugs), programs for diagnosing network
troubles, programs for doing backups, interfaces to outside services,
software that drove an impressive collection of dials displaying
real-time server statistics (a hit with visitors, but indispensable
for us too), modifications (including bug fixes) to open-source
software, and a great many configuration files and settings.  Trevor
Blackwell wrote a spectacular program for moving stores to new
servers across the country, without shutting them down, after we
were bought by Yahoo. Programs paged us, sent faxes and email to
users, conducted transactions with credit card processors, and
talked to one another through sockets, pipes, http requests, ssh,
udp packets, shared memory, and files.  Some of Viaweb even consisted
of the absence of programs, since one of the keys to Unix security
is not to run unnecessary utilities that people might use to break
into your servers.

It did not end with software.  We spent a lot of time thinking
about server configurations.  We built the servers ourselves, from
components-- partly to save money, and partly to get exactly what
we wanted.  We had to think about whether our upstream ISP had fast
enough connections to all the backbones.  We serially  
dated
RAID suppliers.

But hardware is not just something to worry about.  When you control
it you can do more for users.  With a desktop application, you can
specify certain minimum hardware, but you can't add more.  If you
administer the servers, you can in one step enable all your users
to page people, or send faxes, or send commands by phone, or process
credit cards, etc, just by installing the relevant hardware.  We
always looked for new ways to add features with hardware, not just
because it pleased users, but also as a way to distinguish ourselves
from competitors who (either because they sold desktop software,
or resold Web-based applications through ISPs) didn't have direct
control over the hardware.

Because the software in a Web-based application will be a collection
of programs rather than a single binary, it can be written in any
number of different languages.  When you're writing desktop software,
you're practically forced to write the application in the same
language as the underlying operating system-- meaning C and C++.
And so these languages (especially among nontechnical people like
managers and VCs) got to be considered as the languages for "serious"
software development.  But that was just an artifact of the way
desktop software had to be delivered.  For server-based software
you can use any language you want. [3]  Today a lot of the top
hackers are using languages far removed from C and C++:  Perl,
Python, and even Lisp.

With server-based software, no one can tell you what language to
use, because you control the whole system, right down to the
hardware.  Different languages are good for different tasks.  You
can use whichever is best for each.  And when you have competitors,
"you can" means "you must" (we'll return to this later), because
if you don't take advantage of this possibility, your competitors
will.

Most of our competitors used C and C++, and this made their software
visibly inferior because (among other things), they had no way
around the statelessness of CGI scripts.  If you were going to
change something, all the changes had to happen on one page, with
an Update button at the bottom.  As I've written elsewhere, by
using Lisp, which many people still consider 
a research language,
we could make the Viaweb editor behave more like desktop software.

Releases

One of the most important changes in this new world is the way you
do releases. In the desktop software business, doing a release is
a huge trauma, in which the whole company sweats and strains to
push out a single, giant piece of code.  Obvious comparisons suggest
themselves, both to the process and the resulting product.

With server-based software, you can make changes almost as you
would in a program you were writing for yourself.  You release
software as a series of incremental changes instead of an occasional
big explosion. A typical desktop software company might do one or
two releases a year.  At Viaweb we often did three to five releases
a day.

When you switch to this new model, you realize how much software
development is affected by the way it is released.  Many of the
nastiest problems you see in the desktop software business are due
to catastrophic nature of releases.

When you release only one new version a year, you tend to deal with
bugs wholesale.  Some time before the release date you assemble a
new version in which half the code has been torn out and replaced,
introducing countless bugs.  Then a squad of QA people step in and
start counting them, and the programmers work down the list, fixing
them.  They do not generally get to the end of the list, and indeed,
no one is sure where the end is.  It's like fishing rubble out of
a pond.  You never really know what's happening inside the software.
At best you end up with a statistical sort of correctness.

With server-based software, most of the change is small and
incremental.  That in itself is less likely to introduce bugs.  It
also means you know what to test most carefully when you're about
to release software: the last thing you changed.  You end up with
a much firmer grip on the code.  As a general rule, you do know
what's happening inside it.  You don't have the source code memorized,
of course, but when you read the source you do it like a pilot
scanning the instrument panel, not like a detective trying to
unravel some mystery.

Desktop software breeds a certain fatalism about bugs.  You know
that you're shipping something loaded with bugs, and you've even
set up mechanisms to compensate for it (e.g. patch releases).  So
why worry about a few more?  Soon you're releasing whole features
you know are broken.  
Apple 
did this earlier this year.  They felt
under pressure to release their new OS, whose release date had
already slipped four times, but some of the software (support for
CDs and DVDs) wasn't ready. The solution?  They released the OS
without the unfinished parts, and users will have to install them
later.

With Web-based software, you never have to release software before
it works, and you can release it as soon as it does work.

The industry veteran may be thinking, it's a fine-sounding idea to
say that you never have to release software before it works, but
what happens when you've promised to deliver a new version of your
software by a certain date?  With Web-based software, you wouldn't
make such a promise, because there are no versions.  Your software
changes gradually and continuously.  Some changes might be bigger
than others, but the idea of versions just doesn't naturally fit
onto Web-based software.

If anyone remembers Viaweb this might sound odd, because we were
always announcing new versions.  This was done entirely for PR
purposes.  The trade press, we learned, thinks in version numbers.
They will give you major coverage for a major release, meaning a
new first digit on the version number, and generally a paragraph
at most for a point release, meaning a new digit after the decimal
point.

Some of our competitors were offering desktop software and actually
had version numbers.  And for these releases, the mere fact of
which seemed to us evidence of their backwardness, they would get
all kinds of publicity.  We didn't want to miss out, so we started
giving version numbers to our software too.  When we wanted some
publicity, we'd make a list of all the features we'd added since
the last "release," stick a new version number on the software,
and issue a press release saying that the new version was available
immediately.  Amazingly, no one ever called us on it.

By the time we were bought, we had done this three times, so we
were on Version 4.  Version 4.1 if I remember correctly.  After
Viaweb became Yahoo Store, there was no longer such a desperate
need for publicity, so although the software continued to evolve,
the whole idea of version numbers was quietly dropped.

Bugs

The other major technical advantage of Web-based software is that
you can reproduce most bugs.  You have the users' data right there
on your disk.  If someone breaks your software, you don't have to
try to guess what's going on, as you would with desktop software:
you should be able to reproduce the error while they're on the
phone with you.  You might even know about it already, if you have
code for noticing errors built into your application.

Web-based software gets used round the clock, so everything you do
is immediately put through the wringer.  Bugs turn up quickly.

Software companies are sometimes accused of letting the users debug
their software.  And that is just what I'm advocating.  For Web-based
software it's actually a good plan, because the bugs are fewer and
transient.  When you release software gradually you get far fewer
bugs to start with.  And when you can reproduce errors and release
changes instantly, you can find and fix most bugs as soon as they
appear.  We never had enough bugs at any one time to bother with
a formal bug-tracking system.

You should test changes before you release them, of course, so no
major bugs should get released.  Those few that inevitably slip
through will involve borderline cases and will only affect the few
users that encounter them before someone calls in to complain.  As
long as you fix bugs right away, the net effect, for the average
user, is far fewer bugs.  I doubt the average Viaweb user ever saw
a bug.

Fixing fresh bugs is easier than fixing old ones.  It's usually
fairly quick to find a bug in code you just wrote.  When it turns
up you often know what's wrong before you even look at the source,
because you were already worrying about it subconsciously. Fixing
a bug in something you wrote six months ago (the average case if
you release once a year) is a lot more work.  And since you don't
understand the code as well, you're more likely to fix it in an
ugly way, or even introduce more bugs. [4]

When you catch bugs early, you also get fewer compound bugs.
Compound bugs are two separate bugs that interact:  you trip going
downstairs, and when you reach for the handrail it comes off in
your hand.  In software this kind of bug is the hardest to find,
and also tends to have the worst consequences. [5]  The traditional
"break everything and then filter out the bugs" approach inherently
yields a lot of compound bugs.  And software that's released in a
series of small changes inherently tends not to. The floors are
constantly being swept clean of any loose objects that might later
get stuck in something.

It helps if you use a technique called functional programming.
Functional programming means avoiding side-effects.  It's something
you're more likely to see in research papers than commercial
software, but for Web-based applications it turns out to be really
useful.  It's hard to write entire programs as purely functional
code, but you can write substantial chunks this way.  It makes
those parts of your software easier to test, because they have no
state, and that is very convenient in a situation where you are
constantly making and testing small modifications.  I wrote much
of Viaweb's editor in this style, and we made our scripting language,
RTML, 
a purely functional language.

People from the desktop software business will find this hard to
credit, but at Viaweb bugs became almost a game.  Since most released
bugs involved borderline cases, the users who encountered them were
likely to be advanced users, pushing the envelope.  Advanced users
are more forgiving about bugs, especially since you probably
introduced them in the course of adding some feature they were
asking for.  In fact, because bugs were rare and you had to be
doing sophisticated things to see them, advanced users were often
proud to catch one.  They would call support in a spirit more of
triumph than anger, as if they had scored points off us.

Support

When you can reproduce errors, it changes your approach to customer
support.  At most software companies, support is offered as a way
to make customers feel better.  They're either calling you about
a known bug, or they're just doing something wrong and you have to
figure out what.  In either case there's not much you can learn
from them.  And so you tend to view support calls as a pain in the
ass that you want to isolate from your developers as much as
possible.

This was not how things worked at Viaweb.  At Viaweb, support was
free, because we wanted to hear from customers.  If someone had a
problem, we wanted to know about it right away so that we could
reproduce the error and release a fix.

So at Viaweb the developers were always in close contact with
support.  The customer support people were about thirty feet away
from the programmers, and knew that they could always interrupt
anything with a report of a genuine bug.  We would leave a board
meeting to fix a serious bug.

Our approach to support made everyone happier.  The customers were
delighted.  Just imagine how it would feel to call a support line
and be treated as someone bringing important news.  The customer
support people liked it because it meant they could help the users,
instead of reading scripts to them.  And the programmers liked it
because they could reproduce bugs instead of just hearing vague
second-hand reports about them.

Our policy of fixing bugs on the fly changed the relationship
between customer support people and hackers.  At most software
companies, support people are underpaid human shields, and hackers
are little copies of God the Father, creators of the world.  Whatever
the procedure for reporting bugs, it is likely to be one-directional:
support people who hear about bugs fill out some form that eventually
gets passed on (possibly via QA) to programmers, who put it on
their list of things to do.  It was very different at Viaweb.
Within a minute of hearing about a bug from a customer, the support
people could be standing next to a programmer hearing him say "Shit,
you're right, it's a bug." It delighted the support people to hear
that "you're right" from the hackers.  They used to bring us bugs
with the same expectant air as a cat bringing you a mouse it has
just killed.   It also made them more careful in judging the
seriousness of a bug, because now their honor was on the line.

After we were bought by Yahoo, the customer support people were
moved far away from the programmers.  It was only then that we
realized that they were effectively QA and to some extent marketing
as well.  In addition to catching bugs, they were the keepers of
the knowledge of vaguer, buglike things, like features that confused
users. [6]  They were also a kind of proxy focus group; we could
ask them which of two new features users wanted more, and they were
always right.

Morale

Being able to release software immediately is a big motivator.
Often as I was walking to work I would think of some change I wanted
to make to the software, and do it that day.  This worked for bigger
features as well.  Even if something was going to take two weeks
to write (few projects took longer), I knew I could see the effect
in the software as soon as it was done.

If I'd had to wait a year for the next release, I would have shelved
most of these ideas, for a while at least.  The thing about ideas,
though, is that they lead to more ideas.  Have you ever noticed
that when you sit down to write something, half the ideas that end
up in it are ones you thought of while writing it?  The same thing
happens with software.  Working to implement one idea gives you
more ideas.  So shelving an idea costs you not only that delay in
implementing it, but also all the ideas that implementing it would
have led to.  In fact, shelving an idea probably even inhibits new
ideas: as you start to think of some new feature, you catch sight
of the shelf and think "but I already have a lot of new things I
want to do for the next release."

What big companies do instead of implementing features is plan
them.  At Viaweb we sometimes ran into trouble on this account.
Investors and analysts would ask us what we had planned for the
future.  The truthful answer would have been, we didn't have any
plans.  We had general ideas about things we wanted to improve,
but if we knew how we would have done it already.  What were we
going to do in the next six months? Whatever looked like the biggest
win.  I don't know if I ever dared give this answer, but that was
the truth.  Plans are just another word for ideas on the shelf.
When we thought of good ideas, we implemented them.

At Viaweb, as at many software companies, most code had one definite
owner.  But when you owned something you really owned it: no one
except the owner of a piece of software had to approve (or even
know about) a release.  There was no protection against breakage
except the fear of looking like an idiot to one's peers, and that
was more than enough.  I may have given the impression that we just
blithely plowed forward writing code.  We did go fast, but we
thought very carefully before we released software onto those
servers.  And paying attention is more important to reliability
than moving slowly.  Because he pays close attention, a Navy pilot
can land a 40,000 lb. aircraft at 140 miles per hour on a pitching
carrier deck, at night, more safely than the average teenager can
cut a bagel.

This way of writing software is a double-edged sword of course.
It works a lot better for a small team of good, trusted programmers
than it would for a big company of mediocre ones, where bad ideas
are caught by committees instead of the people that had them.

Brooks in Reverse

Fortunately, Web-based software does require fewer programmers.
I once worked for a medium-sized desktop software company that had
over 100 people working in engineering as a whole.  Only 13 of
these were in product development.  All the rest were working on
releases, ports, and so on.  With Web-based software, all you need
(at most) are the 13 people, because there are no releases, ports,
and so on.

Viaweb was written by just three people. [7]  I was always under
pressure to hire more, because we wanted to get bought, and we knew
that buyers would have a hard time paying a high price for a company
with only three programmers.  (Solution:  we hired more, but created
new projects for them.)

When you can write software with fewer programmers, it saves you
more than money.  As Fred Brooks pointed out in The Mythical
Man-Month, adding people to a project tends to slow it down.  The
number of possible connections between developers grows exponentially
with the size of the group.  The larger the group, the more time
they'll spend in meetings negotiating how their software will work
together, and the more bugs they'll get from unforeseen interactions.
Fortunately, this process also works in reverse: as groups get
smaller, software development gets exponentially more efficient.
I can't remember the programmers at Viaweb ever having an actual
meeting.  We never had more to say at any one time than we could
say as we were walking to lunch.

If there is a downside here, it is that all the programmers have
to be to some degree system administrators as well.  When you're
hosting software, someone has to be watching the servers, and in
practice the only people who can do this properly are the ones who
wrote the software.  At Viaweb our system had so many components
and changed so frequently that there was no definite border between
software and infrastructure.  Arbitrarily declaring such a border
would have constrained our design choices.  And so although we were
constantly hoping that one day ("in a couple months") everything
would be stable enough that we could hire someone whose job was
just to worry about the servers, it never happened.

I don't think it could be any other way, as long as you're still
actively developing the product.  Web-based software is never going
to be something you write, check in, and go home.  It's a live
thing, running on your servers right now.  A bad bug might not just
crash one user's process; it could crash them all.  If a bug in
your code corrupts some data on disk, you have to fix it.  And so
on.  We found that you don't have to watch the servers every minute
(after the first year or so), but you definitely want to keep an
eye on things you've changed recently.  You don't release code late
at night and then go home.

Watching Users

With server-based software, you're in closer touch with your code.
You can also be in closer touch with your users.  Intuit is famous
for introducing themselves to customers at retail stores and asking
to follow them home.  If you've ever watched someone use your
software for the first time, you know what surprises must have
awaited them.

Software should do what users think it will.  But you can't have
any idea what users will be thinking, believe me, until you watch
them.  And server-based software gives you unprecedented information
about their behavior.  You're not limited to small, artificial
focus groups.  You can see every click made by every user.  You
have to consider carefully what you're going to look at, because
you don't want to violate users' privacy, but even the most general
statistical sampling can be very useful.

When you have the users on your server, you don't have to rely on
benchmarks, for example.  Benchmarks are simulated users.  With
server-based software, you can watch actual users.  To decide what
to optimize, just log into a server and see what's consuming all
the CPU.  And you know when to stop optimizing too: we eventually
got the Viaweb editor to the point where it was memory-bound rather
than CPU-bound, and since there was nothing we could do to decrease
the size of users' data (well, nothing easy), we knew we might as
well stop there.

Efficiency matters for server-based software, because you're paying
for the hardware.  The number of users you can support per server
is the divisor of your capital cost, so if you can make your software
very efficient you can undersell competitors and still make a
profit.  At Viaweb we got the capital cost per user down to about
$5.  It would be less now, probably less than the cost of sending
them the first month's bill.  Hardware is free now, if your software
is reasonably efficient.

Watching users can guide you in design as well as optimization.
Viaweb had a scripting language called RTML that let advanced users
define their own page styles.  We found that RTML became a kind of
suggestion box, because users only used it when the predefined page
styles couldn't do what they wanted.  Originally the editor put
button bars across the page, for example, but after a number of
users used RTML to put buttons down the left 
side, 
we made that an
option (in fact the default) in the predefined page styles.

Finally, by watching users you can often tell when they're in
trouble.  And since the customer is always right, that's a sign of
something you need to fix.  At Viaweb the key to getting users was
the online test drive.  It was not just a series of slides built
by marketing people.  In our test drive, users actually used the
software.  It took about five minutes, and at the end of it they
had built a real, working store.


The test drive was the way we got nearly all our new users.  I
think it will be the same for most Web-based applications.   If
users can get through a test drive successfully, they'll like the
product.  If they get confused or bored, they won't.  So anything
we could do to get more people through the test drive would increase
our growth rate.

I studied click trails of people taking the test drive and found
that at a certain step they would get confused and click on the
browser's Back button.  (If you try writing Web-based applications,
you'll find that the Back button becomes one of your most interesting
philosophical problems.) So I added a message at that point, telling
users that they were nearly finished, and reminding them not to
click on the Back button.  Another great thing about Web-based
software is that you get instant feedback from changes:  the number
of people completing the test drive rose immediately from 60% to
90%.  And since the number of new users was a function of the number
of completed test drives, our revenue growth increased by 50%, just
from that change.

Money

In the early 1990s I read an article in which someone said that
software was a subscription business.  At first this seemed a very
cynical statement.  But later I realized that it reflects reality:
software development is an ongoing process.  I think it's cleaner
if you openly charge subscription fees, instead of forcing people
to keep buying and installing new versions so that they'll keep
paying you.  And fortunately, subscriptions are the natural way to
bill for Web-based applications.

Hosting applications is an area where companies will play a role
that is not likely to be filled by freeware.  Hosting applications
is a lot of stress, and has real expenses.  No one is going to want
to do it for free.

For companies, Web-based applications are an ideal source of revenue.
Instead of starting each quarter with a blank slate, you have a
recurring revenue stream.  Because your software evolves gradually,
you don't have to worry that a new model will flop; there never
need be a new model, per se, and if you do something to the software
that users hate, you'll know right away.  You have no trouble with
uncollectable bills; if someone won't pay you can just turn off
the service.  And there is no possibility of piracy.

That last "advantage" may turn out to be a problem.  Some amount
of piracy is to the advantage of software companies.  If some user
really would not have bought your software at any price, you haven't
lost anything if he uses a pirated copy.  In fact you gain, because
he is one more user helping to make your software the standard--
or who might buy a copy later, when he graduates from high school.

When they can, companies like to do something called price
discrimination, which means charging each customer as much as they
can afford. [8] Software is particularly suitable for price
discrimination, because the marginal cost is close to zero.  This
is why some software costs more to run on Suns than on Intel boxes:
a company that uses Suns is not interested in saving money and can
safely be charged more.  Piracy is effectively the lowest tier of
price discrimination.  I think that software companies understand
this and deliberately turn a blind eye to some kinds of piracy. [9] 
With server-based software they are going to have to come up with 
some other solution.

Web-based software sells well, especially in comparison to desktop
software, because it's easy to buy.  You might think that people
decide to buy something, and then buy it, as two separate steps.
That's what I thought before Viaweb, to the extent I thought about
the question at all.  In fact the second step can propagate back
into the first: if something is hard to buy, people will change
their mind about whether they wanted it.  And vice versa: you'll
sell more of something when it's easy to buy.  I buy more books
because Amazon exists.  Web-based software is just about the easiest
thing in the world to buy, especially if you have just done an
online demo.  Users should not have to do much more than enter a
credit card number.  (Make them do more at your peril.)


Sometimes Web-based software is offered through ISPs acting as
resellers.  This is a bad idea.  You have to be administering the
servers, because you need to be constantly improving both hardware
and software.  If you give up direct control of the servers, you
give up most of the advantages of developing Web-based applications.

Several of our competitors shot themselves in the foot this way--
usually, I think, because they were overrun by suits who were
excited about this huge potential channel, and didn't realize that
it would ruin the product they hoped to sell through it.  Selling
Web-based software through ISPs is like selling sushi through
vending machines.

Customers

Who will the customers be?  At Viaweb they were initially individuals
and smaller companies, and I think this will be the rule with
Web-based applications.  These are the users who are ready to try
new things, partly because they're more flexible, and partly because
they want the lower costs of new technology.

Web-based applications will often be the best thing for big companies
too (though they'll be slow to realize it).  The best intranet is
the Internet.  If a company uses true Web-based applications, the
software will work better, the servers will be better administered,
and employees will have access to the system from anywhere.

The argument against this approach usually hinges on security: if
access is easier for employees, it will be for bad guys too.  Some
larger merchants were reluctant to use Viaweb because they thought
customers' credit card information would be safer on their own
servers.  It was not easy to make this point diplomatically, but
in fact the data was almost certainly safer in our hands than
theirs.  Who can hire better people to manage security, a technology
startup whose whole business is running servers, or a clothing
retailer?  Not only did we have better people worrying about
security, we worried more about it.  If someone broke into the
clothing retailer's servers, it would affect at most one merchant,
could probably be hushed up, and in the worst case might get one
person fired.  If someone broke into ours, it could affect thousands
of merchants, would probably end up as news on CNet, and could put
us out of business.

If you want to keep your money safe, do you keep it under your
mattress at home, or put it in a bank? This argument applies to
every aspect of server administration:  not just security, but
uptime, bandwidth, load management, backups, etc.  Our existence
depended on doing these things right.  Server problems were the
big no-no for us, like a dangerous toy would be for a toy maker,
or a salmonella outbreak for a food processor.

A big company that uses Web-based applications is to that extent
outsourcing IT.  Drastic as it sounds, I think this is generally
a good idea.  Companies are likely to get better service this way
than they would from in-house system administrators.  System
administrators can become cranky and unresponsive because they're
not directly exposed to competitive pressure:  a salesman has to
deal with customers, and a developer has to deal with competitors'
software, but a system administrator, like an old bachelor, has
few external forces to keep him in line. [10]  At Viaweb we had
external forces in plenty to keep us in line. The people calling
us were customers, not just co-workers.  If a server got wedged,
we jumped; just thinking about it gives me a jolt of adrenaline,
years later.

So Web-based applications will ordinarily be the right answer for
big companies too.  They will be the last to realize it, however,
just as they were with desktop computers.  And partly for the same
reason: it will be worth a lot of money to convince big companies
that they need something more expensive.

There is always a tendency for rich customers to buy expensive
solutions, even when cheap solutions are better, because the people
offering expensive solutions can spend more to sell them.  At Viaweb
we were always up against this.  We lost several high-end merchants
to Web consulting firms who convinced them they'd be better off if
they paid half a million dollars for a custom-made online store on
their own server.  They were, as a rule, not better off, as more
than one discovered when Christmas shopping season came around and
loads rose on their server.  Viaweb was a lot more sophisticated
than what most of these merchants got, but we couldn't afford to
tell them.  At $300 a month, we couldn't afford to send a team of
well-dressed and authoritative-sounding people to make presentations
to customers.

A large part of what big companies pay extra for is the cost of
selling expensive things to them.  (If the Defense Department pays
a thousand dollars for toilet seats, it's partly because it costs
a lot to sell toilet seats for a thousand dollars.)  And this is
one reason intranet software will continue to thrive, even though
it is probably a bad idea.  It's simply more expensive.  There is
nothing you can do about this conundrum, so the best plan is to go
for the smaller customers first.  The rest will come in time.

Son of Server

Running software on the server is nothing new.  In fact it's the
old model: mainframe applications are all server-based.  If
server-based software is such a good idea, why did it lose last
time?  Why did desktop computers eclipse mainframes?

At first desktop computers didn't look like much of a threat.  The
first users were all hackers-- or hobbyists, as they were called
then.  They liked microcomputers because they were cheap.  For the
first time, you could have your own computer.  The phrase "personal
computer" is part of the language now, but when it was first used
it had a deliberately audacious sound, like the phrase "personal
satellite" would today.

Why did desktop computers take over? I think it was because they
had better software.  And I think the reason microcomputer software
was better was that it could be written by small companies.

I don't think many people realize how fragile and tentative startups
are in the earliest stage.  Many startups begin almost by accident--
as a couple guys, either with day jobs or in school, writing a
prototype of something that might, if it looks promising, turn into
a company. At this larval stage, any significant obstacle will stop
the startup dead in its tracks.  Writing mainframe software required
too much commitment up front.  Development machines were expensive,
and because the customers would be big companies, you'd need an
impressive-looking sales force to sell it to them.  Starting a
startup to write mainframe software would be a much more serious
undertaking than just hacking something together on your Apple II
in the evenings.  And so you didn't get a lot of startups writing
mainframe applications.

The arrival of desktop computers inspired a lot of new software,
because writing applications for them seemed an attainable goal to
larval startups.  Development was cheap, and the customers would
be individual people that you could reach through computer stores
or even by mail-order.

The application that pushed desktop computers out into the mainstream
was VisiCalc, the 
first spreadsheet.  It was written by two guys
working in an attic, and yet did things no mainframe software could
do. [11]  VisiCalc was such an advance, in its time, that people
bought Apple IIs just to run it.  And this was the beginning of a
trend: desktop computers won because startups wrote software for
them.

It looks as if server-based software will be good this time around,
because startups will write it.  Computers are so cheap now that
you can get started, as we did, using a desktop computer as a
server.  Inexpensive processors have eaten the workstation market
(you rarely even hear the word now) and are most of the way through
the server market; Yahoo's servers, which deal with loads as high
as any on the Internet, all have the same inexpensive Intel processors
that you have in your desktop machine.  And once you've written
the software, all you need to sell it is a Web site.  Nearly all
our users came direct to our site through word of mouth and references
in the press. [12]

Viaweb was a typical larval startup.  We were terrified of starting
a company, and for the first few months comforted ourselves by
treating the whole thing as an experiment that we might call off
at any moment.  Fortunately, there were few obstacles except
technical ones.  While we were writing the software, our Web server
was the same desktop machine we used for development, connected to
the outside world by a dialup line.  Our only expenses in that
phase were food and rent.

There is all the more reason for startups to write Web-based software
now, because writing desktop software has become a lot less fun.
If you want to write desktop software now you do it on Microsoft's
terms, calling their APIs and working around their buggy OS.  And
if you manage to write something that takes off, you may find that
you were merely doing market research for Microsoft.

If a company wants to make a platform that startups will build on,
they have to make it something that hackers themselves will want
to use.   That means it has to be inexpensive and well-designed.
The Mac was popular with hackers when it first came out, and a lot
of them wrote software for it. [13] You see this less with Windows,
because hackers don't use it.  The kind of people who are good at
writing software tend to be running Linux or FreeBSD now.

I don't think we would have started a startup to write desktop
software, because desktop software has to run on Windows, and before
we could write software for Windows we'd have to use it.  The Web 
let us do an end-run around Windows, and deliver software running 
on Unix direct to users through the browser.  That is a liberating 
prospect, a lot like the arrival of PCs twenty-five years ago.

Microsoft

Back when desktop computers arrived, IBM was the giant that everyone
was afraid of.  It's hard to imagine now, but I remember the feeling
very well.  Now the frightening giant is Microsoft, and I don't
think they are as blind to the threat facing them as IBM was.
After all, Microsoft deliberately built their business in IBM's
blind spot.

I mentioned earlier that my mother doesn't really need a desktop
computer.  Most users probably don't.  That's a problem for Microsoft,
and they know it.  If applications run on remote servers, no one
needs Windows.  What will Microsoft do?  Will they be able to use
their control of the desktop to prevent, or constrain, this new
generation of software?

My guess is that Microsoft will develop some kind of server/desktop
hybrid, where the operating system works together with servers they
control.  At a minimum, files will be centrally available for users
who want that.  I don't expect Microsoft to go all the way to the
extreme of doing the computations on the server, with only a browser
for a client, if they can avoid it. If you only need a browser for
a client, you don't need Microsoft on the client, and if Microsoft
doesn't control the client, they can't push users towards their
server-based applications.

I think Microsoft will have a hard time keeping the genie in the
bottle.  There will be too many different types of clients for them
to control them all.  And if Microsoft's applications only work
with some clients, competitors will be able to trump them by offering
applications that work from any client. [14]

In a world of Web-based applications, there is no automatic place
for Microsoft.  They may succeed in making themselves a place, but
I don't think they'll dominate this new world as they did the world
of desktop applications.

It's not so much that a competitor will trip them up as that they
will trip over themselves.  With the rise of Web-based software,
they will be facing not just technical problems but their own
wishful thinking.  What they need to do is cannibalize their existing
business, and I can't see them facing that.  The same single-mindedness
that has brought them this far will now be working against them.
IBM was in exactly the same situation, and they could not master
it.  IBM made a late and half-hearted entry into the microcomputer
business because they were ambivalent about threatening their cash
cow, mainframe computing.  Microsoft will likewise be hampered by
wanting to save the desktop.  A cash cow can be a damned heavy
monkey on your back.

I'm not saying that no one will dominate server-based applications.
Someone probably will eventually.  But I think that there will be
a good long period of cheerful chaos, just as there was in the
early days of microcomputers.  That was a good time for startups.
Lots of small companies flourished, and did it by making cool
things.

Startups but More So

The classic startup is fast and informal, with few people and little
money.  Those few people work very hard, and technology magnifies
the effect of the decisions they make.  If they win, they win big.

In a startup writing Web-based applications, everything you associate
with startups is taken to an extreme.  You can write and launch a
product with even fewer people and even less money.  You have to
be even faster, and you can get away with being more informal.
You can literally launch your product as three guys sitting in the
living room of an apartment, and a server collocated at an ISP.
We did.

Over time the teams have gotten smaller, faster, and more informal.
In 1960, software development meant a roomful of men with horn
rimmed glasses and narrow black neckties, industriously writing
ten lines of code a day on IBM coding forms.  In 1980, it was a
team of eight to ten people wearing jeans to the office and typing
into vt100s.  Now it's a couple of guys sitting in a living room
with laptops.  (And jeans turn out not to be the last word in
informality.)

Startups are stressful, and this, unfortunately, is also taken to
an extreme with Web-based applications.  
Many software companies, especially at the beginning, have periods
where the developers slept under their desks and so on.  The alarming
thing about Web-based software is that there is nothing to prevent
this becoming the default.  The stories about sleeping under desks
usually end:  then at last we shipped it and we all went home and
slept for a week.  Web-based software never ships.  You can work
16-hour days for as long as you want to.  And because you can, and
your competitors can, you tend to be forced to.  You can, so you
must.  It's Parkinson's Law running in reverse.

The worst thing is not the hours but the responsibility.  Programmers
and system administrators traditionally each have their own separate
worries.  Programmers have to worry about bugs, and system
administrators have to worry about infrastructure.  Programmers
may spend a long day up to their elbows in source code, but at some
point they get to go home and forget about it.  System administrators
never quite leave the job behind, but when they do get paged at
4:00 AM, they don't usually have to do anything very complicated.
With Web-based applications, these two kinds of stress get combined.
The programmers become system administrators, but without the
sharply defined limits that ordinarily make the job bearable.

At Viaweb we spent the first six months just writing software.  We
worked the usual long hours of an early startup.  In a desktop
software company, this would have been the part where we were
working hard, but it felt like a vacation compared to the next
phase, when we took users onto our server.  The second biggest
benefit of selling Viaweb to Yahoo (after the money) was to be able
to dump ultimate responsibility for the whole thing onto the
shoulders of a big company.

Desktop software forces users to become system administrators.
Web-based software forces programmers to.  There is less stress in
total, but more for the programmers.  That's not necessarily bad
news.  If you're a startup competing with a big company, it's good
news. [15]  Web-based applications offer a straightforward way to
outwork your competitors.  No startup asks for more.

Just Good Enough

One thing that might deter you from writing Web-based applications
is the lameness of Web pages as a UI.  That is a problem, I admit.
There were a few things we would have really liked to add to
HTML and HTTP.  What matters, though, is that Web pages are just
good enough.

There is a parallel here with the first microcomputers.  The
processors in those machines weren't actually intended to be the
CPUs of computers.  They were designed to be used in things like
traffic lights.  But guys like Ed Roberts, who designed the 
Altair,
realized that they were just good enough.  You could combine one
of these chips with some memory (256 bytes in the first Altair),
and front panel switches, and you'd have a working computer.  Being
able to have your own computer was so exciting that there were
plenty of people who wanted to buy them, however limited.

Web pages weren't designed to be a UI for applications, but they're
just good enough.  And for a significant number of users, software
that you can use from any browser will be enough of a win in itself
to outweigh any awkwardness in the UI.  Maybe you can't write the
best-looking spreadsheet using HTML, but you can write a spreadsheet
that several people can use simultaneously from different locations
without special client software, or that can incorporate live data
feeds, or that can page you when certain conditions are triggered.
More importantly, you can write new kinds of applications that
don't even have names yet.  VisiCalc was not merely a microcomputer
version of a mainframe application, after all-- it was a new type
of application.

Of course, server-based applications don't have to be Web-based.
You could have some other kind of client.  But I'm pretty sure
that's a bad idea.  It would be very convenient if you could assume
that everyone would install your client-- so convenient that you
could easily convince yourself that they all would-- but if they
don't, you're hosed.  Because Web-based software assumes nothing
about the client, it will work anywhere the Web works.  That's a
big advantage already, and the advantage will grow as new Web
devices proliferate.  Users will like you because your software
just works, and your life will be easier because you won't have to
tweak it for every new client.   [16]

I feel like I've watched the evolution of the Web as closely as
anyone, and I can't predict what's going to happen with clients.
Convergence is probably coming, but where?  I can't pick a winner.
One thing I can predict is conflict between AOL and Microsoft.
Whatever Microsoft's .NET turns out to be, it will probably involve
connecting the desktop to servers.  Unless AOL fights back, they
will either be pushed aside or turned into a pipe between Microsoft
client and server software.  If Microsoft and AOL get into a client
war, the only thing sure to work on both will be browsing the Web,
meaning Web-based applications will be the only kind that work
everywhere.

How will it all play out?  I don't know.  And you don't have to
know if you bet on Web-based applications.  No one can break that
without breaking browsing.  The Web may not be the only way to
deliver software, but it's one that works now and will continue to
work for a long time.  Web-based applications are cheap to develop,
and easy for even the smallest startup to deliver.  They're a lot
of work, and of a particularly stressful kind, but that only makes
the odds better for startups.

Why Not?

E. B. White was amused to learn from a farmer friend that many
electrified fences don't have any current running through them.
The cows apparently learn to stay away from them, and after that
you don't need the current.  "Rise up, cows!" he wrote, "Take your
liberty while despots snore!"

If you're a hacker who has thought of one day starting a startup,
there are probably two things keeping you from doing it.  One is
that you don't know anything about business.  The other is that
you're afraid of competition.  Neither of these fences have any
current in them.

There are only two things you have to know about business:  build
something users love, and make more than you spend.  If you get
these two right, you'll be ahead of most startups.  You can figure
out the rest as you go.

You may not at first make more than you spend, but as long as the
gap is closing fast enough you'll be ok.  If you start out underfunded,
it will at least encourage a habit of frugality.  The less you
spend, the easier it is to make more than you spend.  Fortunately,
it can be very cheap to launch a Web-based application.  We launched
on under $10,000, and it would be even cheaper today.  We had to
spend thousands on a server, and thousands more to get SSL.  (The
only company selling SSL software at the time was Netscape.)  Now
you can rent a much more powerful server, with SSL included, for
less than we paid for bandwidth alone.  You could launch a Web-based
application now for less than the cost of a fancy office chair.

As for building something users love, here are some general tips.
Start by making something clean and simple that you would want to
use yourself.  Get a version 1.0 out fast, then continue to improve
the software, listening closely to the users as you do.  The customer
is always right, but different customers are right about different
things; the least sophisticated users show you what you need to
simplify and clarify, and the most sophisticated tell you what
features you need to add. The best thing software can be is easy,
but the way to do this is to get the defaults right, not to limit
users' choices.  Don't get complacent if your competitors' software
is lame; the standard to compare your software to is what it could
be, not what your current competitors happen to have.  Use your
software yourself, all the time.  Viaweb was supposed to be an
online store builder, but we used it to make our own site too.
Don't listen to marketing people or designers or product managers
just because of their job titles.  If they have good ideas, use
them, but it's up to you to decide; software has to be designed by
hackers who understand design, not designers who know a little
about software.  If you can't design software as well as implement
it, don't start a startup.

Now let's talk about competition.  What you're afraid of is not
presumably groups of hackers like you, but actual companies, with
offices and business plans and salesmen and so on, right?  Well,
they are more afraid of you than you are of them, and they're right.
It's a lot easier for a couple of hackers to figure out how to rent
office space or hire sales people than it is for a company of any
size to get software written.  I've been on both sides, and I know.
When Viaweb was bought by Yahoo, I suddenly found myself working
for a big company, and it was like trying to run through waist-deep
water.

I don't mean to disparage Yahoo.  They had some good hackers, and
the top management were real butt-kickers.  For a big company, they
were exceptional.  But they were still only about a tenth as
productive as a small startup.  No big company can do much better
than that.  What's scary about Microsoft is that a company so
big can develop software at all.  They're like a mountain that
can walk.

Don't be intimidated.  You can do as much that Microsoft can't as
they can do that you can't.  And no one can stop you.  You don't
have to ask anyone's permission to develop Web-based applications.
You don't have to do licensing deals, or get shelf space in retail
stores, or grovel to have your application bundled with the OS.
You can deliver software right to the browser, and no one can get
between you and potential users without preventing them from browsing
the Web.

You may not believe it, but I promise you, Microsoft is scared of
you.  The complacent middle managers may not be, but Bill is,
because he was you once, back in 1975, the last time a new way of
delivering software appeared.





Notes

[1] Realizing that much of the money is in the services, companies
building lightweight clients have usually tried to combine the
hardware with an 
online service. 
 This approach has not worked
well, partly because you need two different kinds of companies to
build consumer electronics and to run an online service, and partly
because users hate the idea.  Giving away the razor and making
money on the blades may work for Gillette, but a razor is much
smaller commitment than a Web terminal.  Cell phone handset makers
are satisfied to sell hardware without trying to capture the service
revenue as well.  That should probably be the model for Internet
clients too.  If someone just sold a nice-looking little box with
a Web browser that you could use to connect through any ISP, every
technophobe in the country would buy one.

[2] Security always depends more on not screwing up than any design
decision, but the nature of server-based software will make developers
pay more attention to not screwing up.  Compromising a server could
cause such damage that ASPs (that want to stay in business) are
likely to be careful about security.

[3] In 1995, when we started Viaweb, Java applets were supposed to
be the technology everyone was going to use to develop server-based
applications.  Applets seemed to us an old-fashioned idea.  Download
programs to run on the client? Simpler just to go all the way and
run the programs on the server.  We wasted little time
on applets, but countless other startups must have been lured into
this tar pit.  Few can have escaped alive, or Microsoft could not
have gotten away with dropping Java in the most recent version of
Explorer.

[4] This point is due to Trevor Blackwell, who adds "the cost of
writing software goes up more than linearly with its size.  Perhaps
this is mainly due to fixing old bugs, and the cost can be more
linear if all bugs are found quickly."

[5] The hardest kind of bug to find may be a variant of compound
bug where one bug happens to compensate for another.  When you fix
one bug, the other becomes visible.  But it will seem as if the
fix is at fault, since that was the last thing you changed.

[6] Within Viaweb we once had a contest to describe the worst thing
about our software.  Two customer support people tied for first
prize with entries I still shiver to recall.  We fixed both problems
immediately.

[7] Robert Morris wrote the ordering system, which shoppers used
to place orders. Trevor Blackwell wrote the image generator and
the manager, which merchants used to retrieve orders, view statistics,
and configure domain names etc.  I wrote the editor, which merchants
used to build their sites.  The ordering system and image generator
were written in C and C++, the manager mostly in Perl, and the editor
in Lisp.

[8] Price discrimination is so pervasive (how often have you heard
a retailer claim that their buying power meant lower prices for
you?) that I was surprised to find it was outlawed in the U.S. by
the Robinson-Patman Act of 1936.  This law does not appear to be
vigorously enforced.

[9] In No Logo, Naomi Klein says that clothing brands favored by
"urban youth" do not try too hard to prevent shoplifting because
in their target market the shoplifters are also the fashion leaders.

[10] Companies often wonder what to outsource and what not to.
One possible answer: outsource any job that's not directly exposed
to competitive pressure, because outsourcing it will thereby expose
it to competitive pressure.

[11] The two guys were Dan Bricklin and Bob Frankston.  Dan wrote
a prototype in Basic in a couple days, then over the course of the
next year they worked together (mostly at night) to make a more
powerful version written in 6502 machine language.  Dan was at
Harvard Business School at the time and Bob nominally had a day
job writing software.  "There was no great risk in doing a business,"
Bob wrote, "If it failed it failed. No big deal."

[12] It's not quite as easy as I make it sound.  It took a painfully
long time for word of mouth to get going, and we did not start to
get a lot of press coverage until we hired a 
PR firm 
(admittedly
the best in the business) for $16,000 per month.  However, it was
true that the only significant channel was our own Web site.

[13] If the Mac was so great, why did it lose?  Cost, again.
Microsoft concentrated on the software business, and unleashed a
swarm of cheap component suppliers on Apple hardware.  It did not
help, either, that suits took over during a critical period.

[14] One thing that would help Web-based applications, and help
keep the next generation of software from being overshadowed by
Microsoft, would be a good open-source browser.  Mozilla is
open-source but seems to have suffered from having been corporate
software for so long.  A small, fast browser that was actively
maintained would be a great thing in itself, and would probably
also encourage companies to build little Web appliances.

Among other things, a proper open-source browser would cause HTTP
and HTML to continue to evolve (as e.g. Perl has).  It would help
Web-based applications greatly to be able to distinguish between
selecting a link and following it; all you'd need to do this would
be a trivial enhancement of HTTP, to allow multiple urls in a
request.  Cascading menus would also be good.

If you want to change the world, write a new Mosaic.  Think it's
too late?  In 1998 a lot of people thought it was too late to launch
a new search engine, but Google proved them wrong.  There is always
room for something new if the current options suck enough.  Make
sure it works on all the free OSes first-- new things start with
their users.

[15] Trevor Blackwell, who probably knows more about this from
personal experience than anyone, writes:

"I would go farther in saying that because server-based software
is so hard on the programmers, it causes a fundamental economic
shift away from large companies. It requires the kind of intensity
and dedication from programmers that they will only be willing to
provide when it's their own company.  Software companies can hire
skilled people to work in a not-too-demanding environment, and can
hire unskilled people to endure hardships, but they can't hire
highly skilled people to bust their asses. Since capital is no
longer needed, big companies have little to bring to the table."

[16] In the original version of this essay, I advised avoiding
Javascript.  That was a good plan in 2001, but Javascript now works.


Thanks to Sarah Harlin, Trevor Blackwell, Robert Morris, Eric Raymond, Ken Anderson,
and Dan Giffin for reading drafts of this paper; to Dan Bricklin and
Bob Frankston for information about VisiCalc; and again to Ken Anderson
for inviting me to speak at BBN.



You'll find this essay and 14 others in
Hackers & Painters.


May 2001

(I wrote this article to help myself understand exactly
what McCarthy discovered.  You don't need to know this stuff
to program in Lisp, but it should be helpful to 
anyone who wants to
understand the essence of Lisp  both in the sense of its
origins and its semantic core.  The fact that it has such a core
is one of Lisp's distinguishing features, and the reason why,
unlike other languages, Lisp has dialects.)

In 1960, John 
McCarthy published a remarkable paper in
which he did for programming something like what Euclid did for
geometry. He showed how, given a handful of simple
operators and a notation for functions, you can
build a whole programming language.
He called this language Lisp, for "List Processing,"
because one of his key ideas was to use a simple
data structure called a list for both
code and data.

It's worth understanding what McCarthy discovered, not
just as a landmark in the history of computers, but as
a model for what programming is tending to become in
our own time.  It seems to me that there have been
two really clean, consistent models of programming so
far: the C model and the Lisp model.
These two seem points of high ground, with swampy lowlands
between them.  As computers have grown more powerful,
the new languages being developed have been moving
steadily toward the Lisp model.  A popular recipe
for new programming languages in the past 20 years 
has been to take the C model of computing and add to
it, piecemeal, parts taken from the Lisp model,
like runtime typing and garbage collection.

In this article I'm going to try to explain in the
simplest possible terms what McCarthy discovered.
The point is not just to learn about an interesting
theoretical result someone figured out forty years ago,
but to show where languages are heading.
The unusual thing about Lisp  in fact, the defining
quality of Lisp  is that it can be written in
itself.  To understand what McCarthy meant by this,
we're going to retrace his steps, with his mathematical
notation translated into running Common Lisp code.
May 2001

(These are some notes I made
for a panel discussion on programming language design
at MIT on May 10, 2001.)





1. Programming Languages Are for People.

Programming languages
are how people talk to computers.  The computer would be just as
happy speaking any language that was unambiguous.  The reason we
have high level languages is because people can't deal with
machine language.  The point of programming
languages is to prevent our poor frail human brains from being 
overwhelmed by a mass of detail.

Architects know that some kinds of design problems are more personal
than others.  One of the cleanest, most abstract design problems
is designing bridges.  There your job is largely a matter of spanning
a given distance with the least material.  The other end of the
spectrum is designing chairs.  Chair designers have to spend their
time thinking about human butts.

Software varies in the same way. Designing algorithms for routing
data through a network is a nice, abstract problem, like designing
bridges.  Whereas designing programming languages is like designing
chairs: it's all about dealing with human weaknesses.

Most of us hate to acknowledge this.  Designing systems of great
mathematical elegance sounds a lot more appealing to most of us
than pandering to human weaknesses.  And there is a role for mathematical
elegance: some kinds of elegance make programs easier to understand.
But elegance is not an end in itself.

And when I say languages have to be designed to suit human weaknesses,
I don't mean that languages have to be designed for bad programmers.
In fact I think you ought to design for the 
best programmers, but
even the best programmers have limitations.  I don't think anyone
would like programming in a language where all the variables were
the letter x with integer subscripts.

2. Design for Yourself and Your Friends.

If you look at the history of programming languages, a lot of the best
ones were languages designed for their own authors to use, and a
lot of the worst ones were designed for other people to use.

When languages are designed for other people, it's always a specific
group of other people: people not as smart as the language designer.
So you get a language that talks down to you.  Cobol is the most
extreme case, but a lot of languages are pervaded by this spirit.

It has nothing to do with how abstract the language is.  C is pretty
low-level, but it was designed for its authors to use, and that's
why hackers like it.

The argument for designing languages for bad programmers is that
there are more bad programmers than good programmers.  That may be
so.  But those few good programmers write a disproportionately
large percentage of the software.

I'm interested in the question, how do you design a language that
the very best hackers will like?  I happen to think this is
identical to the question, how do you design a good programming
language?, but even if it isn't, it is at least an interesting
question.

3. Give the Programmer as Much Control as Possible.

Many languages
(especially the ones designed for other people) have the attitude
of a governess: they try to prevent you from
doing things that they think aren't good for you.  I like the   
opposite approach: give the programmer as much
control as you can.

When I first learned Lisp, what I liked most about it was
that it considered me an equal partner.  In the other languages
I had learned up till then, there was the language and there was my   
program, written in the language, and the two were very separate.
But in Lisp the functions and macros I wrote were just like those
that made up the language itself.  I could rewrite the language
if I wanted.  It had the same appeal as open-source software.

4. Aim for Brevity.

Brevity is underestimated and even scorned.
But if you look into the hearts of hackers, you'll see that they
really love it.  How many times have you heard hackers speak fondly
of how in, say, APL, they could do amazing things with just a couple
lines of code?  I think anything that really smart people really
love is worth paying attention to.

I think almost anything
you can do to make programs shorter is good.  There should be lots
of library functions; anything that can be implicit should be;
the syntax should be terse to a fault; even the names of things
should be short.

And it's not only programs that should be short.  The manual should
be thin as well.  A good part of manuals is taken up with clarifications
and reservations and warnings and special cases.  If you force  
yourself to shorten the manual, in the best case you do it by fixing
the things in the language that required so much explanation.

5. Admit What Hacking Is.

A lot of people wish that hacking was
mathematics, or at least something like a natural science.  I think
hacking is more like architecture.  Architecture is
related to physics, in the sense that architects have to design
buildings that don't fall down, but the actual goal of architects
is to make great buildings, not to make discoveries about statics.

What hackers like to do is make great programs.
And I think, at least in our own minds, we have to remember that it's
an admirable thing to write great programs, even when this work 
doesn't translate easily into the conventional intellectual
currency of research papers.  Intellectually, it is just as
worthwhile to design a language programmers will love as it is to design a
horrible one that embodies some idea you can publish a paper
about.





1. How to Organize Big Libraries?

Libraries are becoming an
increasingly important component of programming languages.  They're
also getting bigger, and this can be dangerous.  If it takes longer
to find the library function that will do what you want than it
would take to write it yourself, then all that code is doing nothing
but make your manual thick.  (The Symbolics manuals were a case in 
point.)  So I think we will have to work on ways to organize
libraries.  The ideal would be to design them so that the programmer
could guess what library call would do the right thing.

2. Are People Really Scared of Prefix Syntax?

This is an open
problem in the sense that I have wondered about it for years and
still don't know the answer.  Prefix syntax seems perfectly natural
to me, except possibly for math.  But it could be that a lot of 
Lisp's unpopularity is simply due to having an unfamiliar syntax.   
Whether to do anything about it, if it is true, is another question. 

3. What Do You Need for Server-Based Software?

I think a lot of the most exciting new applications that get written
in the next twenty years will be Web-based applications, meaning
programs that sit on the server and talk to you through a Web
browser.  And to write these kinds of programs we may need some
new things.

One thing we'll need is support for the new way that server-based 
apps get released.  Instead of having one or two big releases a
year, like desktop software, server-based apps get released as a
series of small changes.  You may have as many as five or ten
releases a day.  And as a rule everyone will always use the latest
version.

You know how you can design programs to be debuggable?
Well, server-based software likewise has to be designed to be
changeable.  You have to be able to change it easily, or at least
to know what is a small change and what is a momentous one.

Another thing that might turn out to be useful for server based
software, surprisingly, is continuations.  In Web-based software
you can use something like continuation-passing style to get the
effect of subroutines in the inherently 
stateless world of a Web
session.  Maybe it would be worthwhile having actual continuations,
if it was not too expensive.

4. What New Abstractions Are Left to Discover?

I'm not sure how
reasonable a hope this is, but one thing I would really love to    
do, personally, is discover a new abstraction-- something that would
make as much of a difference as having first class functions or
recursion or even keyword parameters.  This may be an impossible
dream.  These things don't get discovered that often.  But I am always
looking.





1. You Can Use Whatever Language You Want.

Writing application
programs used to mean writing desktop software.  And in desktop
software there is a big bias toward writing the application in the
same language as the operating system.  And so ten years ago,
writing software pretty much meant writing software in C.
Eventually a tradition evolved:
application programs must not be written in unusual languages.  
And this tradition had so long to develop that nontechnical people
like managers and venture capitalists also learned it.

Server-based software blows away this whole model.  With server-based
software you can use any language you want.  Almost nobody understands
this yet (especially not managers and venture capitalists).
A few hackers understand it, and that's why we even hear
about new, indy languages like Perl and Python.  We're not hearing
about Perl and Python because people are using them to write Windows
apps.

What this means for us, as people interested in designing programming
languages, is that there is now potentially an actual audience for
our work.

2. Speed Comes from Profilers.

Language designers, or at least
language implementors, like to write compilers that generate fast
code.  But I don't think this is what makes languages fast for users.
Knuth pointed out long ago that speed only matters in a few critical
bottlenecks.  And anyone who's tried it knows that you can't guess
where these bottlenecks are.  Profilers are the answer.

Language designers are solving the wrong problem.  Users don't need
benchmarks to run fast.  What they need is a language that can show
them what parts of their own programs need to be rewritten.  That's
where speed comes from in practice.  So maybe it would be a net 
win if language implementors took half the time they would
have spent doing compiler optimizations and spent it writing a
good profiler instead.

3. You Need an Application to Drive the Design of a Language.

This may not be an absolute rule, but it seems like the best languages
all evolved together with some application they were being used to
write.  C was written by people who needed it for systems programming.
Lisp was developed partly to do symbolic differentiation, and
McCarthy was so eager to get started that he was writing differentiation
programs even in the first paper on Lisp, in 1960.

It's especially good if your application solves some new problem.
That will tend to drive your language to have new features that   
programmers need.  I personally am interested in writing
a language that will be good for writing server-based applications.

[During the panel, Guy Steele also made this point, with the
additional suggestion that the application should not consist of
writing the compiler for your language, unless your language
happens to be intended for writing compilers.]

4. A Language Has to Be Good for Writing Throwaway Programs.

You know what a throwaway program is: something you write quickly for
some limited task.  I think if you looked around you'd find that  
a lot of big, serious programs started as throwaway programs.  I
would not be surprised if most programs started as throwaway
programs.  And so if you want to make a language that's good for
writing software in general, it has to be good for writing throwaway
programs, because that is the larval stage of most software.

5. Syntax Is Connected to Semantics.

It's traditional to think of
syntax and semantics as being completely separate.  This will
sound shocking, but it may be that they aren't.
I think that what you want in your language may be related
to how you express it.

I was talking recently to Robert Morris, and he pointed out that
operator overloading is a bigger win in languages with infix
syntax.  In a language with prefix syntax, any function you define
is effectively an operator.  If you want to define a plus for a
new type of number you've made up, you can just define a new function
to add them.  If you do that in a language with infix syntax,
there's a big difference in appearance between the use of an
overloaded operator and a function call.





1. New Programming Languages.

Back in the 1970s
it was fashionable to design new programming languages.  Recently
it hasn't been.  But I think server-based software will make new  
languages fashionable again.  With server-based software, you can
use any language you want, so if someone does design a language that
actually seems better than others that are available, there will be
people who take a risk and use it.

2. Time-Sharing.

Richard Kelsey gave this as an idea whose time
has come again in the last panel, and I completely agree with him.
My guess (and Microsoft's guess, it seems) is that much computing
will move from the desktop onto remote servers.  In other words,  
time-sharing is back.  And I think there will need to be support
for it at the language level.  For example, I know that Richard
and Jonathan Rees have done a lot of work implementing process  
scheduling within Scheme 48.

3. Efficiency.

Recently it was starting to seem that computers
were finally fast enough.  More and more we were starting to hear
about byte code, which implies to me at least that we feel we have
cycles to spare.  But I don't think we will, with server-based
software.   Someone is going to have to pay for the servers that
the software runs on, and the number of users they can support per
machine will be the divisor of their capital cost.

So I think efficiency will matter, at least in computational
bottlenecks.  It will be especially important to do i/o fast,
because server-based applications do a lot of i/o.

It may turn out that byte code is not a win, in the end.  Sun and
Microsoft seem to be facing off in a kind of a battle of the byte
codes at the moment.  But they're doing it because byte code is a
convenient place to insert themselves into the process, not because
byte code is in itself a good idea.  It may turn out that this
whole battleground gets bypassed.  That would be kind of amusing.





1. Clients.

This is just a guess, but my guess is that
the winning model for most applications will be purely server-based.
Designing software that works on the assumption that everyone will 
have your client is like designing a society on the assumption that
everyone will just be honest.  It would certainly be convenient, but
you have to assume it will never happen.

I think there will be a proliferation of devices that have some
kind of Web access, and all you'll be able to assume about them is
that they can support simple html and forms.  Will you have a
browser on your cell phone?  Will there be a phone in your palm  
pilot?  Will your blackberry get a bigger screen? Will you be able
to browse the Web on your gameboy?  Your watch?  I don't know.  
And I don't have to know if I bet on
everything just being on the server.  It's
just so much more robust to have all the 
brains on the server.

2. Object-Oriented Programming.

I realize this is a
controversial one, but I don't think object-oriented programming
is such a big deal.  I think it is a fine model for certain kinds
of applications that need that specific kind of data structure,   
like window systems, simulations, and cad programs.  But I don't
see why it ought to be the model for all programming.

I think part of the reason people in big companies like object-oriented
programming is because it yields a lot of what looks like work.
Something that might naturally be represented as, say, a list of
integers, can now be represented as a class with all kinds of
scaffolding and hustle and bustle.

Another attraction of
object-oriented programming is that methods give you some of the
effect of first class functions.  But this is old news to Lisp
programmers.  When you have actual first class functions, you can
just use them in whatever way is appropriate to the task at hand,
instead of forcing everything into a mold of classes and methods.

What this means for language design, I think, is that you shouldn't
build object-oriented programming in too deeply.  Maybe the
answer is to offer more general, underlying stuff, and let people design
whatever object systems they want as libraries.

3. Design by Committee.

Having your language designed by a committee is a big pitfall,  
and not just for the reasons everyone knows about.  Everyone
knows that committees tend to yield lumpy, inconsistent designs.  
But I think a greater danger is that they won't take risks.
When one person is in charge he can take risks
that a committee would never agree on.

Is it necessary to take risks to design a good language though?
Many people might suspect
that language design is something where you should stick fairly
close to the conventional wisdom.  I bet this isn't true.
In everything else people do, reward is proportionate to risk.
Why should language design be any different?
May 2001

(This article was written as a kind of business plan for a
new language.
So it is missing (because it takes for granted) the most important
feature of a good programming language: very powerful abstractions.)

A friend of mine once told an eminent operating systems
expert that he wanted to design a really good
programming language.  The expert told him that it would be a
waste of time, that programming languages don't become popular
or unpopular based on their merits, and so no matter how
good his language was, no one would use it.  At least, that
was what had happened to the language he had designed.

What does make a language popular?  Do popular
languages deserve their popularity?  Is it worth trying to
define a good programming language?  How would you do it?

I think the answers to these questions can be found by looking 
at hackers, and learning what they want.  Programming
languages are for hackers, and a programming language
is good as a programming language (rather than, say, an
exercise in denotational semantics or compiler design)
if and only if hackers like it.

1 The Mechanics of Popularity

It's true, certainly, that most people don't choose programming
languages simply based on their merits.  Most programmers are told
what language to use by someone else.  And yet I think the effect
of such external factors on the popularity of programming languages
is not as great as it's sometimes thought to be. I think a bigger
problem is that a hacker's idea of a good programming language is
not the same as most language designers'.

Between the two, the hacker's opinion is the one that matters.
Programming languages are not theorems. They're tools, designed
for people, and they have to be designed to suit human strengths
and weaknesses as much as shoes have to be designed for human feet.
If a shoe pinches when you put it on, it's a bad shoe, however
elegant it may be as a piece of sculpture.

It may be that the majority of programmers can't tell a good language
from a bad one. But that's no different with any other tool. It
doesn't mean that it's a waste of time to try designing a good
language. Expert hackers 
can tell a good language when they see
one, and they'll use it. Expert hackers are a tiny minority,
admittedly, but that tiny minority write all the good software,
and their influence is such that the rest of the programmers will
tend to use whatever language they use. Often, indeed, it is not
merely influence but command: often the expert hackers are the very
people who, as their bosses or faculty advisors, tell the other
programmers what language to use.

The opinion of expert hackers is not the only force that determines
the relative popularity of programming languages  legacy software
(Cobol) and hype (Ada, Java) also play a role  but I think it is
the most powerful force over the long term. Given an initial critical
mass and enough time, a programming language probably becomes about
as popular as it deserves to be. And popularity further separates
good languages from bad ones, because feedback from real live users
always leads to improvements. Look at how much any popular language
has changed during its life. Perl and Fortran are extreme cases,
but even Lisp has changed a lot. Lisp 1.5 didn't have macros, for
example; these evolved later, after hackers at MIT had spent a
couple years using Lisp to write real programs. [1]

So whether or not a language has to be good to be popular, I think
a language has to be popular to be good. And it has to stay popular
to stay good. The state of the art in programming languages doesn't
stand still. And yet the Lisps we have today are still pretty much
what they had at MIT in the mid-1980s, because that's the last time
Lisp had a sufficiently large and demanding user base.

Of course, hackers have to know about a language before they can
use it. How are they to hear? From other hackers. But there has to
be some initial group of hackers using the language for others even
to hear about it. I wonder how large this group has to be; how many
users make a critical mass? Off the top of my head, I'd say twenty.
If a language had twenty separate users, meaning twenty users who
decided on their own to use it, I'd consider it to be real.

Getting there can't be easy. I would not be surprised if it is
harder to get from zero to twenty than from twenty to a thousand.
The best way to get those initial twenty users is probably to use
a trojan horse: to give people an application they want, which
happens to be written in the new language.

2 External Factors

Let's start by acknowledging one external factor that does affect
the popularity of a programming language. To become popular, a
programming language has to be the scripting language of a popular
system. Fortran and Cobol were the scripting languages of early
IBM mainframes. C was the scripting language of Unix, and so, later,
was Perl. Tcl is the scripting language of Tk. Java and Javascript
are intended to be the scripting languages of web browsers.

Lisp is not a massively popular language because it is not the
scripting language of a massively popular system. What popularity
it retains dates back to the 1960s and 1970s, when it was the
scripting language of MIT. A lot of the great programmers of the
day were associated with MIT at some point. And in the early 1970s,
before C, MIT's dialect of Lisp, called MacLisp, was one of the
only programming languages a serious hacker would want to use.

Today Lisp is the scripting language of two moderately popular
systems, Emacs and Autocad, and for that reason I suspect that most
of the Lisp programming done today is done in Emacs Lisp or AutoLisp.

Programming languages don't exist in isolation. To hack is a
transitive verb  hackers are usually hacking something  and in
practice languages are judged relative to whatever they're used to
hack. So if you want to design a popular language, you either have
to supply more than a language, or you have to design your language
to replace the scripting language of some existing system.

Common Lisp is unpopular partly because it's an orphan. It did
originally come with a system to hack: the Lisp Machine. But Lisp
Machines (along with parallel computers) were steamrollered by the
increasing power of general purpose processors in the 1980s. Common
Lisp might have remained popular if it had been a good scripting
language for Unix. It is, alas, an atrociously bad one.

One way to describe this situation is to say that a language isn't
judged on its own merits. Another view is that a programming language
really isn't a programming language unless it's also the scripting
language of something. This only seems unfair if it comes as a
surprise. I think it's no more unfair than expecting a programming
language to have, say, an implementation. It's just part of what
a programming language is.

A programming language does need a good implementation, of course,
and this must be free. Companies will pay for software, but individual
hackers won't, and it's the hackers you need to attract.

A language also needs to have a book about it. The book should be
thin, well-written, and full of good examples. K&R is the ideal
here. At the moment I'd almost say that a language has to have a
book published by O'Reilly. That's becoming the test of mattering
to hackers.

There should be online documentation as well. In fact, the book
can start as online documentation. But I don't think that physical
books are outmoded yet. Their format is convenient, and the de
facto censorship imposed by publishers is a useful if imperfect
filter. Bookstores are one of the most important places for learning
about new languages.

3 Brevity

Given that you can supply the three things any language needs  a
free implementation, a book, and something to hack  how do you
make a language that hackers will like?

One thing hackers like is brevity. Hackers are lazy, in the same
way that mathematicians and modernist architects are lazy: they
hate anything extraneous. It would not be far from the truth to
say that a hacker about to write a program decides what language
to use, at least subconsciously, based on the total number of
characters he'll have to type. If this isn't precisely how hackers
think, a language designer would do well to act as if it were.

It is a mistake to try to baby the user with long-winded expressions
that are meant to resemble English. Cobol is notorious for this
flaw. A hacker would consider being asked to write

add x to y giving z

instead of

z = x+y

as something between an insult to his intelligence and a sin against
God.

It has sometimes been said that Lisp should use first and rest
instead of car and cdr, because it would make programs easier to
read. Maybe for the first couple hours. But a hacker can learn
quickly enough that car means the first element of a list and cdr
means the rest. Using first and rest means 50% more typing. And
they are also different lengths, meaning that the arguments won't
line up when they're called, as car and cdr often are, in successive
lines. I've found that it matters a lot how code lines up on the
page. I can barely read Lisp code when it is set in a variable-width
font, and friends say this is true for other languages too.

Brevity is one place where strongly typed languages lose. All other
things being equal, no one wants to begin a program with a bunch
of declarations. Anything that can be implicit, should be.

The individual tokens should be short as well. Perl and Common Lisp
occupy opposite poles on this question. Perl programs can be almost
cryptically dense, while the names of built-in Common Lisp operators
are comically long. The designers of Common Lisp probably expected
users to have text editors that would type these long names for
them. But the cost of a long name is not just the cost of typing
it. There is also the cost of reading it, and the cost of the space
it takes up on your screen.

4 Hackability

There is one thing more important than brevity to a hacker: being
able to do what you want. In the history of programming languages
a surprising amount of effort has gone into preventing programmers
from doing things considered to be improper. This is a dangerously
presumptuous plan. How can the language designer know what the
programmer is going to need to do? I think language designers would
do better to consider their target user to be a genius who will
need to do things they never anticipated, rather than a bumbler
who needs to be protected from himself. The bumbler will shoot
himself in the foot anyway. You may save him from referring to
variables in another package, but you can't save him from writing
a badly designed program to solve the wrong problem, and taking
forever to do it.

Good programmers often want to do dangerous and unsavory things.
By unsavory I mean things that go behind whatever semantic facade
the language is trying to present: getting hold of the internal
representation of some high-level abstraction, for example. Hackers
like to hack, and hacking means getting inside things and second
guessing the original designer.

Let yourself be second guessed. When you make any tool, people use
it in ways you didn't intend, and this is especially true of a
highly articulated tool like a programming language. Many a hacker
will want to tweak your semantic model in a way that you never
imagined. I say, let them; give the programmer access to as much
internal stuff as you can without endangering runtime systems like
the garbage collector.

In Common Lisp I have often wanted to iterate through the fields
of a struct  to comb out references to a deleted object, for example,
or find fields that are uninitialized. I know the structs are just
vectors underneath. And yet I can't write a general purpose function
that I can call on any struct. I can only access the fields by
name, because that's what a struct is supposed to mean.

A hacker may only want to subvert the intended model of things once
or twice in a big program. But what a difference it makes to be
able to. And it may be more than a question of just solving a
problem. There is a kind of pleasure here too. Hackers share the
surgeon's secret pleasure in poking about in gross innards, the
teenager's secret pleasure in popping zits. [2] For boys, at least,
certain kinds of horrors are fascinating. Maxim magazine publishes
an annual volume of photographs, containing a mix of pin-ups and
grisly accidents. They know their audience.

Historically, Lisp has been good at letting hackers have their way.
The political correctness of Common Lisp is an aberration. Early
Lisps let you get your hands on everything. A good deal of that
spirit is, fortunately, preserved in macros. What a wonderful thing,
to be able to make arbitrary transformations on the source code.

Classic macros are a real hacker's tool  simple, powerful, and
dangerous. It's so easy to understand what they do: you call a
function on the macro's arguments, and whatever it returns gets
inserted in place of the macro call. Hygienic macros embody the
opposite principle. They try to protect you from understanding what
they're doing. I have never heard hygienic macros explained in one
sentence. And they are a classic example of the dangers of deciding
what programmers are allowed to want. Hygienic macros are intended
to protect me from variable capture, among other things, but variable
capture is exactly what I want in some macros.

A really good language should be both clean and dirty: cleanly
designed, with a small core of well understood and highly orthogonal
operators, but dirty in the sense that it lets hackers have their
way with it. C is like this. So were the early Lisps. A real hacker's
language will always have a slightly raffish character.

A good programming language should have features that make the kind
of people who use the phrase "software engineering" shake their
heads disapprovingly. At the other end of the continuum are languages
like Ada and Pascal, models of propriety that are good for teaching
and not much else.

5 Throwaway Programs

To be attractive to hackers, a language must be good for writing
the kinds of programs they want to write. And that means, perhaps
surprisingly, that it has to be good for writing throwaway programs.

A throwaway program is a program you write quickly for some limited
task: a program to automate some system administration task, or
generate test data for a simulation, or convert data from one format
to another. The surprising thing about throwaway programs is that,
like the "temporary" buildings built at so many American universities
during World War II, they often don't get thrown away. Many evolve
into real programs, with real features and real users.

I have a hunch that the best big programs begin life this way,
rather than being designed big from the start, like the Hoover Dam.
It's terrifying to build something big from scratch. When people
take on a project that's too big, they become overwhelmed. The
project either gets bogged down, or the result is sterile and
wooden: a shopping mall rather than a real downtown, Brasilia rather
than Rome, Ada rather than C.

Another way to get a big program is to start with a throwaway
program and keep improving it. This approach is less daunting, and
the design of the program benefits from evolution. I think, if one
looked, that this would turn out to be the way most big programs
were developed. And those that did evolve this way are probably
still written in whatever language they were first written in,
because it's rare for a program to be ported, except for political
reasons. And so, paradoxically, if you want to make a language that
is used for big systems, you have to make it good for writing
throwaway programs, because that's where big systems come from.

Perl is a striking example of this idea. It was not only designed
for writing throwaway programs, but was pretty much a throwaway
program itself. Perl began life as a collection of utilities for
generating reports, and only evolved into a programming language
as the throwaway programs people wrote in it grew larger. It was
not until Perl 5 (if then) that the language was suitable for
writing serious programs, and yet it was already massively popular.

What makes a language good for throwaway programs? To start with,
it must be readily available. A throwaway program is something that
you expect to write in an hour. So the language probably must
already be installed on the computer you're using. It can't be
something you have to install before you use it. It has to be there.
C was there because it came with the operating system. Perl was
there because it was originally a tool for system administrators,
and yours had already installed it.

Being available means more than being installed, though. An
interactive language, with a command-line interface, is more
available than one that you have to compile and run separately. A
popular programming language should be interactive, and start up
fast.

Another thing you want in a throwaway program is brevity. Brevity
is always attractive to hackers, and never more so than in a program
they expect to turn out in an hour.

6 Libraries

Of course the ultimate in brevity is to have the program already
written for you, and merely to call it. And this brings us to what
I think will be an increasingly important feature of programming
languages: library functions. Perl wins because it has large
libraries for manipulating strings. This class of library functions
are especially important for throwaway programs, which are often
originally written for converting or extracting data.  Many Perl
programs probably begin as just a couple library calls stuck
together.

I think a lot of the advances that happen in programming languages
in the next fifty years will have to do with library functions. I
think future programming languages will have libraries that are as
carefully designed as the core language. Programming language design
will not be about whether to make your language strongly or weakly
typed, or object oriented, or functional, or whatever, but about
how to design great libraries. The kind of language designers who
like to think about how to design type systems may shudder at this.
It's almost like writing applications! Too bad. Languages are for
programmers, and libraries are what programmers need.

It's hard to design good libraries. It's not simply a matter of
writing a lot of code. Once the libraries get too big, it can
sometimes take longer to find the function you need than to write
the code yourself. Libraries need to be designed using a small set
of orthogonal operators, just like the core language. It ought to
be possible for the programmer to guess what library call will do
what he needs.

Libraries are one place Common Lisp falls short. There are only
rudimentary libraries for manipulating strings, and almost none
for talking to the operating system. For historical reasons, Common
Lisp tries to pretend that the OS doesn't exist. And because you
can't talk to the OS, you're unlikely to be able to write a serious
program using only the built-in operators in Common Lisp. You have
to use some implementation-specific hacks as well, and in practice
these tend not to give you everything you want. Hackers would think
a lot more highly of Lisp if Common Lisp had powerful string
libraries and good OS support.

7 Syntax

Could a language with Lisp's syntax, or more precisely, lack of
syntax, ever become popular? I don't know the answer to this
question. I do think that syntax is not the main reason Lisp isn't
currently popular. Common Lisp has worse problems than unfamiliar
syntax. I know several programmers who are comfortable with prefix
syntax and yet use Perl by default, because it has powerful string
libraries and can talk to the os.

There are two possible problems with prefix notation: that it is
unfamiliar to programmers, and that it is not dense enough. The
conventional wisdom in the Lisp world is that the first problem is
the real one. I'm not so sure. Yes, prefix notation makes ordinary
programmers panic. But I don't think ordinary programmers' opinions
matter. Languages become popular or unpopular based on what expert
hackers think of them, and I think expert hackers might be able to
deal with prefix notation. Perl syntax can be pretty incomprehensible,
but that has not stood in the way of Perl's popularity. If anything
it may have helped foster a Perl cult.

A more serious problem is the diffuseness of prefix notation. For
expert hackers, that really is a problem. No one wants to write
(aref a x y) when they could write a[x,y].

In this particular case there is a way to finesse our way out of
the problem. If we treat data structures as if they were functions
on indexes, we could write (a x y) instead, which is even shorter
than the Perl form. Similar tricks may shorten other types of
expressions.

We can get rid of (or make optional) a lot of parentheses by making
indentation significant. That's how programmers read code anyway:
when indentation says one thing and delimiters say another, we go
by the indentation. Treating indentation as significant would
eliminate this common source of bugs as well as making programs
shorter.

Sometimes infix syntax is easier to read. This is especially true
for math expressions. I've used Lisp my whole programming life and
I still don't find prefix math expressions natural. And yet it is
convenient, especially when you're generating code, to have operators
that take any number of arguments. So if we do have infix syntax,
it should probably be implemented as some kind of read-macro.

I don't think we should be religiously opposed to introducing syntax
into Lisp, as long as it translates in a well-understood way into
underlying s-expressions. There is already a good deal of syntax
in Lisp. It's not necessarily bad to introduce more, as long as no
one is forced to use it. In Common Lisp, some delimiters are reserved
for the language, suggesting that at least some of the designers
intended to have more syntax in the future.

One of the most egregiously unlispy pieces of syntax in Common Lisp
occurs in format strings; format is a language in its own right,
and that language is not Lisp. If there were a plan for introducing
more syntax into Lisp, format specifiers might be able to be included
in it. It would be a good thing if macros could generate format
specifiers the way they generate any other kind of code.

An eminent Lisp hacker told me that his copy of CLTL falls open to
the section format. Mine too. This probably indicates room for
improvement. It may also mean that programs do a lot of I/O.

8 Efficiency

A good language, as everyone knows, should generate fast code. But
in practice I don't think fast code comes primarily from things
you do in the design of the language. As Knuth pointed out long
ago, speed only matters in certain critical bottlenecks.  And as
many programmers have observed since, one is very often mistaken
about where these bottlenecks are.

So, in practice, the way to get fast code is to have a very good
profiler, rather than by, say, making the language strongly typed.
You don't need to know the type of every argument in every call in
the program. You do need to be able to declare the types of arguments
in the bottlenecks. And even more, you need to be able to find out
where the bottlenecks are.

One complaint people have had with Lisp is that it's hard to tell
what's expensive. This might be true. It might also be inevitable,
if you want to have a very abstract language. And in any case I
think good profiling would go a long way toward fixing the problem:
you'd soon learn what was expensive.

Part of the problem here is social. Language designers like to
write fast compilers. That's how they measure their skill. They
think of the profiler as an add-on, at best. But in practice a good
profiler may do more to improve the speed of actual programs written
in the language than a compiler that generates fast code. Here,
again, language designers are somewhat out of touch with their
users. They do a really good job of solving slightly the wrong
problem.

It might be a good idea to have an active profiler  to push
performance data to the programmer instead of waiting for him to
come asking for it. For example, the editor could display bottlenecks
in red when the programmer edits the source code. Another approach
would be to somehow represent what's happening in running programs.
This would be an especially big win in server-based applications,
where you have lots of running programs to look at. An active
profiler could show graphically what's happening in memory as a
program's running, or even make sounds that tell what's happening.

Sound is a good cue to problems. In one place I worked, we had a
big board of dials showing what was happening to our web servers.
The hands were moved by little servomotors that made a slight noise
when they turned. I couldn't see the board from my desk, but I
found that I could tell immediately, by the sound, when there was
a problem with a server.

It might even be possible to write a profiler that would automatically
detect inefficient algorithms. I would not be surprised if certain
patterns of memory access turned out to be sure signs of bad
algorithms. If there were a little guy running around inside the
computer executing our programs, he would probably have as long
and plaintive a tale to tell about his job as a federal government
employee. I often have a feeling that I'm sending the processor on
a lot of wild goose chases, but I've never had a good way to look
at what it's doing.

A number of Lisps now compile into byte code, which is then executed
by an interpreter. This is usually done to make the implementation
easier to port, but it could be a useful language feature. It might
be a good idea to make the byte code an official part of the
language, and to allow programmers to use inline byte code in
bottlenecks. Then such optimizations would be portable too.

The nature of speed, as perceived by the end-user, may be changing.
With the rise of server-based applications, more and more programs
may turn out to be i/o-bound. It will be worth making i/o fast.
The language can help with straightforward measures like simple,
fast, formatted output functions, and also with deep structural
changes like caching and persistent objects.

Users are interested in response time. But another kind of efficiency
will be increasingly important: the number of simultaneous users
you can support per processor. Many of the interesting applications
written in the near future will be server-based, and the number of
users per server is the critical question for anyone hosting such
applications. In the capital cost of a business offering a server-based
application, this is the divisor.

For years, efficiency hasn't mattered much in most end-user
applications. Developers have been able to assume that each user
would have an increasingly powerful processor sitting on their
desk. And by Parkinson's Law, software has expanded to use the
resources available. That will change with server-based applications.
In that world, the hardware and software will be supplied together.
For companies that offer server-based applications, it will make
a very big difference to the bottom line how many users they can
support per server.

In some applications, the processor will be the limiting factor,
and execution speed will be the most important thing to optimize.
But often memory will be the limit; the number of simultaneous
users will be determined by the amount of memory you need for each
user's data. The language can help here too. Good support for
threads will enable all the users to share a single heap. It may
also help to have persistent objects and/or language level support
for lazy loading.

9 Time

The last ingredient a popular language needs is time. No one wants
to write programs in a language that might go away, as so many
programming languages do. So most hackers will tend to wait until
a language has been around for a couple years before even considering
using it.

Inventors of wonderful new things are often surprised to discover
this, but you need time to get any message through to people. A
friend of mine rarely does anything the first time someone asks
him. He knows that people sometimes ask for things that they turn
out not to want. To avoid wasting his time, he waits till the third
or fourth time he's asked to do something; by then, whoever's asking
him may be fairly annoyed, but at least they probably really do
want whatever they're asking for.

Most people have learned to do a similar sort of filtering on new
things they hear about. They don't even start paying attention
until they've heard about something ten times. They're perfectly
justified: the majority of hot new whatevers do turn out to be a
waste of time, and eventually go away. By delaying learning VRML,
I avoided having to learn it at all.

So anyone who invents something new has to expect to keep repeating
their message for years before people will start to get it. We
wrote what was, as far as I know, the first web-server based
application, and it took us years to get it through to people that
it didn't have to be downloaded. It wasn't that they were stupid.
They just had us tuned out.

The good news is, simple repetition solves the problem. All you
have to do is keep telling your story, and eventually people will
start to hear. It's not when people notice you're there that they
pay attention; it's when they notice you're still there.

It's just as well that it usually takes a while to gain momentum.
Most technologies evolve a good deal even after they're first
launched  programming languages especially. Nothing could be better,
for a new techology, than a few years of being used only by a small
number of early adopters. Early adopters are sophisticated and
demanding, and quickly flush out whatever flaws remain in your
technology. When you only have a few users you can be in close
contact with all of them. And early adopters are forgiving when
you improve your system, even if this causes some breakage.

There are two ways new technology gets introduced: the organic
growth method, and the big bang method. The organic growth method
is exemplified by the classic seat-of-the-pants underfunded garage
startup. A couple guys, working in obscurity, develop some new
technology. They launch it with no marketing and initially have
only a few (fanatically devoted) users. They continue to improve
the technology, and meanwhile their user base grows by word of
mouth. Before they know it, they're big.

The other approach, the big bang method, is exemplified by the
VC-backed, heavily marketed startup. They rush to develop a product,
launch it with great publicity, and immediately (they hope) have
a large user base.

Generally, the garage guys envy the big bang guys. The big bang
guys are smooth and confident and respected by the VCs. They can
afford the best of everything, and the PR campaign surrounding the
launch has the side effect of making them celebrities. The organic
growth guys, sitting in their garage, feel poor and unloved. And
yet I think they are often mistaken to feel sorry for themselves.
Organic growth seems to yield better technology and richer founders
than the big bang method. If you look at the dominant technologies
today, you'll find that most of them grew organically.

This pattern doesn't only apply to companies. You see it in sponsored
research too. Multics and Common Lisp were big-bang projects, and
Unix and MacLisp were organic growth projects.

10 Redesign

"The best writing is rewriting," wrote E. B. White.  Every good
writer knows this, and it's true for software too. The most important
part of design is redesign. Programming languages, especially,
don't get redesigned enough.

To write good software you must simultaneously keep two opposing
ideas in your head. You need the young hacker's naive faith in
his abilities, and at the same time the veteran's skepticism. You
have to be able to think 
how hard can it be? with one half of
your brain while thinking 
it will never work with the other.

The trick is to realize that there's no real contradiction here.
You want to be optimistic and skeptical about two different things.
You have to be optimistic about the possibility of solving the
problem, but skeptical about the value of whatever solution you've
got so far.

People who do good work often think that whatever they're working
on is no good. Others see what they've done and are full of wonder,
but the creator is full of worry. This pattern is no coincidence:
it is the worry that made the work good.

If you can keep hope and worry balanced, they will drive a project
forward the same way your two legs drive a bicycle forward. In the
first phase of the two-cycle innovation engine, you work furiously
on some problem, inspired by your confidence that you'll be able
to solve it. In the second phase, you look at what you've done in
the cold light of morning, and see all its flaws very clearly. But
as long as your critical spirit doesn't outweigh your hope, you'll
be able to look at your admittedly incomplete system, and think,
how hard can it be to get the rest of the way?, thereby continuing
the cycle.

It's tricky to keep the two forces balanced. In young hackers,
optimism predominates. They produce something, are convinced it's
great, and never improve it. In old hackers, skepticism predominates,
and they won't even dare to take on ambitious projects.

Anything you can do to keep the redesign cycle going is good. Prose
can be rewritten over and over until you're happy with it. But
software, as a rule, doesn't get redesigned enough. Prose has
readers, but software has users. If a writer rewrites an essay,
people who read the old version are unlikely to complain that their
thoughts have been broken by some newly introduced incompatibility.

Users are a double-edged sword. They can help you improve your
language, but they can also deter you from improving it. So choose
your users carefully, and be slow to grow their number. Having
users is like optimization: the wise course is to delay it. Also,
as a general rule, you can at any given time get away with changing
more than you think. Introducing change is like pulling off a
bandage: the pain is a memory almost as soon as you feel it.

Everyone knows that it's not a good idea to have a language designed
by a committee. Committees yield bad design. But I think the worst
danger of committees is that they interfere with redesign. It is
so much work to introduce changes that no one wants to bother.
Whatever a committee decides tends to stay that way, even if most
of the members don't like it.

Even a committee of two gets in the way of redesign. This happens
particularly in the interfaces between pieces of software written
by two different people. To change the interface both have to agree
to change it at once. And so interfaces tend not to change at all,
which is a problem because they tend to be one of the most ad hoc
parts of any system.

One solution here might be to design systems so that interfaces
are horizontal instead of vertical  so that modules are always
vertically stacked strata of abstraction. Then the interface will
tend to be owned by one of them. The lower of two levels will either
be a language in which the upper is written, in which case the
lower level will own the interface, or it will be a slave, in which
case the interface can be dictated by the upper level.

11 Lisp

What all this implies is that there is hope for a new Lisp.  There
is hope for any language that gives hackers what they want, including
Lisp. I think we may have made a mistake in thinking that hackers
are turned off by Lisp's strangeness. This comforting illusion may
have prevented us from seeing the real problem with Lisp, or at
least Common Lisp, which is that it sucks for doing what hackers
want to do. A hacker's language needs powerful libraries and
something to hack. Common Lisp has neither. A hacker's language is
terse and hackable. Common Lisp is not.

The good news is, it's not Lisp that sucks, but Common Lisp. If we
can develop a new Lisp that is a real hacker's language, I think
hackers will use it. They will use whatever language does the job.
All we have to do is make sure this new Lisp does some important
job better than other languages.

History offers some encouragement. Over time, successive new
programming languages have taken more and more features from Lisp.
There is no longer much left to copy before the language you've
made is Lisp. The latest hot language, Python, is a watered-down
Lisp with infix syntax and no macros. A new Lisp would be a natural
step in this progression.

I sometimes think that it would be a good marketing trick to call
it an improved version of Python. That sounds hipper than Lisp. To
many people, Lisp is a slow AI language with a lot of parentheses.
Fritz Kunze's official biography carefully avoids mentioning the
L-word.  But my guess is that we shouldn't be afraid to call the
new Lisp Lisp. Lisp still has a lot of latent respect among the
very best hackers  the ones who took 6.001 and understood it, for
example. And those are the users you need to win.

In "How to Become a Hacker," Eric Raymond describes Lisp as something
like Latin or Greek  a language you should learn as an intellectual
exercise, even though you won't actually use it:

  Lisp is worth learning for the profound enlightenment experience
  you will have when you finally get it; that experience will make
  you a better programmer for the rest of your days, even if you
  never actually use Lisp itself a lot.

If I didn't know Lisp, reading this would set me asking questions.
A language that would make me a better programmer, if it means
anything at all, means a language that would be better for programming.
And that is in fact the implication of what Eric is saying.

As long as that idea is still floating around, I think hackers will
be receptive enough to a new Lisp, even if it is called Lisp. But
this Lisp must be a hacker's language, like the classic Lisps of
the 1970s. It must be terse, simple, and hackable. And it must have
powerful libraries for doing what hackers want to do now.

In the matter of libraries I think there is room to beat languages
like Perl and Python at their own game. A lot of the new applications
that will need to be written in the coming years will be 
server-based
applications. There's no reason a new Lisp shouldn't have string
libraries as good as Perl, and if this new Lisp also had powerful
libraries for server-based applications, it could be very popular.
Real hackers won't turn up their noses at a new tool that will let
them solve hard problems with a few library calls. Remember, hackers
are lazy.

It could be an even bigger win to have core language support for
server-based applications. For example, explicit support for programs
with multiple users, or data ownership at the level of type tags.

Server-based applications also give us the answer to the question
of what this new Lisp will be used to hack. It would not hurt to
make Lisp better as a scripting language for Unix. (It would be
hard to make it worse.) But I think there are areas where existing
languages would be easier to beat. I think it might be better to
follow the model of Tcl, and supply the Lisp together with a complete
system for supporting server-based applications. Lisp is a natural
fit for server-based applications. Lexical closures provide a way
to get the effect of subroutines when the ui is just a series of
web pages. S-expressions map nicely onto html, and macros are good
at generating it. There need to be better tools for writing
server-based applications, and there needs to be a new Lisp, and
the two would work very well together.

12 The Dream Language

By way of summary, let's try describing the hacker's dream language.
The dream language is 
beautiful, clean, and terse. It has an
interactive toplevel that starts up fast. You can write programs
to solve common problems with very little code.  Nearly all the
code in any program you write is code that's specific to your
application. Everything else has been done for you.

The syntax of the language is brief to a fault. You never have to
type an unnecessary character, or even to use the shift key much.

Using big abstractions you can write the first version of a program
very quickly. Later, when you want to optimize, there's a really
good profiler that tells you where to focus your attention. You
can make inner loops blindingly fast, even writing inline byte code
if you need to.

There are lots of good examples to learn from, and the language is
intuitive enough that you can learn how to use it from examples in
a couple minutes. You don't need to look in the manual much. The
manual is thin, and has few warnings and qualifications.

The language has a small core, and powerful, highly orthogonal
libraries that are as carefully designed as the core language. The
libraries all work well together; everything in the language fits
together like the parts in a fine camera. Nothing is deprecated,
or retained for compatibility. The source code of all the libraries
is readily available. It's easy to talk to the operating system
and to applications written in other languages.

The language is built in layers. The higher-level abstractions are
built in a very transparent way out of lower-level abstractions,
which you can get hold of if you want.

Nothing is hidden from you that doesn't absolutely have to be. The
language offers abstractions only as a way of saving you work,
rather than as a way of telling you what to do. In fact, the language
encourages you to be an equal participant in its design. You can
change everything about it, including even its syntax, and anything
you write has, as much as possible, the same status as what comes
predefined.



Notes

[1]  Macros very close to the modern idea were proposed by Timothy
Hart in 1964, two years after Lisp 1.5 was released. What was
missing, initially, were ways to avoid variable capture and multiple
evaluation; Hart's examples are subject to both.

[2]  In When the Air Hits Your Brain, neurosurgeon Frank Vertosick
recounts a conversation in which his chief resident, Gary, talks
about the difference between surgeons and internists ("fleas"):

  Gary and I ordered a large pizza and found an open booth. The
  chief lit a cigarette. "Look at those goddamn fleas, jabbering
  about some disease they'll see once in their lifetimes. That's
  the trouble with fleas, they only like the bizarre stuff. They
  hate their bread and butter cases. That's the difference between
  us and the fucking fleas. See, we love big juicy lumbar disc
  herniations, but they hate hypertension...."

It's hard to think of a lumbar disc herniation as juicy (except
literally). And yet I think I know what they mean. I've often had
a juicy bug to track down. Someone who's not a programmer would
find it hard to imagine that there could be pleasure in a bug.
Surely it's better if everything just works. In one way, it is.
And yet there is undeniably a grim satisfaction in hunting down
certain sorts of bugs.
April 2001

This essay developed out of conversations I've had with
several other programmers about why Java smelled suspicious.  It's not
a critique of Java!  It is a case study of hacker's radar.

Over time, hackers develop a nose for good (and bad) technology.
I thought it might be interesting to try and write down what
made Java seem suspect to me.

Some people who've read this think it's an interesting attempt to write about
something that hasn't been written about before.  Others say I
will get in trouble for appearing to be writing about
things I don't understand.  So, just in
case it does any good, let me clarify that I'm not writing here
about Java (which I have never used) but about hacker's radar
(which I have thought about a lot).



The aphorism "you can't tell a book by its cover" originated in
the times when books were sold in plain cardboard covers, to be
bound by each purchaser according to his own taste.  In those days,
you couldn't tell a book by its cover.  But publishing has advanced
since then: present-day publishers work hard to make the cover
something you can tell a book by.

I spend a lot of time in bookshops and I feel as if I have by now
learned to understand everything publishers mean to tell me about
a book, and perhaps a bit more.  The time I haven't spent in
bookshops I've spent mostly in front of computers, and I feel as
if I've learned, to some degree, to judge technology by its cover
as well.  It may be just luck, but I've saved myself from a few
technologies that turned out to be real stinkers.

So far, Java seems like a stinker to me.  I've never written a Java
program, never more than glanced over reference books about it,
but I have a hunch that it won't be a very successful language.
I may turn out to be mistaken; making predictions about technology
is a dangerous business.  But for what it's worth, as a sort of
time capsule, here's why I don't like the look of Java:


1. It has been so energetically hyped.  Real standards don't have
to be promoted.  No one had to promote C, or Unix, or HTML.  A real
standard tends to be already established by the time most people
hear about it.  On the hacker radar screen, Perl is as big as Java,
or bigger, just on the strength of its own merits.

2. It's aimed low.  In the original Java white paper, Gosling
explicitly says Java was designed not to be too difficult for
programmers used to C.  It was designed to be another C++: C plus
a few ideas taken from more advanced languages.  Like the creators
of sitcoms or junk food or package tours, Java's designers were
consciously designing a product for people not as smart as them.
Historically, languages designed for other people to use have been
bad:  Cobol, PL/I, Pascal, Ada, C++.  The good languages have been
those that were designed for their own creators:  C, Perl, Smalltalk,
Lisp.

3. It has ulterior motives.  Someone once said that the world would
be a better place if people only wrote books because they had
something to say, rather than because they wanted to write a book.
Likewise, the reason we hear about Java all the time is not because
it has something to say about programming languages.  We hear about
Java as part of a plan by Sun to undermine Microsoft.

4. No one loves it.  C, Perl, Python, Smalltalk, and Lisp programmers
love their languages.  I've never heard anyone say that they loved
Java.

5. People are forced to use it.  A lot of the people I know using
Java are using it because they feel they have to.  Either it's
something they felt they had to do to get funded, or something they
thought customers would want, or something they were told to do by
management.  These are smart people; if the technology was good,
they'd have used it voluntarily.

6. It has too many cooks.  The best programming languages have been
developed by small groups.  Java seems to be run by a committee.
If it turns out to be a good language, it will be the first time
in history that a committee has designed a good language.

7. It's bureaucratic.  From what little I know about Java, there
seem to be a lot of protocols for doing things.  Really good
languages aren't like that.  They let you do what you want and get
out of the way.

8. It's pseudo-hip.  Sun now pretends that Java is a grassroots,
open-source language effort like Perl or Python.  This one just
happens to be controlled by a giant company.  So the language is
likely to have the same drab clunkiness as anything else that comes
out of a big company.

9. It's designed for large organizations.  Large organizations have
different aims from hackers. They want languages that are (believed
to be) suitable for use by large teams of mediocre programmers--
languages with features that, like the speed limiters in U-Haul
trucks, prevent fools from doing too much damage.  Hackers don't
like a language that talks down to them.  Hackers just want power.
Historically, languages designed for large organizations (PL/I,
Ada) have lost, while hacker languages (C, Perl) have won.  The
reason: today's teenage hacker is tomorrow's CTO.

10. The wrong people like it.  The programmers I admire most are
not, on the whole, captivated by Java.  Who does like Java?  Suits,
who don't know one language from another, but know that they keep
hearing about Java in the press; programmers at big companies, who
are amazed to find that there is something even better than C++;
and plug-and-chug undergrads, who are ready to like anything that
might get them a job (will this be on the test?).  These people's
opinions change with every wind.

11. Its daddy is in a pinch.  Sun's business model is being undermined
on two fronts.  Cheap Intel processors, of the same type used in
desktop machines, are now more than fast enough for servers.  And
FreeBSD seems to be at least as good an OS for servers as Solaris.
Sun's advertising implies that you need Sun servers for industrial
strength applications.  If this were true, Yahoo would be first in
line to buy Suns;  but when I worked there, the servers were all
Intel boxes running FreeBSD.  This bodes ill for Sun's future.  If
Sun runs into trouble, they could drag Java down with them.

12. The DoD likes it.  The Defense Department is encouraging
developers to use Java. This seems to me the most damning sign of
all.  The Defense Department does a fine (though expensive) job of
defending the country, but they love plans and procedures and
protocols.  Their culture is the opposite of hacker culture; on
questions of software they will tend to bet wrong.  The last time
the DoD really liked a programming language, it was Ada.


Bear in mind, this is not a critique of Java, but a critique of
its cover.  I don't know Java well enough to like it or dislike
it.  This is just an explanation of why I don't find that I'm eager
to learn it.

It may seem cavalier to dismiss a language before you've even tried
writing programs in it.  But this is something all programmers have
to do.  There are too many technologies out there to learn them
all.  You have to learn to judge by outward signs which will be
worth your time.  I have likewise cavalierly dismissed Cobol, Ada,
Visual Basic, the IBM AS400, VRML, ISO 9000, the SET protocol, VMS,
Novell Netware, and CORBA, among others.  They just smelled wrong.

It could be that in Java's case I'm mistaken.  It could be that a
language promoted by one big company to undermine another, designed
by a committee for a "mainstream" audience, hyped to the skies,
and beloved of the DoD, happens nonetheless to be a clean, beautiful,
powerful language that I would love programming in.  It could be,
but it seems very unlikely.


Want to start a startup?  Get funded by
Y Combinator.





April 2001, rev. April 2003

(This article is derived from a talk given at the 2001 Franz
Developer Symposium.)


In the summer of 1995, my friend Robert Morris and I
started a startup called 
Viaweb.  
Our plan was to write
software that would let end users build online stores.
What was novel about this software, at the time, was
that it ran on our server, using ordinary Web pages
as the interface.

A lot of people could have been having this idea at the
same time, of course, but as far as I know, Viaweb was
the first Web-based application.  It seemed such
a novel idea to us that we named the company after it:
Viaweb, because our software worked via the Web,
instead of running on your desktop computer.

Another unusual thing about this software was that it
was written primarily in a programming language called
Lisp. It was one of the first big end-user
applications to be written in Lisp, which up till then
had been used mostly in universities and research labs. [1]

The Secret Weapon

Eric Raymond has written an essay called "How to Become a Hacker,"
and in it, among other things, he tells would-be hackers what
languages they should learn.  He suggests starting with Python and
Java, because they are easy to learn.  The serious hacker will also
want to learn C, in order to hack Unix, and Perl for system
administration and cgi scripts.  Finally, the truly serious hacker
should consider learning Lisp:

  Lisp is worth learning for the profound enlightenment experience
  you will have when you finally get it; that experience will make
  you a better programmer for the rest of your days, even if you
  never actually use Lisp itself a lot.

This is the same argument you tend to hear for learning Latin.  It
won't get you a job, except perhaps as a classics professor, but
it will improve your mind, and make you a better writer in languages
you do want to use, like English.

But wait a minute.  This metaphor doesn't stretch that far.  The
reason Latin won't get you a job is that no one speaks it.  If you
write in Latin, no one can understand you.  But Lisp is a computer
language, and computers speak whatever language you, the programmer,
tell them to.

So if Lisp makes you a better programmer, like he says, why wouldn't
you want to use it? If a painter were offered a brush that would
make him a better painter, it seems to me that he would want to
use it in all his paintings, wouldn't he? I'm not trying to make
fun of Eric Raymond here.  On the whole, his advice is good.  What
he says about Lisp is pretty much the conventional wisdom.  But
there is a contradiction in the conventional wisdom:  Lisp will
make you a better programmer, and yet you won't use it.

Why not?  Programming languages are just tools, after all.  If Lisp
really does yield better programs, you should use it.  And if it
doesn't, then who needs it?

This is not just a theoretical question.  Software is a very
competitive business, prone to natural monopolies.  A company that
gets software written faster and better will, all other things
being equal, put its competitors out of business.  And when you're
starting a startup, you feel this very keenly.  Startups tend to
be an all or nothing proposition.  You either get rich, or you get
nothing.  In a startup, if you bet on the wrong technology, your
competitors will crush you.

Robert and I both knew Lisp well, and we couldn't see any reason
not to trust our instincts and go with Lisp.  We knew that everyone
else was writing their software in C++ or Perl.  But we also knew
that that didn't mean anything.  If you chose technology that way,
you'd be running Windows.  When you choose technology, you have to
ignore what other people are doing, and consider only what will
work the best.

This is especially true in a startup.  In a big company, you can
do what all the other big companies are doing.  But a startup can't
do what all the other startups do.  I don't think a lot of people
realize this, even in startups.

The average big company grows at about ten percent a year.  So if
you're running a big company and you do everything the way the
average big company does it, you can expect to do as well as the
average big company-- that is, to grow about ten percent a year.

The same thing will happen if you're running a startup, of course.
If you do everything the way the average startup does it, you should
expect average performance.  The problem here is, average performance
means that you'll go out of business.  The survival rate for startups
is way less than fifty percent.  So if you're running a startup,
you had better be doing something odd.  If not, you're in trouble.

Back in 1995, we knew something that I don't think our competitors
understood, and few understand even now:  when you're writing
software that only has to run on your own servers, you can use
any language you want.  When you're writing desktop software,
there's a strong bias toward writing applications in the same
language as the operating system.  Ten years ago, writing applications
meant writing applications in C.  But with Web-based software,
especially when you have the source code of both the language and
the operating system, you can use whatever language you want.

This new freedom is a double-edged sword, however.  Now that you
can use any language, you have to think about which one to use.
Companies that try to pretend nothing has changed risk finding that
their competitors do not.

If you can use any language, which do you use?  We chose Lisp.
For one thing, it was obvious that rapid development would be
important in this market.  We were all starting from scratch, so
a company that could get new features done before its competitors
would have a big advantage.  We knew Lisp was a really good language
for writing software quickly, and server-based applications magnify
the effect of rapid development, because you can release software
the minute it's done.

If other companies didn't want to use Lisp, so much the better.
It might give us a technological edge, and we needed all the help
we could get.  When we started Viaweb, we had no experience in
business.  We didn't know anything about marketing, or hiring
people, or raising money, or getting customers.  Neither of us had
ever even had what you would call a real job.  The only thing we
were good at was writing software.  We hoped that would save us.
Any advantage we could get in the software department, we would
take.

So you could say that using Lisp was an experiment.  Our hypothesis
was that if we wrote our software in Lisp, we'd be able to get
features done faster than our competitors, and also to do things
in our software that they couldn't do.  And because Lisp was so
high-level, we wouldn't need a big development team, so our costs
would be lower.  If this were so, we could offer a better product
for less money, and still make a profit.  We would end up getting
all the users, and our competitors would get none, and eventually
go out of business.  That was what we hoped would happen, anyway.

What were the results of this experiment?  Somewhat surprisingly,
it worked.  We eventually had many competitors, on the order of
twenty to thirty of them, but none of their software could compete
with ours.  We had a wysiwyg online store builder that ran on the
server and yet felt like a desktop application.  Our competitors
had cgi scripts.  And we were always far ahead of them in features.
Sometimes, in desperation, competitors would try to introduce
features that we didn't have.  But with Lisp our development cycle
was so fast that we could sometimes duplicate a new feature within
a day or two of a competitor announcing it in a press release.  By
the time journalists covering the press release got round to calling
us, we would have the new feature too.

It must have seemed to our competitors that we had some kind of
secret weapon-- that we were decoding their Enigma traffic or
something.  In fact we did have a secret weapon, but it was simpler
than they realized.  No one was leaking news of their features to
us.   We were just able to develop software faster than anyone
thought possible.

When I was about nine I happened to get hold of a copy of The Day
of the Jackal, by Frederick Forsyth.  The main character is an
assassin who is hired to kill the president of France.  The assassin
has to get past the police to get up to an apartment that overlooks
the president's route.  He walks right by them, dressed up as an
old man on crutches, and they never suspect him.

Our secret weapon was similar.  We wrote our software in a weird
AI language, with a bizarre syntax full of parentheses.  For years
it had annoyed me to hear Lisp described that way.  But now it
worked to our advantage.  In business, there is nothing more valuable
than a technical advantage your competitors don't understand.  In
business, as in war, surprise is worth as much as force.

And so, I'm a little embarrassed to say, I never said anything
publicly about Lisp while we were working on Viaweb.  We never
mentioned it to the press, and if you searched for Lisp on our Web
site, all you'd find were the titles of two books in my bio.  This
was no accident.  A startup should give its competitors as little
information as possible.  If they didn't know what language our
software was written in, or didn't care, I wanted to keep it that
way.[2]

The people who understood our technology best were the customers.
They didn't care what language Viaweb was written in either, but
they noticed that it worked really well.  It let them build great
looking online stores literally in minutes.  And so, by word of
mouth mostly, we got more and more users.  By the end of 1996 we
had about 70 stores online.  At the end of 1997 we had 500.  Six
months later, when Yahoo bought us, we had 1070 users.  Today, as
Yahoo Store, this software continues to dominate its market.  It's
one of the more profitable pieces of Yahoo, and the stores built
with it are the foundation of Yahoo Shopping.  I left Yahoo in
1999, so I don't know exactly how many users they have now, but
the last I heard there were about 20,000.


The Blub Paradox

What's so great about Lisp?  And if Lisp is so great, why doesn't
everyone use it?  These sound like rhetorical questions, but actually
they have straightforward answers.  Lisp is so great not because
of some magic quality visible only to devotees, but because it is
simply the most powerful language available.  And the reason everyone
doesn't use it is that programming languages are not merely
technologies, but habits of mind as well, and nothing changes
slower.  Of course, both these answers need explaining.

I'll begin with a shockingly controversial statement:  programming
languages vary in power.

Few would dispute, at least, that high level languages are more
powerful than machine language.  Most programmers today would agree
that you do not, ordinarily, want to program in machine language.
Instead, you should program in a high-level language, and have a
compiler translate it into machine language for you.  This idea is
even built into the hardware now: since the 1980s, instruction sets
have been designed for compilers rather than human programmers.

Everyone knows it's a mistake to write your whole program by hand
in machine language.  What's less often understood is that there
is a more general principle here: that if you have a choice of
several languages, it is, all other things being equal, a mistake
to program in anything but the most powerful one. [3]

There are many exceptions to this rule.  If you're writing a program
that has to work very closely with a program written in a certain
language, it might be a good idea to write the new program in the
same language.  If you're writing a program that only has to do
something very simple, like number crunching or bit manipulation,
you may as well use a less abstract language, especially since it
may be slightly faster.  And if you're writing a short, throwaway
program, you may be better off just using whatever language has
the best library functions for the task.  But in general, for
application software, you want to be using the most powerful
(reasonably efficient) language you can get, and using anything
else is a mistake, of exactly the same kind, though possibly in a
lesser degree, as programming in machine language.

You can see that machine language is very low level.  But, at least
as a kind of social convention, high-level languages are often all
treated as equivalent.  They're not.  Technically the term "high-level
language" doesn't mean anything very definite.  There's no dividing
line with machine languages on one side and all the high-level
languages on the other.  Languages fall along a continuum [4] of
abstractness, from the most powerful all the way down to machine
languages, which themselves vary in power.

Consider Cobol.  Cobol is a high-level language, in the sense that
it gets compiled into machine language.  Would anyone seriously
argue that Cobol is equivalent in power to, say, Python?  It's
probably closer to machine language than Python.

Or how about Perl 4?  Between Perl 4 and Perl 5, lexical closures
got added to the language.  Most Perl hackers would agree that Perl
5 is more powerful than Perl 4.  But once you've admitted that,
you've admitted that one high level language can be more powerful
than another.  And it follows inexorably that, except in special
cases, you ought to use the most powerful you can get.

This idea is rarely followed to its conclusion, though.  After a
certain age, programmers rarely switch languages voluntarily.
Whatever language people happen to be used to, they tend to consider
just good enough.

Programmers get very attached to their favorite languages, and I
don't want to hurt anyone's feelings, so to explain this point I'm
going to use a hypothetical language called Blub.  Blub falls right
in the middle of the abstractness continuum.  It is not the most
powerful language, but it is more powerful than Cobol or machine
language.

And in fact, our hypothetical Blub programmer wouldn't use either
of them.  Of course he wouldn't program in machine language.  That's
what compilers are for.  And as for Cobol, he doesn't know how
anyone can get anything done with it.  It doesn't even have x (Blub
feature of your choice).

As long as our hypothetical Blub programmer is looking down the
power continuum, he knows he's looking down.  Languages less powerful
than Blub are obviously less powerful, because they're missing some
feature he's used to.  But when our hypothetical Blub programmer
looks in the other direction, up the power continuum, he doesn't
realize he's looking up.  What he sees are merely weird languages.
He probably considers them about equivalent in power to Blub, but
with all this other hairy stuff thrown in as well.  Blub is good
enough for him, because he thinks in Blub.

When we switch to the point of view of a programmer using any of
the languages higher up the power continuum, however, we find that
he in turn looks down upon Blub.  How can you get anything done in
Blub? It doesn't even have y.

By induction, the only programmers in a position to see all the
differences in power between the various languages are those who
understand the most powerful one.  (This is probably what Eric
Raymond meant about Lisp making you a better programmer.) You can't
trust the opinions of the others, because of the Blub paradox:
they're satisfied with whatever language they happen to use, because
it dictates the way they think about programs.

I know this from my own experience, as a high school kid writing
programs in Basic.  That language didn't even support recursion.
It's hard to imagine writing programs without using recursion, but
I didn't miss it at the time.  I thought in Basic.  And I was a
whiz at it.  Master of all I surveyed.

The five languages that Eric Raymond recommends to hackers fall at
various points on the power continuum.  Where they fall relative
to one another is a sensitive topic.  What I will say is that I
think Lisp is at the top.  And to support this claim I'll tell you
about one of the things I find missing when I look at the other
four languages.  How can you get anything done in them, I think,
without macros? [5]

Many languages have something called a macro.  But Lisp macros are
unique.  And believe it or not, what they do is related to the
parentheses.  The designers of Lisp didn't put all those parentheses
in the language just to be different.  To the Blub programmer, Lisp
code looks weird.  But those parentheses are there for a reason.
They are the outward evidence of a fundamental difference between
Lisp and other languages.

Lisp code is made out of Lisp data objects.  And not in the trivial
sense that the source files contain characters, and strings are
one of the data types supported by the language.  Lisp code, after
it's read by the parser, is made of data structures that you can
traverse.

If you understand how compilers work, what's really going on is
not so much that Lisp has a strange syntax as that Lisp has no
syntax.  You write programs in the parse trees that get generated
within the compiler when other languages are parsed.  But these
parse trees are fully accessible to your programs.  You can write
programs that manipulate them.  In Lisp, these programs are called
macros.  They are programs that write programs.

Programs that write programs?  When would you ever want to do that?
Not very often, if you think in Cobol.  All the time, if you think
in Lisp.  It would be convenient here if I could give an example
of a powerful macro, and say there! how about that?  But if I did,
it would just look like gibberish to someone who didn't know Lisp;
there isn't room here to explain everything you'd need to know to
understand what it meant.  In 
Ansi Common Lisp I tried to move
things along as fast as I could, and even so I didn't get to macros
until page 160.

But I think I can give a kind of argument that might be convincing.
The source code of the Viaweb editor was probably about 20-25%
macros.  Macros are harder to write than ordinary Lisp functions,
and it's considered to be bad style to use them when they're not
necessary.  So every macro in that code is there because it has to
be.  What that means is that at least 20-25% of the code in this
program is doing things that you can't easily do in any other
language.  However skeptical the Blub programmer might be about my
claims for the mysterious powers of Lisp, this ought to make him
curious.  We weren't writing this code for our own amusement.  We
were a tiny startup, programming as hard as we could in order to
put technical barriers between us and our competitors.

A suspicious person might begin to wonder if there was some
correlation here.  A big chunk of our code was doing things that
are very hard to do in other languages.  The resulting software
did things our competitors' software couldn't do.  Maybe there was
some kind of connection.  I encourage you to follow that thread.
There may be more to that old man hobbling along on his crutches
than meets the eye.

Aikido for Startups

But I don't expect to convince anyone 
(over 25) 
to go out and learn
Lisp.  The purpose of this article is not to change anyone's mind,
but to reassure people already interested in using Lisp-- people
who know that Lisp is a powerful language, but worry because it
isn't widely used.  In a competitive situation, that's an advantage.
Lisp's power is multiplied by the fact that your competitors don't
get it.

If you think of using Lisp in a startup, you shouldn't worry that
it isn't widely understood.  You should hope that it stays that
way. And it's likely to.  It's the nature of programming languages
to make most people satisfied with whatever they currently use.
Computer hardware changes so much faster than personal habits that
programming practice is usually ten to twenty years behind the
processor.  At places like MIT they were writing programs in
high-level languages in the early 1960s, but many companies continued
to write code in machine language well into the 1980s.  I bet a
lot of people continued to write machine language until the processor,
like a bartender eager to close up and go home, finally kicked them
out by switching to a risc instruction set.

Ordinarily technology changes fast.  But programming languages are
different: programming languages are not just technology, but what
programmers think in.  They're half technology and half religion.[6]
And so the median language, meaning whatever language the median
programmer uses, moves as slow as an iceberg.  Garbage collection,
introduced by Lisp in about 1960, is now widely considered to be
a good thing.  Runtime typing, ditto, is growing in popularity.
Lexical closures, introduced by Lisp in the early 1970s, are now,
just barely, on the radar screen.  Macros, introduced by Lisp in the
mid 1960s, are still terra incognita.

Obviously, the median language has enormous momentum.  I'm not
proposing that you can fight this powerful force.  What I'm proposing
is exactly the opposite: that, like a practitioner of Aikido, you
can use it against your opponents.

If you work for a big company, this may not be easy.  You will have
a hard time convincing the pointy-haired boss to let you build
things in Lisp, when he has just read in the paper that some other
language is poised, like Ada was twenty years ago, to take over
the world.  But if you work for a startup that doesn't have
pointy-haired bosses yet, you can, like we did, turn the Blub
paradox to your advantage:  you can use technology that your
competitors, glued immovably to the median language, will never be
able to match.

If you ever do find yourself working for a startup, here's a handy
tip for evaluating competitors.  Read their job listings.  Everything
else on their site may be stock photos or the prose equivalent,
but the job listings have to be specific about what they want, or
they'll get the wrong candidates.

During the years we worked on Viaweb I read a lot of job descriptions.
A new competitor seemed to emerge out of the woodwork every month
or so.  The first thing I would do, after checking to see if they
had a live online demo, was look at their job listings.  After a
couple years of this I could tell which companies to worry about
and which not to.  The more of an IT flavor the job descriptions
had, the less dangerous the company was.  The safest kind were the
ones that wanted Oracle experience.  You never had to worry about
those.  You were also safe if they said they wanted C++ or Java
developers.  If they wanted Perl or Python programmers, that would
be a bit frightening-- that's starting to sound like a company
where the technical side, at least, is run by real hackers.  If I
had ever seen a job posting looking for Lisp hackers, I would have
been really worried.




Notes

[1] Viaweb at first had two parts: the editor, written in Lisp,
which people used to build their sites, and the ordering system,
written in C, which handled orders.  The first version was mostly
Lisp, because the ordering system was small.  Later we added two
more modules, an image generator written in C, and a back-office
manager written mostly in Perl.

In January 2003, Yahoo released a new version of the editor 
written in C++ and Perl.  It's hard to say whether the program is no
longer written in Lisp, though, because to translate this program
into C++ they literally had to write a Lisp interpreter: the source
files of all the page-generating templates are still, as far as I
know,  Lisp code.  (See Greenspun's Tenth Rule.)

[2] Robert Morris says that I didn't need to be secretive, because
even if our competitors had known we were using Lisp, they wouldn't
have understood why:  "If they were that smart they'd already be
programming in Lisp."

[3] All languages are equally powerful in the sense of being Turing
equivalent, but that's not the sense of the word programmers care
about. (No one wants to program a Turing machine.)  The kind of
power programmers care about may not be formally definable, but
one way to explain it would be to say that it refers to features
you could only get in the less powerful language by writing an
interpreter for the more powerful language in it. If language A
has an operator for removing spaces from strings and language B
doesn't, that probably doesn't make A more powerful, because you
can probably write a subroutine to do it in B.  But if A supports,
say, recursion, and B doesn't, that's not likely to be something
you can fix by writing library functions.

[4] Note to nerds: or possibly a lattice, narrowing toward the top;
it's not the shape that matters here but the idea that there is at
least a partial order.

[5] It is a bit misleading to treat macros as a separate feature.
In practice their usefulness is greatly enhanced by other Lisp
features like lexical closures and rest parameters.

[6] As a result, comparisons of programming languages either take
the form of religious wars or undergraduate textbooks so determinedly
neutral that they're really works of anthropology.  People who
value their peace, or want tenure, avoid the topic.  But the question
is only half a religious one; there is something there worth
studying, especially if you want to design new languages.

After a link to 
Beating the Averages was posted on slashdot, 
some readers wanted to hear in more detail 
about the specific technical advantages we got from using
Lisp in Viaweb.  For those who are interested,
here are some excerpts from a talk I gave in April 2001 at
BBN Labs in Cambridge, MA.
1993

(This essay is from the introduction to On Lisp.  The red text
explains the origins of Arc's name.)

It's a long-standing principle of programming style that the functional
elements of a program should not be too large.  If some component of a
program grows beyond the stage where it's readily comprehensible,
it becomes a mass of complexity which conceals errors as easily
as a big city conceals fugitives.  Such software will be
hard to read, hard to test, and hard to debug.

In accordance with this principle, a large program must be divided
into pieces, and the larger the program, the more it must be divided.
How do you divide a program?  The traditional approach is
called top-down design: you say "the purpose of the
program is to do these seven things, so I divide it into seven major
subroutines.  The first subroutine has to do these four things, so
it in turn will have four of its own subroutines," and so on.
This process continues until the whole program has the right level
of granularity-- each part large enough to do something substantial,
but small enough to be understood as a single unit.

Experienced Lisp programmers divide up their programs differently.
As well as top-down design, they follow a principle which
could be called bottom-up design-- changing the language
to suit the problem.
In Lisp, you don't just write your program down toward the language,
you also build the language up toward your program.  As you're
writing a program you may think "I wish Lisp had such-and-such an
operator." So you go and write it. Afterward
you realize that using the new operator would simplify the design  
of another part of the program, and so on.
Language and program evolve together.
Like the border between two warring states,
the boundary between language and program is drawn and redrawn,
until eventually it comes to rest along the mountains and rivers,
the natural frontiers of your problem.
In the end your program will look as if the language had been
designed for it.
And when language and
program fit one another well, you end up with code which is
clear, small, and efficient.


It's worth emphasizing that bottom-up design doesn't mean
just writing the same program in a different order.  When you
work bottom-up, you usually end up with a different program.
Instead of a single, monolithic program,
you will get a larger language with more abstract operators,   
and a smaller program written in it.  Instead of a lintel,
you'll get an arch.


In typical code, once you abstract out the parts which are
merely bookkeeping, what's left is much shorter;
the higher you build up the language, the less distance you
will have to travel from the top down to it.
This brings several advantages:


 By making the language do more of the work, bottom-up design
yields programs which are smaller and more agile.  A shorter
program doesn't have to be divided into so many components, and
fewer components means programs which are easier to read or
modify.  Fewer components also means fewer connections between   
components, and thus less chance for errors there.  As
industrial designers strive to reduce the number of moving parts
in a machine, experienced Lisp programmers use bottom-up design
to reduce the size and complexity of their programs.

 Bottom-up design promotes code re-use.
When you write two
or more programs, many of the utilities you wrote for the first
program will also be useful in the succeeding ones.  Once you've  
acquired a large substrate of utilities, writing a new program can
take only a fraction of the effort it would require if you had to 
start with raw Lisp.

 Bottom-up design makes programs easier to read.

An instance of this type
of abstraction asks the reader to understand a general-purpose operator;
an instance of functional abstraction asks the reader to understand
a special-purpose subroutine. [1]

 Because it causes you always to be on the lookout for patterns
in your code, working bottom-up helps to clarify your ideas about
the design of your program.  If two distant components of a program
are similar in form, you'll be led to notice the similarity and
perhaps to redesign the program in a simpler way.


Bottom-up design is possible to a certain degree in languages
other than Lisp.  Whenever you see library functions,
bottom-up design is happening.  However, Lisp gives you much broader
powers in this department, and augmenting the language plays a
proportionately larger role in Lisp style-- so much so that
Lisp is not just a different language, but a whole different way
of programming.

It's true that this style of development is better suited to
programs which can be written by small groups.  However, at the
same time, it extends the limits of what can be done by a small
group.  In The Mythical Man-Month,
Frederick Brooks
proposed that the productivity of a group of programmers
does not grow linearly with its size.  As the size of the
group increases, the productivity of individual programmers
goes down.  The experience of Lisp programming  
suggests a more cheerful way
to phrase this law: as the size of the group decreases, the
productivity of individual programmers goes up.
A small group wins, relatively speaking, simply because it's
smaller.  When a small group also takes advantage of the
techniques that Lisp makes possible, it can 
win outright.



New: Download On Lisp for Free.






[1] "But no one can read
the program without understanding all your new utilities."
To see why such statements are usually mistaken,
see Section 4.8.
November 2016

If you're a California voter, there is an important proposition
on your ballot this year: Proposition 62, which bans the death
penalty.

When I was younger I used to think the debate about the death
penalty was about when it's ok to take a human life.  Is it ok
to kill a killer?

But that is not the issue here.

The real world does not work like the version I was shown on TV growing up.  The police 
often arrest the wrong person.
Defendants' lawyers are often incompetent.  And prosecutors
are often motivated more by publicity than justice.

In the real world,       
about 4% of people sentenced to death
are innocent.
So this is not about whether it's ok to kill killers. This
is about whether it's ok to kill innocent people.

A child could answer that one for you.

This year, in California, you have a chance to end this, by
voting yes on Proposition 62. But beware, because there is another 
proposition, Proposition 66, whose goal is to make it 
easier to execute people. So yes on 62, no on 66.

It's time.
